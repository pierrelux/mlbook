{"version":"1","records":[{"hierarchy":{"lvl1":"Introduction"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Introduction"},"content":"","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Introduction","lvl2":"Qu’est-ce qu’un modèle?"},"type":"lvl2","url":"/#quest-ce-quun-mod-le","position":2},{"hierarchy":{"lvl1":"Introduction","lvl2":"Qu’est-ce qu’un modèle?"},"content":"Le mot modèle est omniprésent en apprentissage machine, mais sa signification précise est rarement explicitée. Un modèle est une représentation simplifiée d’un phénomène — une abstraction qui capture certains aspects de la réalité tout en en ignorant d’autres. Cette idée n’est pas propre à l’informatique: les physiciens utilisent des modèles (la mécanique newtonienne ignore les effets relativistes), les économistes aussi (l’homo economicus ignore l’irrationalité humaine), et les statisticiens depuis toujours.\n\nEn apprentissage machine, un modèle est typiquement une fonction paramétrée f_\\theta: \\mathcal{X} \\to \\mathcal{Y} qui associe des entrées x à des sorties y. Les paramètres \\theta déterminent le comportement de cette fonction. Apprendre, c’est trouver les valeurs de \\theta qui font que f_\\theta se comporte de façon utile — par exemple, qu’elle prédise correctement des étiquettes sur de nouvelles données.\n\nPrenons un exemple concret. Supposons qu’on veuille prédire le prix d’une maison à partir de sa superficie. Un modèle linéaire simple serait:f_\\theta(x) = \\theta_0 + \\theta_1 x\n\noù x est la superficie, \\theta_0 est l’ordonnée à l’origine, et \\theta_1 est la pente. Ce modèle fait une hypothèse forte: la relation entre superficie et prix est linéaire. Cette hypothèse est probablement fausse — les très grandes maisons ne suivent pas la même dynamique que les petites — mais elle peut être utile si elle capture l’essentiel de la variation pour les maisons qui nous intéressent.\n\nC’est là une tension fondamentale: un modèle trop simple ignore des régularités importantes (sous-apprentissage), mais un modèle trop complexe risque de capturer du bruit plutôt que du signal (surapprentissage). Tout le livre tourne autour de cette tension.","type":"content","url":"/#quest-ce-quun-mod-le","position":3},{"hierarchy":{"lvl1":"Introduction","lvl3":"Modèle vs algorithme d’apprentissage","lvl2":"Qu’est-ce qu’un modèle?"},"type":"lvl3","url":"/#mod-le-vs-algorithme-dapprentissage","position":4},{"hierarchy":{"lvl1":"Introduction","lvl3":"Modèle vs algorithme d’apprentissage","lvl2":"Qu’est-ce qu’un modèle?"},"content":"Il est important de distinguer le modèle (la famille de fonctions \\{f_\\theta : \\theta \\in \\Theta\\}) de l’algorithme d’apprentissage (la procédure qui, étant donné des données, choisit un \\theta particulier). Le même modèle peut être entraîné par différents algorithmes; le même algorithme peut être appliqué à différents modèles.\n\nPar exemple, pour un modèle linéaire:\n\nL’algorithme des moindres carrés trouve le \\theta qui minimise la somme des erreurs au carré\n\nLa descente de gradient trouve (approximativement) le même \\theta par itérations successives\n\nL’inférence bayésienne produit une distribution sur les \\theta possibles plutôt qu’un point unique\n\nCes algorithmes ont des propriétés différentes (rapidité, stabilité, interprétabilité), mais ils opèrent tous sur le même espace de modèles.","type":"content","url":"/#mod-le-vs-algorithme-dapprentissage","position":5},{"hierarchy":{"lvl1":"Introduction","lvl2":"Pourquoi apprendre l’apprentissage machine à l’ère des LLMs?"},"type":"lvl2","url":"/#pourquoi-apprendre-lapprentissage-machine-l-re-des-llms","position":6},{"hierarchy":{"lvl1":"Introduction","lvl2":"Pourquoi apprendre l’apprentissage machine à l’ère des LLMs?"},"content":"Aujourd’hui, un assistant de programmation peut écrire un pipeline d’apprentissage machine complet en quelques secondes. Il peut charger des données, définir un modèle, l’entraîner, afficher des courbes d’apprentissage, et rapporter des métriques de performance. Pourquoi, alors, passer un semestre à étudier les fondements théoriques et algorithmiques de l’AM?\n\nLa réponse courte: parce que les LLMs sont eux-mêmes des systèmes d’apprentissage machine, et ils utilisent constamment l’AM comme sous-routine.","type":"content","url":"/#pourquoi-apprendre-lapprentissage-machine-l-re-des-llms","position":7},{"hierarchy":{"lvl1":"Introduction","lvl3":"Les LLMs sont construits sur ces fondements","lvl2":"Pourquoi apprendre l’apprentissage machine à l’ère des LLMs?"},"type":"lvl3","url":"/#les-llms-sont-construits-sur-ces-fondements","position":8},{"hierarchy":{"lvl1":"Introduction","lvl3":"Les LLMs sont construits sur ces fondements","lvl2":"Pourquoi apprendre l’apprentissage machine à l’ère des LLMs?"},"content":"Un grand modèle de langage comme GPT ou Claude est, fondamentalement:\n\nUne architecture de réseau de neurones (le transformer, couvert au chapitre sur les réseaux récurrents et l’attention)\n\nEntraînée par descente de gradient stochastique sur une fonction de perte (la cross-entropie sur la prédiction du prochain token)\n\nAvec des techniques de régularisation pour éviter le surapprentissage (dropout, weight decay)\n\nEt des considérations de généralisation pour que le modèle fonctionne sur des textes jamais vus\n\nComprendre les LLMs, c’est comprendre l’AM. Un praticien qui ne connaît pas la descente de gradient ne peut pas diagnostiquer pourquoi un fine-tuning diverge. Un praticien qui ne comprend pas le surapprentissage ne saura pas interpréter les courbes de validation. Un praticien qui ignore la notion de distribution des données ne comprendra pas les échecs de généralisation hors domaine.","type":"content","url":"/#les-llms-sont-construits-sur-ces-fondements","position":9},{"hierarchy":{"lvl1":"Introduction","lvl3":"Les LLMs utilisent l’AM comme sous-routine","lvl2":"Pourquoi apprendre l’apprentissage machine à l’ère des LLMs?"},"type":"lvl3","url":"/#les-llms-utilisent-lam-comme-sous-routine","position":10},{"hierarchy":{"lvl1":"Introduction","lvl3":"Les LLMs utilisent l’AM comme sous-routine","lvl2":"Pourquoi apprendre l’apprentissage machine à l’ère des LLMs?"},"content":"Quand vous demandez à un assistant de programmation d’analyser des données ou de construire un modèle prédictif, il génère du code qui appelle des algorithmes d’AM classiques: régression logistique, forêts aléatoires, réseaux de neurones, validation croisée. Le LLM ne fait pas l’apprentissage — il écrit du code qui le fait.\n\nEt ce code peut être faux.\n\nUn LLM peut écrire un pipeline où:\n\nLes données de test sont utilisées pour choisir les hyperparamètres (fuite d’information)\n\nLes caractéristiques incluent des variables qui ne seront pas disponibles en production (variables privilégiées)\n\nLe modèle mémorise les exemples d’entraînement plutôt que d’apprendre des régularités (surapprentissage)\n\nLa métrique optimisée ne correspond pas à l’objectif métier réel (erreur de spécification)\n\nDans tous ces cas, les métriques rapportées par le pipeline seront excellentes, mais le modèle échouera en déploiement. Savoir détecter ces erreurs requiert une compréhension des fondements.","type":"content","url":"/#les-llms-utilisent-lam-comme-sous-routine","position":11},{"hierarchy":{"lvl1":"Introduction","lvl3":"La compétence centrale: auditer un pipeline","lvl2":"Pourquoi apprendre l’apprentissage machine à l’ère des LLMs?"},"type":"lvl3","url":"/#la-comp-tence-centrale-auditer-un-pipeline","position":12},{"hierarchy":{"lvl1":"Introduction","lvl3":"La compétence centrale: auditer un pipeline","lvl2":"Pourquoi apprendre l’apprentissage machine à l’ère des LLMs?"},"content":"À l’ère où le code s’écrit facilement, la compétence rare n’est plus d’écrire du code — c’est de savoir si le code fait ce qu’il prétend faire. Un praticien compétent doit pouvoir:\n\nLire un pipeline d’AM et identifier sa structure (quel modèle? quelle perte? quel algorithme d’optimisation?)\n\nÉvaluer si le protocole expérimental est valide (les données de test sont-elles vraiment indépendantes? la métrique est-elle pertinente?)\n\nDiagnostiquer les modes d’échec (surapprentissage? sous-apprentissage? fuite d’information?)\n\nCorriger en utilisant les bons outils (régularisation, validation croisée, augmentation de données)\n\nCe livre vise à développer ces compétences. Chaque chapitre introduit des concepts qui permettent de poser des questions précises sur un système d’apprentissage: Quelle est la classe d’hypothèses? Quel est le risque que nous minimisons? Comment savons-nous que le modèle généralisera?\n\nCompétence centrale\n\nSavoir entraîner un modèle ne suffit plus. Il faut savoir inspecter, évaluer et critiquer les artefacts produits par du code — qu’il soit écrit par un humain ou généré par un LLM.","type":"content","url":"/#la-comp-tence-centrale-auditer-un-pipeline","position":13},{"hierarchy":{"lvl1":"Introduction","lvl2":"Types d’apprentissage"},"type":"lvl2","url":"/#types-dapprentissage","position":14},{"hierarchy":{"lvl1":"Introduction","lvl2":"Types d’apprentissage"},"content":"Les problèmes d’apprentissage machine se divisent en plusieurs catégories selon la nature des données disponibles et l’objectif visé.","type":"content","url":"/#types-dapprentissage","position":15},{"hierarchy":{"lvl1":"Introduction","lvl3":"Apprentissage supervisé","lvl2":"Types d’apprentissage"},"type":"lvl3","url":"/#apprentissage-supervis","position":16},{"hierarchy":{"lvl1":"Introduction","lvl3":"Apprentissage supervisé","lvl2":"Types d’apprentissage"},"content":"Dans l’apprentissage supervisé, nous disposons de paires d’entrées et de sorties: un ensemble \\{(x_1, y_1), \\ldots, (x_n, y_n)\\} où chaque x_i est une entrée et y_i est la sortie correspondante (l’étiquette ou la cible). L’objectif est d’apprendre une fonction f telle que f(x) \\approx y pour de nouvelles paires (x, y) jamais vues.\n\nSelon la nature de la sortie:\n\nClassification: y \\in \\{0, 1, \\ldots, K-1\\} (un nombre fini de classes). Exemple: déterminer si un courriel est un spam.\n\nRégression: y \\in \\mathbb{R} (une valeur continue). Exemple: prédire le prix d’une maison.","type":"content","url":"/#apprentissage-supervis","position":17},{"hierarchy":{"lvl1":"Introduction","lvl3":"Apprentissage non supervisé","lvl2":"Types d’apprentissage"},"type":"lvl3","url":"/#apprentissage-non-supervis","position":18},{"hierarchy":{"lvl1":"Introduction","lvl3":"Apprentissage non supervisé","lvl2":"Types d’apprentissage"},"content":"Dans l’apprentissage non supervisé, nous n’avons que des entrées \\{x_1, \\ldots, x_n\\} sans étiquettes associées. L’objectif est de découvrir une structure cachée dans les données:\n\nPartitionnement (clustering): regrouper les données en clusters similaires\n\nRéduction de dimensionnalité: trouver une représentation compacte des données\n\nEstimation de densité: modéliser la distribution p(x) des données","type":"content","url":"/#apprentissage-non-supervis","position":19},{"hierarchy":{"lvl1":"Introduction","lvl3":"Apprentissage par renforcement","lvl2":"Types d’apprentissage"},"type":"lvl3","url":"/#apprentissage-par-renforcement","position":20},{"hierarchy":{"lvl1":"Introduction","lvl3":"Apprentissage par renforcement","lvl2":"Types d’apprentissage"},"content":"Dans l’apprentissage par renforcement, un agent interagit avec un environnement et apprend à prendre des actions qui maximisent une récompense cumulative. Ce paradigme s’applique aux jeux (AlphaGo), à la robotique, et aux systèmes de recommandation. Ce livre ne couvre pas l’apprentissage par renforcement en détail.","type":"content","url":"/#apprentissage-par-renforcement","position":21},{"hierarchy":{"lvl1":"Introduction","lvl2":"Prérequis et ressources"},"type":"lvl2","url":"/#pr-requis-et-ressources","position":22},{"hierarchy":{"lvl1":"Introduction","lvl2":"Prérequis et ressources"},"content":"Ce livre suppose une familiarité avec:\n\nAlgèbre linéaire: vecteurs, matrices, produits, valeurs propres\n\nProbabilités: distributions, espérance, variance, théorème de Bayes\n\nCalcul différentiel: dérivées, gradients, règle de la chaîne\n\nProgrammation: Python, NumPy, matplotlib\n\nLes annexes fournissent des révisions de ce matériel.","type":"content","url":"/#pr-requis-et-ressources","position":23},{"hierarchy":{"lvl1":"Introduction","lvl3":"Ouvrages de référence","lvl2":"Prérequis et ressources"},"type":"lvl3","url":"/#ouvrages-de-r-f-rence","position":24},{"hierarchy":{"lvl1":"Introduction","lvl3":"Ouvrages de référence","lvl2":"Prérequis et ressources"},"content":"Murphy, K. Probabilistic Machine Learning: An Introduction (2022)\n\nHastie, Tibshirani, Friedman. The Elements of Statistical Learning (2009)\n\nBishop, C. Pattern Recognition and Machine Learning (2006)\n\nGoodfellow, Bengio, Courville. Deep Learning (2016)","type":"content","url":"/#ouvrages-de-r-f-rence","position":25},{"hierarchy":{"lvl1":"Introduction","lvl2":"Notation"},"type":"lvl2","url":"/#notation","position":26},{"hierarchy":{"lvl1":"Introduction","lvl2":"Notation"},"content":"Symbole\n\nSignification\n\nx, \\boldsymbol{x}\n\nScalaire, vecteur\n\n\\boldsymbol{X}\n\nMatrice\n\n\\theta, \\boldsymbol{\\theta}\n\nParamètres du modèle\n\n\\mathcal{D} = \\{(x_i, y_i)\\}\n\nEnsemble de données\n\n\\mathcal{H}\n\nClasse d’hypothèses\n\n\\ell(y, \\hat{y})\n\nFonction de perte\n\n\\mathcal{R}(f)\n\nRisque (vrai)\n\n\\hat{\\mathcal{R}}(f)\n\nRisque empirique\n\n\\mathbb{E}[\\cdot]\n\nEspérance\n\n\\mathbb{P}(\\cdot)\n\nProbabilité\n\n\\mathbb{1}_A\n\nIndicatrice de l’événement A","type":"content","url":"/#notation","position":27},{"hierarchy":{"lvl1":"Méthodes non paramétriques"},"type":"lvl1","url":"/knn","position":0},{"hierarchy":{"lvl1":"Méthodes non paramétriques"},"content":"Objectifs d’apprentissage\n\nÀ la fin de ce chapitre, vous serez en mesure de:\n\nDistinguer les méthodes paramétriques et non paramétriques\n\nExpliquer le fonctionnement de l’algorithme des k plus proches voisins\n\nDéfinir et appliquer différentes fonctions de distance\n\nAnalyser l’effet du paramètre k sur le compromis biais-variance\n\nExpliquer le fléau de la dimensionnalité et ses conséquences\n\nImplémenter l’algorithme k-ppv pour la classification et la régression\n\nEn apprentissage automatique, deux grandes familles d’approches s’opposent. Les méthodes paramétriques résument les données d’entraînement dans un ensemble fixe de paramètres: une fois l’apprentissage terminé, les données peuvent être jetées. Les méthodes non paramétriques conservent les données et les consultent directement au moment de la prédiction. Ce chapitre présente cette seconde famille, dont les k plus proches voisins sont l’exemple le plus simple.","type":"content","url":"/knn","position":1},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"L’idée de base"},"type":"lvl2","url":"/knn#lid-e-de-base","position":2},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"L’idée de base"},"content":"Soit \\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N un ensemble d’entraînement avec \\mathbf{x}_i \\in \\mathbb{R}^d et y_i \\in \\{1, \\ldots, C\\}. Nous voulons prédire l’étiquette d’un nouveau point \\mathbf{x}. L’approche la plus simple consiste à regarder les exemples connus qui ressemblent à \\mathbf{x} et à prédire la même chose.\n\nLes k plus proches voisins (k-ppv) formalisent cette intuition. Pour classifier \\mathbf{x}, nous identifions les k points de \\mathcal{D} les plus proches de \\mathbf{x} et prenons un vote majoritaire sur leurs étiquettes. La méthode ne fait aucune hypothèse sur la forme de la relation entre \\mathbf{x} et y. Elle se contente de consulter les données au moment de la prédiction.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate simple 2D data\nnp.random.seed(42)\nn_per_class = 15\n\n# Class 0: cluster around (-1, -1)\nX0 = np.random.randn(n_per_class, 2) * 0.6 + np.array([-1, -1])\n# Class 1: cluster around (1, 1)\nX1 = np.random.randn(n_per_class, 2) * 0.6 + np.array([1, 1])\n\nX_train = np.vstack([X0, X1])\ny_train = np.array([0] * n_per_class + [1] * n_per_class)\n\n# Query point\nx_query = np.array([0.3, 0.2])\nk = 5\n\n# Compute distances and find k nearest\ndistances = np.sqrt(np.sum((X_train - x_query)**2, axis=1))\nk_nearest_idx = np.argsort(distances)[:k]\n\nfig, ax = plt.subplots(figsize=(6, 5))\n\n# Plot training points\nax.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], \n           c='C0', s=60, label='Classe 0', zorder=2)\nax.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], \n           c='C1', s=60, label='Classe 1', zorder=2)\n\n# Highlight k nearest neighbors\nfor idx in k_nearest_idx:\n    ax.plot([x_query[0], X_train[idx, 0]], [x_query[1], X_train[idx, 1]], \n            'k--', alpha=0.4, linewidth=1, zorder=1)\n    ax.scatter(X_train[idx, 0], X_train[idx, 1], \n               s=150, facecolors='none', edgecolors='black', linewidths=2, zorder=3)\n\n# Plot query point\nax.scatter(x_query[0], x_query[1], c='red', s=120, marker='*', \n           label='Point requête', zorder=4)\n\n# Count votes\nvotes = y_train[k_nearest_idx]\nn_class0 = np.sum(votes == 0)\nn_class1 = np.sum(votes == 1)\nprediction = 0 if n_class0 > n_class1 else 1\n\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.legend(loc='upper left')\nax.set_title(f'$k = {k}$: votes = [{n_class0} classe 0, {n_class1} classe 1] → prédiction: classe {prediction}')\nax.set_aspect('equal')\nplt.tight_layout()\n\n\n\nSoit \\mathcal{N}_k(\\mathbf{x}) l’ensemble des indices des k plus proches voisins de \\mathbf{x}. La prédiction est:\\hat{y} = \\arg\\max_{c} \\sum_{i \\in \\mathcal{N}_k(\\mathbf{x})} \\mathbb{1}_{y_i = c}\n\nLa somme compte combien de voisins appartiennent à chaque classe c, et nous retenons la classe la plus fréquente.\n\nCette formulation admet une interprétation probabiliste. La proportion de voisins appartenant à la classe c estime la probabilité conditionnelle:p(y = c \\mid \\mathbf{x}, \\mathcal{D}) = \\frac{1}{k} \\sum_{i \\in \\mathcal{N}_k(\\mathbf{x})} \\mathbb{1}_{y_i = c}\n\nLa prédiction déterministe correspond au mode de cette distribution empirique.","type":"content","url":"/knn#lid-e-de-base","position":3},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"Fonctions de distance"},"type":"lvl2","url":"/knn#fonctions-de-distance","position":4},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"Fonctions de distance"},"content":"L’algorithme repose sur la capacité à mesurer la proximité entre points. Une fonction de distance d: \\mathcal{X} \\times \\mathcal{X} \\to [0, \\infty) doit satisfaire trois axiomes: d(\\mathbf{x}, \\mathbf{y}) = 0 si et seulement si \\mathbf{x} = \\mathbf{y} (identité), d(\\mathbf{x}, \\mathbf{y}) = d(\\mathbf{y}, \\mathbf{x}) (symétrie), et d(\\mathbf{x}, \\mathbf{z}) \\leq d(\\mathbf{x}, \\mathbf{y}) + d(\\mathbf{y}, \\mathbf{z}) (inégalité triangulaire).\n\nLa distance euclidienne est le choix le plus courant:d_2(\\mathbf{x}, \\mathbf{y}) = \\sqrt{\\sum_{j=1}^{d} (x_j - y_j)^2} = \\|\\mathbf{x} - \\mathbf{y}\\|_2\n\nLa distance de Manhattan suit les axes plutôt que la ligne droite:d_1(\\mathbf{x}, \\mathbf{y}) = \\sum_{j=1}^{d} |x_j - y_j| = \\|\\mathbf{x} - \\mathbf{y}\\|_1\n\nCes deux distances appartiennent à la famille des normes \\ell_p, définies par \\|\\mathbf{x}\\|_p = \\left(\\sum_j |x_j|^p\\right)^{1/p}. Le cas limite p \\to \\infty donne la norme \\ell_\\infty:\\|\\mathbf{x}\\|_\\infty = \\max_j |x_j|","type":"content","url":"/knn#fonctions-de-distance","position":5},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Boules unité et géométrie des normes","lvl2":"Fonctions de distance"},"type":"lvl3","url":"/knn#boules-unit-et-g-om-trie-des-normes","position":6},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Boules unité et géométrie des normes","lvl2":"Fonctions de distance"},"content":"Pour comprendre comment une norme mesure les distances, on trace sa boule unité. Formellement, la boule unité d’une norme \\|\\cdot\\| est l’ensemble:B = \\{\\mathbf{x} \\in \\mathbb{R}^d : \\|\\mathbf{x}\\| \\leq 1\\}\n\net sa frontière, la sphère unité, est S = \\{\\mathbf{x} : \\|\\mathbf{x}\\| = 1\\}. Tous les points sur cette sphère sont à distance exactement 1 de l’origine. La forme de la boule révèle ce que la norme considère comme “équidistant”.\n\nNorme \\ell_2 (cercle): Le point (1, 0) et le point (0.71, 0.71) sont à la même distance de l’origine. Se déplacer en diagonale coûte autant que suivre un axe. C’est notre intuition géométrique habituelle.\n\nNorme \\ell_1 (losange): Le point (1, 0) est à distance 1, mais (0.71, 0.71) est à distance 0.71 + 0.71 = 1.42. Se déplacer en diagonale coûte plus cher, comme un taxi qui ne peut tourner qu’aux intersections.\n\nNorme \\ell_\\infty (carré): Seule la plus grande coordonnée compte. Les points (1, 0), (1, 0.5) et (1, 1) sont tous à distance 1. C’est la distance du joueur d’échecs (le roi peut se déplacer d’une case dans n’importe quelle direction).\n\nPour les k-ppv, la forme de la boule détermine quels points sont considérés voisins. Avec \\ell_1, les voisins forment un losange autour de la requête; avec \\ell_\\infty, un carré.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 3, figsize=(11, 4))\n\n# Unit balls for different norms\ntheta = np.linspace(0, 2*np.pi, 1000)\n\n# Key points to highlight\np1 = (1, 0)\np2 = (1/np.sqrt(2), 1/np.sqrt(2))  # ≈ (0.71, 0.71)\n\n# L1 norm (diamond)\nax = axes[0]\nt = np.linspace(0, 1, 250)\nx_l1 = np.concatenate([t, 1-t, -t, -1+t])\ny_l1 = np.concatenate([1-t, -t, -1+t, t])\nax.fill(x_l1, y_l1, alpha=0.3, color='C0')\nax.plot(x_l1, y_l1, 'C0-', linewidth=2)\n\n# Show points - (1,0) is on boundary, (0.71, 0.71) is OUTSIDE\nax.scatter([p1[0]], [p1[1]], s=80, c='black', zorder=5)\nax.scatter([p2[0]], [p2[1]], s=80, c='red', zorder=5)\nax.annotate(f'$(1, 0)$\\n$d=1$', p1, textcoords='offset points', \n            xytext=(5, 10), fontsize=9)\nax.annotate(f'$(0.71, 0.71)$\\n$d=1.42$', p2, textcoords='offset points', \n            xytext=(5, 10), fontsize=9, color='red')\n\nax.set_title(r'Norme $\\ell_1$ (Manhattan)')\nax.set_xlim(-1.5, 1.5)\nax.set_ylim(-1.5, 1.5)\nax.set_aspect('equal')\nax.axhline(0, color='gray', linewidth=0.5)\nax.axvline(0, color='gray', linewidth=0.5)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\n\n# L2 norm (circle)\nax = axes[1]\nx_l2 = np.cos(theta)\ny_l2 = np.sin(theta)\nax.fill(x_l2, y_l2, alpha=0.3, color='C1')\nax.plot(x_l2, y_l2, 'C1-', linewidth=2)\n\n# Both points are on the boundary for L2\nax.scatter([p1[0]], [p1[1]], s=80, c='black', zorder=5)\nax.scatter([p2[0]], [p2[1]], s=80, c='black', zorder=5)\nax.annotate(f'$(1, 0)$\\n$d=1$', p1, textcoords='offset points', \n            xytext=(5, 10), fontsize=9)\nax.annotate(f'$(0.71, 0.71)$\\n$d=1$', p2, textcoords='offset points', \n            xytext=(5, 10), fontsize=9)\n\nax.set_title(r'Norme $\\ell_2$ (Euclidienne)')\nax.set_xlim(-1.5, 1.5)\nax.set_ylim(-1.5, 1.5)\nax.set_aspect('equal')\nax.axhline(0, color='gray', linewidth=0.5)\nax.axvline(0, color='gray', linewidth=0.5)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\n\n# L-infinity norm (square)\nax = axes[2]\nx_linf = np.array([1, 1, -1, -1, 1])\ny_linf = np.array([1, -1, -1, 1, 1])\nax.fill(x_linf, y_linf, alpha=0.3, color='C2')\nax.plot(x_linf, y_linf, 'C2-', linewidth=2)\n\n# For L-inf: (1,0), (1,0.5), (1,1) all have distance 1\np_inf = [(1, 0), (1, 0.5), (1, 1)]\nfor i, p in enumerate(p_inf):\n    ax.scatter([p[0]], [p[1]], s=80, c='black', zorder=5)\nax.annotate('$(1, 0)$\\n$d=1$', p_inf[0], textcoords='offset points', \n            xytext=(-45, -5), fontsize=9)\nax.annotate('$(1, 0.5)$\\n$d=1$', p_inf[1], textcoords='offset points', \n            xytext=(5, -5), fontsize=9)\nax.annotate('$(1, 1)$\\n$d=1$', p_inf[2], textcoords='offset points', \n            xytext=(5, 5), fontsize=9)\n\nax.set_title(r'Norme $\\ell_\\infty$')\nax.set_xlim(-1.5, 1.5)\nax.set_ylim(-1.5, 1.5)\nax.set_aspect('equal')\nax.axhline(0, color='gray', linewidth=0.5)\nax.axvline(0, color='gray', linewidth=0.5)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\n\nplt.tight_layout()\n\n\n\n","type":"content","url":"/knn#boules-unit-et-g-om-trie-des-normes","position":7},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Données numériques: normalisation et corrélation","lvl2":"Fonctions de distance"},"type":"lvl3","url":"/knn#donn-es-num-riques-normalisation-et-corr-lation","position":8},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Données numériques: normalisation et corrélation","lvl2":"Fonctions de distance"},"content":"La distance euclidienne traite toutes les dimensions de manière égale. Si les variables ont des échelles différentes, certaines domineront le calcul. Imaginons un problème où x_1 est l’âge (0-100) et x_2 est le revenu annuel (0-500000). Sans normalisation, la différence de revenu écrasera la différence d’âge.\n\nSolution pratique: normaliser les variables (soustraire la moyenne, diviser par l’écart-type) avant d’appliquer les k-ppv. C’est presque toujours nécessaire pour des données tabulaires.\n\nLa distance de Mahalanobis va plus loin en tenant compte des corrélations:d_M(\\mathbf{x}, \\mathbf{y}) = \\sqrt{(\\mathbf{x} - \\mathbf{y})^\\top \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{y})}\n\noù \\boldsymbol{\\Sigma} est la matrice de covariance des données. Pour comprendre cette formule, décomposons-la.\n\nLa matrice de covariance \\boldsymbol{\\Sigma}. Cette matrice d \\times d capture deux informations: sur la diagonale, les variances de chaque variable; hors diagonale, les covariances (corrélations) entre variables. Si \\boldsymbol{\\Sigma} = \\begin{pmatrix} 4 & 0 \\\\ 0 & 1 \\end{pmatrix}, la première variable a une variance 4 fois plus grande que la seconde, et elles sont indépendantes.\n\nCalcul concret. Soit \\mathbf{X} \\in \\mathbb{R}^{N \\times d} la matrice des données (chaque ligne est un exemple). On centre d’abord les données en soustrayant la moyenne de chaque colonne:\\bar{x}_j = \\frac{1}{N} \\sum_{i=1}^{N} x_{ij}, \\quad \\tilde{\\mathbf{X}} = \\mathbf{X} - \\mathbf{1} \\bar{\\mathbf{x}}^\\top\n\nLa covariance empirique est alors:\\boldsymbol{\\Sigma} = \\frac{1}{N-1} \\tilde{\\mathbf{X}}^\\top \\tilde{\\mathbf{X}}\n\nL’élément (j, k) de cette matrice est \\Sigma_{jk} = \\frac{1}{N-1} \\sum_{i=1}^{N} (x_{ij} - \\bar{x}_j)(x_{ik} - \\bar{x}_k). En Python:X_centered = X - X.mean(axis=0)\nSigma = (X_centered.T @ X_centered) / (len(X) - 1)\n# ou directement: Sigma = np.cov(X.T)\n\nL’inverse \\boldsymbol{\\Sigma}^{-1}. Multiplier par l’inverse de la covariance “blanchit” les données: les directions de forte variance sont comprimées, les corrélations sont supprimées. Après cette transformation, les données ressemblent à un nuage sphérique de variance unitaire.\n\nInterprétation géométrique. La distance de Mahalanobis mesure “à combien d’écarts-types” un point se trouve d’un autre, en tenant compte de la forme du nuage de données. Deux points éloignés dans une direction de forte variance sont considérés plus proches que deux points également éloignés dans une direction de faible variance.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Ellipse\n\nnp.random.seed(42)\n\n# Generate correlated 2D data\nn = 200\nmean = [0, 0]\ncov = [[2, 1.5], [1.5, 1.5]]  # Correlated, different variances\nX = np.random.multivariate_normal(mean, cov, n)\n\n# Compute sample covariance\nSigma = np.cov(X.T)\nSigma_inv = np.linalg.inv(Sigma)\n\n# A reference point and a query point\nref = np.array([0, 0])\nquery1 = np.array([2, 0])    # Along high-variance direction\nquery2 = np.array([-0.5, 1]) # Along low-variance direction\n\n# Compute distances\ndef euclidean(a, b):\n    return np.sqrt(np.sum((a - b)**2))\n\ndef mahalanobis(a, b, Sigma_inv):\n    diff = a - b\n    return np.sqrt(diff @ Sigma_inv @ diff)\n\nd_euc1 = euclidean(ref, query1)\nd_euc2 = euclidean(ref, query2)\nd_mah1 = mahalanobis(ref, query1, Sigma_inv)\nd_mah2 = mahalanobis(ref, query2, Sigma_inv)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Left: Euclidean view\nax = axes[0]\nax.scatter(X[:, 0], X[:, 1], alpha=0.3, s=20, c='gray')\nax.scatter([ref[0]], [ref[1]], s=100, c='black', zorder=5, label='Référence')\nax.scatter([query1[0]], [query1[1]], s=100, c='C0', zorder=5, marker='s')\nax.scatter([query2[0]], [query2[1]], s=100, c='C1', zorder=5, marker='^')\n\n# Draw circles for Euclidean distance\ncircle1 = plt.Circle(ref, d_euc1, fill=False, color='C0', linestyle='--', linewidth=2)\ncircle2 = plt.Circle(ref, d_euc2, fill=False, color='C1', linestyle='--', linewidth=2)\nax.add_patch(circle1)\nax.add_patch(circle2)\n\nax.annotate(f'$d_{{euc}} = {d_euc1:.2f}$', query1, textcoords='offset points', \n            xytext=(10, 10), fontsize=10, color='C0')\nax.annotate(f'$d_{{euc}} = {d_euc2:.2f}$', query2, textcoords='offset points', \n            xytext=(10, 10), fontsize=10, color='C1')\n\nax.set_xlim(-4, 4)\nax.set_ylim(-4, 4)\nax.set_aspect('equal')\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_title('Distance Euclidienne\\n(ignore la structure des données)')\n\n# Right: Mahalanobis view\nax = axes[1]\nax.scatter(X[:, 0], X[:, 1], alpha=0.3, s=20, c='gray')\nax.scatter([ref[0]], [ref[1]], s=100, c='black', zorder=5, label='Référence')\nax.scatter([query1[0]], [query1[1]], s=100, c='C0', zorder=5, marker='s')\nax.scatter([query2[0]], [query2[1]], s=100, c='C1', zorder=5, marker='^')\n\n# Draw ellipses for Mahalanobis distance (iso-distance contours)\n# Eigendecomposition for ellipse orientation\neigenvalues, eigenvectors = np.linalg.eigh(Sigma)\nangle = np.degrees(np.arctan2(eigenvectors[1, 1], eigenvectors[0, 1]))\n\nfor d_mah, color in [(d_mah1, 'C0'), (d_mah2, 'C1')]:\n    width = 2 * d_mah * np.sqrt(eigenvalues[1])\n    height = 2 * d_mah * np.sqrt(eigenvalues[0])\n    ellipse = Ellipse(ref, width, height, angle=angle, fill=False, \n                      color=color, linestyle='--', linewidth=2)\n    ax.add_patch(ellipse)\n\nax.annotate(f'$d_{{mah}} = {d_mah1:.2f}$', query1, textcoords='offset points', \n            xytext=(10, 10), fontsize=10, color='C0')\nax.annotate(f'$d_{{mah}} = {d_mah2:.2f}$', query2, textcoords='offset points', \n            xytext=(10, 10), fontsize=10, color='C1')\n\nax.set_xlim(-4, 4)\nax.set_ylim(-4, 4)\nax.set_aspect('equal')\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_title('Distance de Mahalanobis\\n(tient compte de la covariance)')\n\nplt.tight_layout()\n\n\n\nDans cet exemple, le point bleu (carré) est plus loin en distance euclidienne, mais plus proche en distance de Mahalanobis. Cela s’explique par le fait qu’il se trouve dans la direction où les données varient naturellement. Le point orange (triangle), bien que plus proche en euclidien, est “surprenant” par rapport à la distribution et donc plus loin en Mahalanobis.\n\nEn pratique, on utilise rarement Mahalanobis directement pour les k-ppv. La normalisation standard (centrer et réduire chaque variable) capture l’essentiel. Mahalanobis devient utile quand les corrélations entre variables sont fortes et informatives.","type":"content","url":"/knn#donn-es-num-riques-normalisation-et-corr-lation","position":9},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Au-delà des vecteurs numériques","lvl2":"Fonctions de distance"},"type":"lvl3","url":"/knn#au-del-des-vecteurs-num-riques","position":10},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Au-delà des vecteurs numériques","lvl2":"Fonctions de distance"},"content":"Les k-ppv ne se limitent pas aux vecteurs dans \\mathbb{R}^d. Toute fonction de distance valide permet d’appliquer l’algorithme.\n\nChaînes de caractères et ADN. La distance d’édition (ou distance de Levenshtein) compte le nombre minimum d’opérations (insertion, suppression, substitution) pour transformer une chaîne en une autre:d_{\\text{edit}}(\\texttt{\"chat\"}, \\texttt{\"chien\"}) = 3\n\nCette distance est utilisée pour la correction orthographique, l’alignement de séquences ADN, et la détection de plagiat. Pour comparer des séquences génétiques, on peut aussi utiliser des distances spécialisées qui tiennent compte de la biologie des mutations.\n\nDocuments et texte. Pour comparer des documents, on les représente souvent comme des vecteurs de fréquences de mots (bag-of-words). La similarité cosinus mesure l’angle entre ces vecteurs:\\text{sim}_{\\cos}(\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|}\n\nOn convertit en distance par d = 1 - \\text{sim}_{\\cos}. Cette mesure ignore la longueur des documents et se concentre sur leur contenu thématique. C’est le choix standard pour la recherche d’information et la classification de texte.\n\nEnsembles et données binaires. Pour des données représentées comme des ensembles (mots-clés, tags, gènes exprimés), la distance de Jaccard mesure le chevauchement:d_{\\text{Jaccard}}(A, B) = 1 - \\frac{|A \\cap B|}{|A \\cup B|}\n\nDeux documents partageant 80% de leurs mots-clés ont une distance de 0.2. Pour des vecteurs binaires (présence/absence), la distance de Hamming compte les positions différentes.\n\nImages. Les pixels bruts donnent des distances peu informatives. Par exemple, deux images du même objet décalé d’un pixel seraient très “différentes”. En pratique, on extrait des représentations (embeddings) via des réseaux de neurones pré-entraînés, puis on applique la distance euclidienne ou cosinus dans cet espace de représentation.","type":"content","url":"/knn#au-del-des-vecteurs-num-riques","position":11},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Guide pratique: quelle distance choisir?","lvl2":"Fonctions de distance"},"type":"lvl3","url":"/knn#guide-pratique-quelle-distance-choisir","position":12},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Guide pratique: quelle distance choisir?","lvl2":"Fonctions de distance"},"content":"Type de données\n\nDistance recommandée\n\nPourquoi\n\nVecteurs numériques\n\nEuclidienne (après normalisation)\n\nSimple, efficace, interprétable\n\nDonnées avec corrélations fortes\n\nMahalanobis ou PCA + Euclidienne\n\nTient compte de la structure\n\nTexte / documents\n\nCosinus sur TF-IDF ou embeddings\n\nInvariant à la longueur\n\nSéquences (ADN, protéines)\n\nDistance d’édition ou alignement\n\nCapture les mutations/insertions\n\nEnsembles, tags\n\nJaccard\n\nMesure le chevauchement\n\nVecteurs binaires\n\nHamming\n\nCompte les différences\n\nImages\n\nCosinus sur embeddings CNN\n\nLes pixels bruts sont peu informatifs\n\nLe choix de la distance encode vos hypothèses. Si deux clients ayant acheté les mêmes produits sont “similaires”, utilisez Jaccard sur les paniers. Si deux clients ayant dépensé des montants similaires sont “proches”, utilisez la distance euclidienne sur les dépenses. La distance définit ce que “voisin” signifie: c’est une décision de modélisation, pas un détail technique.","type":"content","url":"/knn#guide-pratique-quelle-distance-choisir","position":13},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"L’effet du paramètre k"},"type":"lvl2","url":"/knn#leffet-du-param-tre-k","position":14},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"L’effet du paramètre k"},"content":"Le paramètre k contrôle la complexité du modèle. Avec k = 1, chaque point est classifié selon son plus proche voisin. La frontière de décision est très irrégulière et s’adapte étroitement aux données. L’erreur d’entraînement est exactement zéro: chaque point est son propre plus proche voisin. Mais cette adaptation excessive aux données d’entraînement nuit à la généralisation.\n\nAvec un grand k, la prédiction moyenne sur plus de voisins et la frontière devient plus lisse. Le cas extrême k = N prédit toujours la classe majoritaire globale, ignorant complètement l’entrée.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n# Generate 2D classification data\nnp.random.seed(0)\nn_samples = 100\n\n# Two interleaved half circles\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=n_samples, noise=0.25, random_state=42)\n\ndef knn_predict(X_train, y_train, X_test, k):\n    predictions = []\n    for x in X_test:\n        distances = np.sqrt(np.sum((X_train - x)**2, axis=1))\n        k_nearest_idx = np.argsort(distances)[:k]\n        k_nearest_labels = y_train[k_nearest_idx]\n        predictions.append(np.round(np.mean(k_nearest_labels)))\n    return np.array(predictions)\n\n# Create mesh for decision boundary\nx_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\ny_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 150),\n                     np.linspace(y_min, y_max, 150))\nX_mesh = np.c_[xx.ravel(), yy.ravel()]\n\nfig, axes = plt.subplots(1, 3, figsize=(12, 4))\nk_values = [1, 5, 50]\ncmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#0000FF'])\n\nfor ax, k in zip(axes, k_values):\n    Z = knn_predict(X, y, X_mesh, k)\n    Z = Z.reshape(xx.shape)\n    \n    ax.contourf(xx, yy, Z, alpha=0.4, cmap=cmap_light)\n    ax.scatter(X[y == 0, 0], X[y == 0, 1], c='C0', s=30, edgecolors='k', linewidths=0.5)\n    ax.scatter(X[y == 1, 0], X[y == 1, 1], c='C1', s=30, edgecolors='k', linewidths=0.5)\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_title(f'$k = {k}$')\n    ax.set_xlabel('$x_1$')\n    ax.set_ylabel('$x_2$')\n\nplt.tight_layout()\n\n\n\nEntre ces deux extrêmes se trouve le compromis biais-variance. Un petit k donne un modèle à faible biais mais haute variance: les prédictions sont sensibles aux fluctuations des données. Un grand k donne un modèle à haute biais mais faible variance: les prédictions sont stables mais peuvent manquer des structures locales.\n\nLe choix de k se fait par validation. On trace l’erreur sur un ensemble de validation en fonction de k et on retient la valeur qui minimise cette erreur. Des valeurs impaires évitent les égalités dans les votes.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\n# Generate a larger dataset for smoother curves\nn_samples = 500\nX = np.random.randn(n_samples, 2)\n# Create a non-linear boundary\ny = ((X[:, 0]**2 + X[:, 1]**2) > 1.5).astype(int)\n# Add label noise\nnoise_idx = np.random.choice(n_samples, size=int(0.1 * n_samples), replace=False)\ny[noise_idx] = 1 - y[noise_idx]\n\n# Split into train and test\nn_train = 350\nX_train, X_test = X[:n_train], X[n_train:]\ny_train, y_test = y[:n_train], y[n_train:]\n\ndef knn_predict(X_train, y_train, X_test, k):\n    predictions = []\n    for x in X_test:\n        distances = np.sqrt(np.sum((X_train - x)**2, axis=1))\n        k_nearest_idx = np.argsort(distances)[:k]\n        k_nearest_labels = y_train[k_nearest_idx]\n        predictions.append(np.round(np.mean(k_nearest_labels)))\n    return np.array(predictions)\n\n# Compute train and test error for different k\n# Use 1/k on x-axis (model complexity decreases as k increases)\nk_values = list(range(1, 100, 2))  # Odd values to avoid ties\ntrain_errors = []\ntest_errors = []\n\nfor k in k_values:\n    y_pred_train = knn_predict(X_train, y_train, X_train, k)\n    y_pred_test = knn_predict(X_train, y_train, X_test, k)\n    train_errors.append(np.mean(y_pred_train != y_train))\n    test_errors.append(np.mean(y_pred_test != y_test))\n\nfig, ax = plt.subplots(figsize=(8, 4.5))\n\nax.plot(k_values, train_errors, 'C0-', linewidth=2, label='Erreur entraînement')\nax.plot(k_values, test_errors, 'C1-', linewidth=2, label='Erreur test')\n\n# Mark optimal k\nbest_idx = np.argmin(test_errors)\nbest_k = k_values[best_idx]\nax.scatter([best_k], [test_errors[best_idx]], s=100, c='C1', zorder=5, edgecolors='black')\nax.axvline(best_k, color='gray', linestyle='--', alpha=0.5)\n\n# Add annotations for the regions - positioned to avoid overlapping curves\nax.text(10, 0.45, 'Surapprentissage\\n(haute variance)', fontsize=10, \n        ha='center', va='top', color='gray', alpha=0.8)\nax.text(85, 0.1, 'Sous-apprentissage\\n(haut biais)', fontsize=10, \n        ha='center', va='bottom', color='gray', alpha=0.8)\n\n# Mark optimal k with an arrow from above\nax.annotate(f'Meilleur $k = {best_k}$', xy=(best_k, test_errors[best_idx]), \n            xytext=(best_k, test_errors[best_idx] + 0.1),\n            fontsize=10, ha='center',\n            arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0', color='black'))\n\nax.set_xlabel('$k$ (nombre de voisins)')\nax.set_ylabel('Taux d\\'erreur')\nax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2, frameon=False)\nax.set_xlim(0, 100)\nax.set_ylim(0, 0.5)\n\n# Add complexity arrow at the bottom\nax.text(0.5, -0.15, r'$\\longleftarrow$ complexité du modèle', transform=ax.transAxes, \n        ha='center', va='top', fontsize=9, color='gray')\n\nplt.tight_layout()\n\n\n\n","type":"content","url":"/knn#leffet-du-param-tre-k","position":15},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"Diagramme de Voronoï"},"type":"lvl2","url":"/knn#diagramme-de-vorono","position":16},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"Diagramme de Voronoï"},"content":"Le cas k = 1 induit une partition de l’espace en cellules. La cellule V_i associée au point \\mathbf{x}_i contient tous les points plus proches de \\mathbf{x}_i que de tout autre point d’entraînement:V_i = \\{\\mathbf{x} \\in \\mathbb{R}^d : d(\\mathbf{x}, \\mathbf{x}_i) \\leq d(\\mathbf{x}, \\mathbf{x}_j) \\text{ pour tout } j \\neq i\\}\n\nCette partition s’appelle le diagramme de Voronoï. Les frontières entre cellules sont des hyperplans en dimension d. Avec le 1-ppv, la frontière de décision suit exactement ce diagramme.","type":"content","url":"/knn#diagramme-de-vorono","position":17},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Stabilité et marge géométrique","lvl2":"Diagramme de Voronoï"},"type":"lvl3","url":"/knn#stabilit-et-marge-g-om-trique","position":18},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Stabilité et marge géométrique","lvl2":"Diagramme de Voronoï"},"content":"Le diagramme de Voronoï donne une lecture géométrique de la stabilité des prédictions. Tant qu’une requête \\mathbf{x} reste dans la même cellule V_i, sa prédiction ne change pas. La distance à la frontière de la cellule mesure donc la robustesse de la prédiction.\n\nPour quantifier cette robustesse, considérons l’écart entre les distances au premier et au deuxième plus proche voisin:\\rho(\\mathbf{x}) = \\frac{1}{2}\\left(d(\\mathbf{x}, \\mathbf{x}_{(2)}) - d(\\mathbf{x}, \\mathbf{x}_{(1)})\\right)\n\noù \\mathbf{x}_{(1)} et \\mathbf{x}_{(2)} sont les premier et deuxième plus proches voisins. Si \\rho(\\mathbf{x}) est grand, le point \\mathbf{x} est loin de toute frontière: sa prédiction est stable. Si \\rho(\\mathbf{x}) est proche de zéro, \\mathbf{x} est sur une frontière: la moindre perturbation (bruit de mesure, erreur d’arrondi) peut changer la prédiction.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\n\n# Generate points with clear class structure\nnp.random.seed(42)\nn_per_class = 8\nX0 = np.random.randn(n_per_class, 2) * 0.8 + np.array([-1.5, 0])\nX1 = np.random.randn(n_per_class, 2) * 0.8 + np.array([1.5, 0])\nX = np.vstack([X0, X1])\ny = np.array([0] * n_per_class + [1] * n_per_class)\n\n# Compute Voronoi diagram\nvor = Voronoi(X)\n\n# Create test points and compute their margin\nnp.random.seed(123)\ntest_points = np.random.randn(5, 2) * 1.5\n\ndef compute_margin(x, X_train):\n    \"\"\"Compute margin: half the difference between 2nd and 1st neighbor distances.\"\"\"\n    distances = np.sqrt(np.sum((X_train - x)**2, axis=1))\n    sorted_dist = np.sort(distances)\n    return 0.5 * (sorted_dist[1] - sorted_dist[0])\n\nmargins = [compute_margin(p, X) for p in test_points]\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Plot Voronoi regions with colors based on class\nfrom matplotlib.patches import Polygon, Circle\n\nfor region_idx, point_idx in enumerate(vor.point_region):\n    region = vor.regions[point_idx]\n    if -1 not in region and len(region) > 0:\n        polygon = [vor.vertices[i] for i in region]\n        poly = Polygon(polygon, alpha=0.2, \n                      facecolor='C0' if y[region_idx] == 0 else 'C1',\n                      edgecolor='gray', linewidth=0.5)\n        ax.add_patch(poly)\n\n# Plot Voronoi edges\nvoronoi_plot_2d(vor, ax=ax, show_vertices=False, show_points=False, \n                line_colors='gray', line_width=1, line_alpha=0.6)\n\n# Plot training points\nax.scatter(X[y == 0, 0], X[y == 0, 1], c='C0', s=80, edgecolors='k', \n           linewidths=1, zorder=5, label='Classe 0')\nax.scatter(X[y == 1, 0], X[y == 1, 1], c='C1', s=80, edgecolors='k', \n           linewidths=1, zorder=5, label='Classe 1')\n\n# Plot test points with margin circles\nfor i, (p, m) in enumerate(zip(test_points, margins)):\n    # Color based on stability: green = stable (large margin), red = unstable\n    color = 'green' if m > 0.3 else 'orange' if m > 0.1 else 'red'\n    ax.scatter(p[0], p[1], c=color, s=100, marker='*', edgecolors='k', \n               linewidths=0.5, zorder=6)\n    # Draw circle showing margin\n    circle = Circle(p, m, fill=False, color=color, linestyle='--', linewidth=1.5, alpha=0.7)\n    ax.add_patch(circle)\n\nax.set_xlim(-4, 4)\nax.set_ylim(-3, 3)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.legend(loc='upper left')\nax.set_aspect('equal')\nax.set_title(r'Marge $\\rho(x)$: distance à la frontière (cercles pointillés)')\nplt.tight_layout()\n\n\n\nLes étoiles représentent des points de test. Le cercle pointillé autour de chaque point montre sa marge \\rho(x): tant que le point reste dans ce cercle, sa prédiction est garantie stable. Les points verts ont une grande marge (prédiction robuste), les points rouges sont près d’une frontière (prédiction fragile).","type":"content","url":"/knn#stabilit-et-marge-g-om-trique","position":19},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Quels points définissent la frontière?","lvl2":"Diagramme de Voronoï"},"type":"lvl3","url":"/knn#quels-points-d-finissent-la-fronti-re","position":20},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Quels points définissent la frontière?","lvl2":"Diagramme de Voronoï"},"content":"La frontière de décision du 1-ppv est constituée des faces de Voronoï séparant des points d’étiquettes différentes. Les faces entre points de même classe ne contribuent pas à la frontière de décision.\n\nCette observation a une conséquence pratique importante: seuls les points proches de la frontière “comptent vraiment” pour le classifieur. Les points bien à l’intérieur d’une région homogène (entourés uniquement de points de même classe) pourraient être retirés sans changer significativement les prédictions.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import Voronoi\n\nnp.random.seed(42)\nn_per_class = 8\nX0 = np.random.randn(n_per_class, 2) * 0.8 + np.array([-1.5, 0])\nX1 = np.random.randn(n_per_class, 2) * 0.8 + np.array([1.5, 0])\nX = np.vstack([X0, X1])\ny = np.array([0] * n_per_class + [1] * n_per_class)\n\nvor = Voronoi(X)\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Find which edges are on the decision boundary (between different classes)\n# For each ridge, check if the two points have different labels\nboundary_edges = []\ninternal_edges = []\n\nfor (p1, p2), (v1, v2) in zip(vor.ridge_points, vor.ridge_vertices):\n    if v1 >= 0 and v2 >= 0:  # Both vertices are finite\n        if y[p1] != y[p2]:\n            boundary_edges.append((vor.vertices[v1], vor.vertices[v2]))\n        else:\n            internal_edges.append((vor.vertices[v1], vor.vertices[v2]))\n\n# Plot internal edges (light gray)\nfor v1, v2 in internal_edges:\n    ax.plot([v1[0], v2[0]], [v1[1], v2[1]], 'gray', linewidth=0.8, alpha=0.4)\n\n# Plot boundary edges (thick black)\nfor v1, v2 in boundary_edges:\n    ax.plot([v1[0], v2[0]], [v1[1], v2[1]], 'k-', linewidth=2.5)\n\n# Identify boundary vs interior points\n# A point is on the boundary if it shares a Voronoi edge with a point of different class\nboundary_points = set()\nfor p1, p2 in vor.ridge_points:\n    if y[p1] != y[p2]:\n        boundary_points.add(p1)\n        boundary_points.add(p2)\n\ninterior_points = set(range(len(X))) - boundary_points\n\n# Plot points: boundary points larger, interior points smaller and faded\nfor i in range(len(X)):\n    if i in boundary_points:\n        color = 'C0' if y[i] == 0 else 'C1'\n        ax.scatter(X[i, 0], X[i, 1], c=color, s=120, edgecolors='k', \n                   linewidths=2, zorder=5)\n    else:\n        color = 'C0' if y[i] == 0 else 'C1'\n        ax.scatter(X[i, 0], X[i, 1], c=color, s=60, edgecolors='gray', \n                   linewidths=1, alpha=0.5, zorder=4)\n\nax.set_xlim(-4, 4)\nax.set_ylim(-3, 3)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_aspect('equal')\nax.set_title('Points définissant la frontière (grands) vs points intérieurs (petits, estompés)')\nplt.tight_layout()\n\n\n\nCette idée motive les variantes condensées et éditées des k-ppv:\n\nCondensed Nearest Neighbor: ne garder que les points nécessaires pour préserver la frontière de décision\n\nEdited Nearest Neighbor: retirer les points mal classifiés par leurs voisins (probablement du bruit)\n\nCes techniques réduisent la taille du jeu de données stocké, accélérant l’inférence sans dégrader significativement la performance.","type":"content","url":"/knn#quels-points-d-finissent-la-fronti-re","position":21},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Du concept au calcul","lvl2":"Diagramme de Voronoï"},"type":"lvl3","url":"/knn#du-concept-au-calcul","position":22},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Du concept au calcul","lvl2":"Diagramme de Voronoï"},"content":"Le diagramme de Voronoï est la structure géométrique idéale pour répondre à la question “quel est le plus proche voisin?”. En dimension 2 ou 3, on peut le construire efficacement et l’utiliser pour des requêtes en temps logarithmique.\n\nMais en grande dimension (d > 20), la construction devient prohibitive: le nombre de faces croît exponentiellement avec d. C’est pourquoi nous utilisons des structures approximatives (arbres k-d, graphes de proximité, hachage) qui renoncent à la perfection géométrique en échange de l’efficacité. Le diagramme de Voronoï reste utile conceptuellement: il nous dit ce que nous voudrions calculer, même si nous devons nous contenter d’approximations.","type":"content","url":"/knn#du-concept-au-calcul","position":23},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"Le fléau de la dimensionnalité"},"type":"lvl2","url":"/knn#le-fl-au-de-la-dimensionnalit","position":24},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"Le fléau de la dimensionnalité"},"content":"Les k-ppv fonctionnent bien en basse dimension mais souffrent en haute dimension. Ce phénomène, le fléau de la dimensionnalité, affecte toutes les méthodes basées sur la localité. Le problème n’est pas principalement calculatoire: le coût O(Nd) croît linéairement en d, mais il est fondamentalement statistique.","type":"content","url":"/knn#le-fl-au-de-la-dimensionnalit","position":25},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"La notion de voisinage s’effondre","lvl2":"Le fléau de la dimensionnalité"},"type":"lvl3","url":"/knn#la-notion-de-voisinage-seffondre","position":26},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"La notion de voisinage s’effondre","lvl2":"Le fléau de la dimensionnalité"},"content":"Considérons des points uniformément distribués dans [0, 1]^d. Pour capturer une fraction p des points dans un hypercube, le côté doit être r = p^{1/d}. En dimension 1, capturer 10% des points requiert un intervalle de longueur 0.1. En dimension 100, il faut un hypercube de côté 0.1^{1/100} \\approx 0.98, couvrant presque tout l’espace.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\np = 0.1  # fraction of points to capture\ndimensions = np.arange(1, 101)\nside_length = p ** (1 / dimensions)\n\nfig, ax = plt.subplots(figsize=(7, 4))\nax.plot(dimensions, side_length, 'C0-', linewidth=2)\nax.axhline(1, color='gray', linestyle='--', alpha=0.5)\n\n# Mark specific points\nfor d in [1, 2, 10, 50, 100]:\n    r = p ** (1/d)\n    ax.plot(d, r, 'ko', markersize=6)\n    if d == 1:\n        ax.annotate(f'd={d}\\nr={r:.2f}', (d, r), textcoords='offset points', \n                   xytext=(10, -15), fontsize=9)\n    elif d == 100:\n        ax.annotate(f'd={d}\\nr={r:.2f}', (d, r), textcoords='offset points', \n                   xytext=(-40, -20), fontsize=9)\n\nax.set_xlabel('Dimension $d$')\nax.set_ylabel('Côté de l\\'hypercube $r$')\nax.set_title(f'Côté nécessaire pour capturer {int(p*100)}% des points: $r = p^{{1/d}}$')\nax.set_xlim(0, 105)\nax.set_ylim(0, 1.1)\nplt.tight_layout()\n\n\n\n","type":"content","url":"/knn#la-notion-de-voisinage-seffondre","position":27},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Concentration des distances","lvl2":"Le fléau de la dimensionnalité"},"type":"lvl3","url":"/knn#concentration-des-distances","position":28},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Concentration des distances","lvl2":"Le fléau de la dimensionnalité"},"content":"En haute dimension, un phénomène contre-intuitif se produit: les distances entre points se concentrent autour d’une même valeur. Le ratio entre la distance au plus proche voisin et au plus éloigné tend vers 1:\\frac{d_{\\min}}{d_{\\max}} \\xrightarrow{d \\to \\infty} 1\n\nTous les points deviennent approximativement équidistants. La notion même de “plus proche voisin” perd son sens: si tous les points sont à la même distance, lequel choisir?","type":"content","url":"/knn#concentration-des-distances","position":29},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Conséquences sur le biais","lvl2":"Le fléau de la dimensionnalité"},"type":"lvl3","url":"/knn#cons-quences-sur-le-biais","position":30},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Conséquences sur le biais","lvl2":"Le fléau de la dimensionnalité"},"content":"Rappelons que les k-ppv estiment \\mathbb{E}[Y \\mid X = x] par une moyenne locale. En haute dimension, cette moyenne n’est plus locale car elle inclut des points qui, bien que “voisins” au sens de la distance, peuvent être très différents de x dans l’espace d’entrée. Le biais augmente: nous moyennons sur des régions trop vastes pour capturer les variations locales de la fonction cible.\n\nPour maintenir une densité constante de voisins dans une boule de rayon fixe, le nombre d’exemples requis croît exponentiellement avec la dimension: N \\propto r^{-d}. Avec 1000 points en dimension 10, la densité locale est déjà très faible.","type":"content","url":"/knn#cons-quences-sur-le-biais","position":31},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Remèdes","lvl2":"Le fléau de la dimensionnalité"},"type":"lvl3","url":"/knn#rem-des","position":32},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Remèdes","lvl2":"Le fléau de la dimensionnalité"},"content":"La réduction de dimension (PCA, autoencodeurs) projette les données dans un espace de plus basse dimension avant d’appliquer les k-ppv. Les distances adaptatives comme Mahalanobis peuvent aider si certaines dimensions sont plus informatives. Mais fondamentalement, les méthodes de voisinage ne sont pas adaptées aux problèmes en très haute dimension, ce qui motive l’étude des méthodes paramétriques dans les chapitres suivants.","type":"content","url":"/knn#rem-des","position":33},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"Régression"},"type":"lvl2","url":"/knn#r-gression","position":34},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"Régression"},"content":"En classification, nous avons combiné les étiquettes des voisins par vote majoritaire. Pour la régression, où y_i \\in \\mathbb{R}, la combinaison naturelle est une moyenne:\\hat{y} = \\frac{1}{k} \\sum_{i \\in \\mathcal{N}_k(\\mathbf{x})} y_i\n\nCette moyenne locale estime l’espérance conditionnelle \\mathbb{E}[Y \\mid \\mathbf{X} = \\mathbf{x}]. L’intuition est simple: si nous voulons prédire la température demain et que nous avons des données historiques, regarder les jours passés qui ressemblaient à aujourd’hui et moyenner leurs températures du lendemain semble raisonnable.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate 1D regression data\nnp.random.seed(42)\nn = 50\nX_train = np.sort(np.random.uniform(0, 10, n))\ny_train = np.sin(X_train) + np.random.randn(n) * 0.3\n\ndef knn_regression(X_train, y_train, X_test, k):\n    predictions = []\n    for x in X_test:\n        distances = np.abs(X_train - x)\n        k_nearest_idx = np.argsort(distances)[:k]\n        predictions.append(np.mean(y_train[k_nearest_idx]))\n    return np.array(predictions)\n\nX_test = np.linspace(0, 10, 200)\n\nfig, axes = plt.subplots(1, 3, figsize=(12, 3.5))\nk_values = [1, 5, 15]\n\nfor ax, k in zip(axes, k_values):\n    y_pred = knn_regression(X_train, y_train, X_test, k)\n    \n    ax.scatter(X_train, y_train, c='C0', s=30, alpha=0.6, label='Données')\n    ax.plot(X_test, y_pred, 'C1-', linewidth=2, label=f'k-ppv ($k={k}$)')\n    ax.plot(X_test, np.sin(X_test), 'k--', alpha=0.5, label=r'$\\sin(x)$')\n    ax.set_xlabel('$x$')\n    ax.set_ylabel('$y$')\n    ax.set_title(f'$k = {k}$')\n    ax.legend(loc='upper right', fontsize=8)\n    ax.set_xlim(0, 10)\n\nplt.tight_layout()\n\n\n\nAvec k = 1, la prédiction saute d’un point à l’autre, créant une fonction en escalier. Augmenter k lisse la prédiction, mais un k trop grand écrase les variations locales.","type":"content","url":"/knn#r-gression","position":35},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"Estimation de densité par noyaux"},"type":"lvl2","url":"/knn#estimation-de-densit-par-noyaux","position":36},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"Estimation de densité par noyaux"},"content":"Avant d’aller plus loin dans la régression, introduisons un outil fondamental: l’estimation de densité par noyaux (kernel density estimation, ou fenêtres de Parzen).\n\nSupposons que nous observons des points x_1, \\ldots, x_N tirés d’une densité inconnue p(x). Comment estimer cette densité? Une approche naïve serait de construire un histogramme, mais les histogrammes dépendent du choix arbitraire des intervalles et produisent des estimations discontinues.\n\nL’idée des fenêtres de Parzen est de placer un petit “noyau” sur chaque observation et de sommer ces contributions:\\hat{p}(x) = \\frac{1}{N} \\sum_{i=1}^{N} K_\\lambda(x - x_i)\n\nLe noyau K_\\lambda est une fonction qui satisfait \\int K_\\lambda(u) \\, du = 1 et K_\\lambda(u) = K_\\lambda(-u). Le paramètre \\lambda contrôle la largeur de bande (bandwidth): plus \\lambda est grand, plus le noyau est étalé, plus l’estimation est lisse.\n\nLe noyau le plus courant est le noyau gaussien:K_\\lambda(u) = \\frac{1}{\\lambda \\sqrt{2\\pi}} \\exp\\left(-\\frac{u^2}{2\\lambda^2}\\right)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate samples from a mixture of Gaussians\nnp.random.seed(42)\nn = 100\nsamples = np.concatenate([\n    np.random.randn(n//2) * 0.5 + 2,\n    np.random.randn(n//2) * 0.8 + 5\n])\n\ndef gaussian_kernel(u, bandwidth):\n    return np.exp(-u**2 / (2 * bandwidth**2)) / (bandwidth * np.sqrt(2 * np.pi))\n\ndef kde(x_query, samples, bandwidth):\n    density = np.zeros_like(x_query)\n    for xi in samples:\n        density += gaussian_kernel(x_query - xi, bandwidth)\n    return density / len(samples)\n\nx = np.linspace(-1, 9, 500)\n\nfig, axes = plt.subplots(1, 3, figsize=(12, 3.5))\nbandwidths = [0.2, 0.5, 1.5]\n\nfor ax, bw in zip(axes, bandwidths):\n    density = kde(x, samples, bw)\n    \n    ax.fill_between(x, density, alpha=0.3, color='C0')\n    ax.plot(x, density, 'C0-', linewidth=2, label=f'KDE ($\\\\lambda={bw}$)')\n    ax.scatter(samples, np.zeros_like(samples) - 0.02, c='k', s=10, alpha=0.5, marker='|')\n    ax.set_xlabel('$x$')\n    ax.set_ylabel(r'$\\hat{p}(x)$')\n    ax.set_title(f'Largeur de bande $\\\\lambda = {bw}$')\n    ax.set_xlim(-1, 9)\n    ax.set_ylim(-0.05, 0.6)\n\nplt.tight_layout()\n\n\n\nAvec une petite largeur de bande (\\lambda = 0.2), chaque observation crée un pic distinct et l’estimation est très variable. Avec une grande largeur de bande (\\lambda = 1.5), les détails sont perdus et la structure bimodale des données est masquée. Le choix de \\lambda incarne encore une fois le compromis biais-variance.","type":"content","url":"/knn#estimation-de-densit-par-noyaux","position":37},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"Régression de Nadaraya-Watson"},"type":"lvl2","url":"/knn#r-gression-de-nadaraya-watson","position":38},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"Régression de Nadaraya-Watson"},"content":"L’estimation de densité par noyaux mène naturellement à une forme de régression plus souple que les k-ppv. Plutôt que de sélectionner exactement k voisins et de les traiter également, nous pouvons pondérer tous les points selon leur proximité à la requête.\n\nL’estimateur de Nadaraya-Watson définit la prédiction comme une moyenne pondérée:\\hat{y}(\\mathbf{x}) = \\frac{\\sum_{i=1}^{N} K_\\lambda(\\mathbf{x} - \\mathbf{x}_i) \\, y_i}{\\sum_{i=1}^{N} K_\\lambda(\\mathbf{x} - \\mathbf{x}_i)} = \\sum_{i=1}^{N} w_i(\\mathbf{x}) \\, y_i\n\noù les poids sont normalisés:w_i(\\mathbf{x}) = \\frac{K_\\lambda(\\mathbf{x} - \\mathbf{x}_i)}{\\sum_{j=1}^{N} K_\\lambda(\\mathbf{x} - \\mathbf{x}_j)}\n\nChaque point d’entraînement contribue à la prédiction, mais les points éloignés ont un poids négligeable. Le noyau agit comme une fenêtre qui détermine l’influence locale.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate 1D regression data\nnp.random.seed(42)\nn = 50\nX_train = np.sort(np.random.uniform(0, 10, n))\ny_train = np.sin(X_train) + np.random.randn(n) * 0.3\n\ndef gaussian_kernel(u, bandwidth):\n    return np.exp(-u**2 / (2 * bandwidth**2))\n\ndef nadaraya_watson(X_train, y_train, X_test, bandwidth):\n    predictions = []\n    for x in X_test:\n        weights = gaussian_kernel(X_train - x, bandwidth)\n        if np.sum(weights) > 1e-10:\n            predictions.append(np.sum(weights * y_train) / np.sum(weights))\n        else:\n            predictions.append(0)\n    return np.array(predictions)\n\nX_test = np.linspace(0, 10, 200)\n\nfig, axes = plt.subplots(1, 3, figsize=(12, 3.5))\nbandwidths = [0.2, 0.5, 1.5]\n\nfor ax, bw in zip(axes, bandwidths):\n    y_pred = nadaraya_watson(X_train, y_train, X_test, bw)\n    \n    ax.scatter(X_train, y_train, c='C0', s=30, alpha=0.6, label='Données')\n    ax.plot(X_test, y_pred, 'C1-', linewidth=2, label=f'Nadaraya-Watson')\n    ax.plot(X_test, np.sin(X_test), 'k--', alpha=0.5, label=r'$\\sin(x)$')\n    ax.set_xlabel('$x$')\n    ax.set_ylabel('$y$')\n    ax.set_title(f'$\\\\lambda = {bw}$')\n    ax.legend(loc='upper right', fontsize=8)\n    ax.set_xlim(0, 10)\n\nplt.tight_layout()\n\n\n\nComparé aux k-ppv, Nadaraya-Watson produit des prédictions plus lisses car la transition entre voisins est graduelle plutôt qu’abrupte. Le paramètre \\lambda joue un rôle analogue à k: une petite largeur de bande donne une courbe qui suit de près les données (haute variance), une grande largeur de bande lisse excessivement (haut biais).\n\nLes deux approches, k-ppv et Nadaraya-Watson, sont des méthodes à moyennes locales. Elles estiment \\mathbb{E}[Y \\mid \\mathbf{X} = \\mathbf{x}] en faisant une moyenne pondérée des y_i pour les points \\mathbf{x}_i proches de \\mathbf{x}. La différence réside dans la définition de “proche”: les k-ppv utilisent une frontière nette (les k plus proches), tandis que Nadaraya-Watson utilise une pondération douce (le noyau).\n\nLien avec le mécanisme d’attention\n\nLa formule de Nadaraya-Watson ressemble étonnamment au mécanisme d’attention qui a révolutionné l’apprentissage profond. Dans les deux cas, la sortie est une moyenne pondérée où les poids sont normalisés (somme égale à 1):\\text{Nadaraya-Watson:} \\quad \\hat{y}(\\mathbf{x}) = \\sum_{i} \\frac{K(\\mathbf{x}, \\mathbf{x}_i)}{\\sum_j K(\\mathbf{x}, \\mathbf{x}_j)} y_i\n\nEn 2014, Bahdanau, Cho et Bengio, alors à Mila (Montréal), ont introduit le mécanisme d’attention pour la traduction automatique neuronale. L’idée: plutôt que de compresser toute une phrase source en un vecteur fixe, le décodeur peut “regarder” différentes parties de la phrase source à chaque étape, avec des poids d’attention appris. Ce mécanisme a ensuite été généralisé dans l’architecture Transformer (Vaswani et al., 2017), qui est à la base des grands modèles de langage comme GPT et Claude.\n\nLa connexion n’est pas que superficielle: Nadaraya-Watson et l’attention résolvent le même problème fondamental. Comment agréger de l’information provenant de plusieurs sources (voisins ou tokens) de manière différenciée selon leur pertinence pour la requête?","type":"content","url":"/knn#r-gression-de-nadaraya-watson","position":39},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"Complexité"},"type":"lvl2","url":"/knn#complexit","position":40},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"Complexité"},"content":"L’entraînement consiste à stocker les données: O(N). L’inférence requiert de calculer la distance à tous les points et d’identifier les k plus proches: O(Nd) par requête. Pour de grands ensembles, des structures comme les arbres k-d ou le hachage sensible à la localité (LSH) réduisent ce coût.","type":"content","url":"/knn#complexit","position":41},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"Passage à l’échelle"},"type":"lvl2","url":"/knn#passage-l-chelle","position":42},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"Passage à l’échelle"},"content":"L’implémentation naïve des k-ppv calcule la distance entre la requête et chaque point d’entraînement: O(Nd) par requête. Pour un million de points en dimension 100, chaque prédiction requiert 100 millions d’opérations. Cette complexité linéaire en N rend la méthode impraticable pour de grands ensembles de données.\n\nComment accélérer la recherche? L’idée clé est d’éviter de calculer des distances inutiles. Si nous pouvons éliminer rapidement des régions entières de l’espace qui ne peuvent pas contenir le plus proche voisin, nous gagnons du temps.","type":"content","url":"/knn#passage-l-chelle","position":43},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Arbres k-d","lvl2":"Passage à l’échelle"},"type":"lvl3","url":"/knn#arbres-k-d","position":44},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Arbres k-d","lvl2":"Passage à l’échelle"},"content":"Un arbre k-d est une structure d’indexation qui découpe l’espace en boîtes. Lors d’une requête, on peut élaguer (ignorer) toute boîte dont la distance minimale à la requête dépasse la meilleure distance trouvée jusqu’ici.","type":"content","url":"/knn#arbres-k-d","position":45},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Élaguer par boîtes englobantes","lvl2":"Passage à l’échelle"},"type":"lvl3","url":"/knn#id-laguer-par-bo-tes-englobantes","position":46},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Élaguer par boîtes englobantes","lvl2":"Passage à l’échelle"},"content":"Chaque nœud de l’arbre correspond à une région rectangulaire (une “boîte” alignée sur les axes). Pour décider si une boîte peut contenir un point plus proche que notre meilleur candidat actuel, on calcule la distance minimale entre la requête et la boîte.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle, FancyArrowPatch\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Left: box that CAN contain a closer point\nax = axes[0]\nquery = np.array([6, 4])\nbest_dist = 3.5\nbox1 = {'xmin': 1, 'xmax': 4, 'ymin': 2, 'ymax': 6}\n\n# Draw box\nrect = Rectangle((box1['xmin'], box1['ymin']), \n                 box1['xmax'] - box1['xmin'], box1['ymax'] - box1['ymin'],\n                 facecolor='C0', alpha=0.3, edgecolor='C0', linewidth=2)\nax.add_patch(rect)\n\n# Draw query and search radius\nax.scatter(*query, s=150, c='red', marker='*', zorder=5, label='Requête')\ncircle = plt.Circle(query, best_dist, fill=False, color='C1', linewidth=2, linestyle='--', label=f'Rayon = {best_dist}')\nax.add_patch(circle)\n\n# Closest point on box to query\nclosest_on_box = np.array([box1['xmax'], query[1]])  # right edge of box\nax.plot([query[0], closest_on_box[0]], [query[1], closest_on_box[1]], 'k-', linewidth=1.5)\ndist_to_box = np.linalg.norm(query - closest_on_box)\nax.scatter(*closest_on_box, s=80, c='black', zorder=5)\n\nax.set_xlim(0, 10)\nax.set_ylim(0, 8)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_aspect('equal')\nax.set_title(f'Distance à la boîte = {dist_to_box:.1f} < {best_dist}\\n→ Explorer cette région')\nax.legend(loc='upper right')\n\n# Right: box that CANNOT contain a closer point\nax = axes[1]\nbox2 = {'xmin': 0, 'xmax': 2, 'ymin': 0, 'ymax': 2}\n\nrect = Rectangle((box2['xmin'], box2['ymin']), \n                 box2['xmax'] - box2['xmin'], box2['ymax'] - box2['ymin'],\n                 facecolor='gray', alpha=0.3, edgecolor='gray', linewidth=2)\nax.add_patch(rect)\n\nax.scatter(*query, s=150, c='red', marker='*', zorder=5, label='Requête')\ncircle = plt.Circle(query, best_dist, fill=False, color='C1', linewidth=2, linestyle='--', label=f'Rayon = {best_dist}')\nax.add_patch(circle)\n\n# Closest point on box to query (corner)\nclosest_on_box2 = np.array([box2['xmax'], box2['ymax']])\nax.plot([query[0], closest_on_box2[0]], [query[1], closest_on_box2[1]], 'k-', linewidth=1.5)\ndist_to_box2 = np.linalg.norm(query - closest_on_box2)\nax.scatter(*closest_on_box2, s=80, c='black', zorder=5)\n\nax.set_xlim(0, 10)\nax.set_ylim(0, 8)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_aspect('equal')\nax.set_title(f'Distance à la boîte = {dist_to_box2:.1f} > {best_dist}\\n→ Élaguer (ignorer)')\nax.legend(loc='upper right')\n\nplt.tight_layout()\n\n\n\nRègle d’élagage: Si la distance minimale de la requête à une boîte dépasse la meilleure distance trouvée, aucun point dans cette boîte ne peut être plus proche. On peut ignorer toute la région.","type":"content","url":"/knn#id-laguer-par-bo-tes-englobantes","position":47},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Structure de l’arbre","lvl2":"Passage à l’échelle"},"type":"lvl3","url":"/knn#structure-de-larbre","position":48},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Structure de l’arbre","lvl2":"Passage à l’échelle"},"content":"L’arbre k-d partitionne récursivement l’espace. À chaque nœud, on choisit une dimension et on divise les points selon leur coordonnée sur cette dimension (au niveau de la médiane). On alterne les dimensions à chaque niveau.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle, FancyBboxPatch\nimport matplotlib.patches as mpatches\n\nnp.random.seed(0)\n# Use fewer points for clarity\npoints = np.array([[2, 7], [5, 4], [9, 6], [4, 2], [8, 1], [7, 8]])\nlabels = ['A', 'B', 'C', 'D', 'E', 'F']\n\n# Build tree structure for visualization\nclass KDNode:\n    def __init__(self, point_idx, dim, split_val, left=None, right=None, bounds=None):\n        self.point_idx = point_idx\n        self.dim = dim\n        self.split_val = split_val\n        self.left = left\n        self.right = right\n        self.bounds = bounds  # (x_min, x_max, y_min, y_max)\n\ndef build_kdtree(point_indices, depth, x_min, x_max, y_min, y_max):\n    if len(point_indices) == 0:\n        return None\n    if len(point_indices) == 1:\n        return KDNode(point_indices[0], None, None, bounds=(x_min, x_max, y_min, y_max))\n    \n    dim = depth % 2\n    pts = points[point_indices]\n    sorted_order = np.argsort(pts[:, dim])\n    median_pos = len(sorted_order) // 2\n    median_idx = point_indices[sorted_order[median_pos]]\n    split_val = points[median_idx, dim]\n    \n    left_indices = point_indices[sorted_order[:median_pos]]\n    right_indices = point_indices[sorted_order[median_pos+1:]]\n    \n    if dim == 0:\n        left = build_kdtree(left_indices, depth+1, x_min, split_val, y_min, y_max)\n        right = build_kdtree(right_indices, depth+1, split_val, x_max, y_min, y_max)\n    else:\n        left = build_kdtree(left_indices, depth+1, x_min, x_max, y_min, split_val)\n        right = build_kdtree(right_indices, depth+1, x_min, x_max, split_val, y_max)\n    \n    return KDNode(median_idx, dim, split_val, left, right, (x_min, x_max, y_min, y_max))\n\nroot = build_kdtree(np.arange(len(points)), 0, 0, 10, 0, 10)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: spatial view\nax = axes[0]\ncolors = ['C0', 'C1', 'C2']\n\ndef draw_splits(node, depth, ax):\n    if node is None or node.dim is None:\n        return\n    x_min, x_max, y_min, y_max = node.bounds\n    color = colors[min(depth, len(colors)-1)]\n    if node.dim == 0:\n        ax.plot([node.split_val, node.split_val], [y_min, y_max], \n                color=color, linewidth=2.5, alpha=0.8)\n    else:\n        ax.plot([x_min, x_max], [node.split_val, node.split_val], \n                color=color, linewidth=2.5, alpha=0.8)\n    draw_splits(node.left, depth+1, ax)\n    draw_splits(node.right, depth+1, ax)\n\ndraw_splits(root, 0, ax)\nax.scatter(points[:, 0], points[:, 1], s=120, c='black', zorder=5)\nfor i, (p, label) in enumerate(zip(points, labels)):\n    ax.annotate(label, p, xytext=(5, 5), textcoords='offset points', fontsize=12, fontweight='bold')\n\nax.set_xlim(-0.5, 10.5)\nax.set_ylim(-0.5, 10.5)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_aspect('equal')\nax.set_title('Partition de l\\'espace')\n\nfrom matplotlib.lines import Line2D\nlegend_elements = [Line2D([0], [0], color=colors[i], linewidth=2.5, \n                          label=f'Profondeur {i} ({\"$x_1$\" if i%2==0 else \"$x_2$\"})') \n                   for i in range(3)]\nax.legend(handles=legend_elements, loc='lower right')\n\n# Right: tree structure\nax = axes[1]\nax.set_xlim(0, 10)\nax.set_ylim(0, 10)\nax.axis('off')\nax.set_title('Structure de l\\'arbre')\n\n# Draw tree manually with positions\nnode_positions = {}\ndef assign_positions(node, x, y, dx, positions):\n    if node is None:\n        return\n    positions[node.point_idx] = (x, y)\n    if node.left:\n        assign_positions(node.left, x - dx, y - 2, dx * 0.5, positions)\n    if node.right:\n        assign_positions(node.right, x + dx, y - 2, dx * 0.5, positions)\n\nassign_positions(root, 5, 9, 2.2, node_positions)\n\n# Draw edges first\ndef draw_edges(node, ax, positions):\n    if node is None:\n        return\n    x, y = positions[node.point_idx]\n    if node.left:\n        lx, ly = positions[node.left.point_idx]\n        ax.plot([x, lx], [y-0.4, ly+0.4], 'k-', linewidth=1.5, zorder=1)\n        ax.text((x+lx)/2 - 0.3, (y+ly)/2, 'G', fontsize=9, color='gray')\n    if node.right:\n        rx, ry = positions[node.right.point_idx]\n        ax.plot([x, rx], [y-0.4, ry+0.4], 'k-', linewidth=1.5, zorder=1)\n        ax.text((x+rx)/2 + 0.2, (y+ry)/2, 'D', fontsize=9, color='gray')\n    draw_edges(node.left, ax, positions)\n    draw_edges(node.right, ax, positions)\n\ndraw_edges(root, ax, node_positions)\n\n# Draw nodes\ndef draw_nodes(node, depth, ax, positions):\n    if node is None:\n        return\n    x, y = positions[node.point_idx]\n    color = colors[min(depth, len(colors)-1)] if node.dim is not None else 'lightgray'\n    \n    # Node box\n    bbox = FancyBboxPatch((x-0.5, y-0.35), 1, 0.7, boxstyle=\"round,pad=0.05\",\n                          facecolor=color, edgecolor='black', linewidth=1.5, \n                          alpha=0.8, zorder=3)\n    ax.add_patch(bbox)\n    ax.text(x, y, labels[node.point_idx], ha='center', va='center', \n            fontsize=14, fontweight='bold', zorder=4)\n    \n    # Split info\n    if node.dim is not None:\n        dim_name = '$x_1$' if node.dim == 0 else '$x_2$'\n        ax.text(x, y-0.6, f'{dim_name} < {node.split_val:.0f}?', ha='center', \n                va='top', fontsize=9, color='black')\n    \n    draw_nodes(node.left, depth+1, ax, positions)\n    draw_nodes(node.right, depth+1, ax, positions)\n\ndraw_nodes(root, 0, ax, node_positions)\n\nplt.tight_layout()\n\n\n\nChaque nœud interne correspond à une question “x_j < t?”. Les branches gauche (G) et droite (D) correspondent aux réponses “oui” et “non”. Les feuilles (en gris) contiennent les points.","type":"content","url":"/knn#structure-de-larbre","position":49},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Déroulement d’une requête","lvl2":"Passage à l’échelle"},"type":"lvl3","url":"/knn#d-roulement-dune-requ-te","position":50},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Déroulement d’une requête","lvl2":"Passage à l’échelle"},"content":"Voici comment se déroule la recherche du plus proche voisin:\n\nDescente: Suivre l’arbre jusqu’à une feuille contenant la requête (comme dans un arbre binaire de recherche)\n\nInitialisation: Le point dans cette feuille devient notre meilleur candidat\n\nRemontée avec élagage: En remontant, pour chaque nœud ancêtre:\n\nSi l’autre côté de la division pourrait contenir un point plus proche (test de la boîte), l’explorer\n\nSinon, l’élaguer\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle, Rectangle\n\nnp.random.seed(42)\npoints = np.random.rand(15, 2) * 10\nquery = np.array([7.5, 3.0])\n\ndistances = np.sqrt(np.sum((points - query)**2, axis=1))\nnn_idx = np.argmin(distances)\nnn_dist = distances[nn_idx]\n\nsplits = []\ndef build_kdtree_splits(points_idx, depth, x_min, x_max, y_min, y_max):\n    if len(points_idx) <= 1:\n        return\n    dim = depth % 2\n    pts = points[points_idx]\n    sorted_idx = np.argsort(pts[:, dim])\n    median_pos = len(sorted_idx) // 2\n    median_val = pts[sorted_idx[median_pos], dim]\n    splits.append((dim, median_val, depth, x_min, x_max, y_min, y_max))\n    left_idx = points_idx[sorted_idx[:median_pos]]\n    right_idx = points_idx[sorted_idx[median_pos:]]\n    if dim == 0:\n        build_kdtree_splits(left_idx, depth + 1, x_min, median_val, y_min, y_max)\n        build_kdtree_splits(right_idx, depth + 1, median_val, x_max, y_min, y_max)\n    else:\n        build_kdtree_splits(left_idx, depth + 1, x_min, x_max, y_min, median_val)\n        build_kdtree_splits(right_idx, depth + 1, x_min, x_max, median_val, y_max)\n\nbuild_kdtree_splits(np.arange(len(points)), 0, 0, 10, 0, 10)\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Pruned region (left side)\nrect = Rectangle((0, 0), 4.5, 10, facecolor='gray', alpha=0.25, edgecolor='none')\nax.add_patch(rect)\n\nfor dim, val, depth, x_min, x_max, y_min, y_max in splits:\n    alpha = 0.6\n    if dim == 0:\n        ax.plot([val, val], [y_min, y_max], color='gray', linewidth=1.5, alpha=alpha)\n    else:\n        ax.plot([x_min, x_max], [val, val], color='gray', linewidth=1.5, alpha=alpha)\n\ncircle = Circle(query, nn_dist, fill=False, color='C1', linewidth=2, linestyle='--')\nax.add_patch(circle)\n\nax.scatter(points[:, 0], points[:, 1], s=80, c='C0', zorder=5, label='Points')\nax.scatter(points[nn_idx, 0], points[nn_idx, 1], s=120, c='C2', zorder=6, \n           edgecolors='black', linewidths=2, label='Plus proche voisin')\nax.scatter(query[0], query[1], s=150, c='red', marker='*', zorder=7, label='Requête')\nax.plot([query[0], points[nn_idx, 0]], [query[1], points[nn_idx, 1]], \n        'C2--', linewidth=1.5, alpha=0.7)\n\nax.set_xlim(-0.5, 10.5)\nax.set_ylim(-0.5, 10.5)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_aspect('equal')\nax.legend(loc='upper left')\nax.annotate('Région élaguée\\n(distance min > rayon)', xy=(2, 5), fontsize=10, \n            ha='center', color='gray')\nax.set_title('Le cercle pointillé montre le rayon de recherche actuel')\nplt.tight_layout()\n\n\n\n","type":"content","url":"/knn#d-roulement-dune-requ-te","position":51},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Limites en haute dimension","lvl2":"Passage à l’échelle"},"type":"lvl3","url":"/knn#limites-en-haute-dimension","position":52},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Limites en haute dimension","lvl2":"Passage à l’échelle"},"content":"Règle pratique\n\nFonctionne bien: dimension d \\lesssim 10 à 20, données bien distribuées\n\nPerd son avantage: dimension d > 20, on finit par visiter presque tous les nœuds\n\nAlternative en haute dimension: méthodes approximatives (section suivante)\n\nPourquoi l’arbre k-d échoue en haute dimension? Le test d’élagage compare la distance à une boîte avec le rayon de recherche. En haute dimension, les boîtes deviennent des “hyper-rectangles” avec beaucoup de faces. La distance minimale à une boîte reste souvent faible même quand la boîte est “loin” intuitivement, car il suffit qu’une seule coordonnée soit proche. Résultat: peu de régions sont élaguées, et on retombe sur une complexité proche de O(N).","type":"content","url":"/knn#limites-en-haute-dimension","position":53},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"En pratique","lvl2":"Passage à l’échelle"},"type":"lvl3","url":"/knn#en-pratique","position":54},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"En pratique","lvl2":"Passage à l’échelle"},"content":"En scikit-learn:from sklearn.neighbors import KNeighborsClassifier\n\n# algorithm='auto' choisit automatiquement entre brute, kd_tree, ball_tree\nclf = KNeighborsClassifier(n_neighbors=5, algorithm='auto')\n\n# Pour forcer un algorithme spécifique:\nclf_kd = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')\nclf_brute = KNeighborsClassifier(n_neighbors=5, algorithm='brute')\n\nL’option algorithm='auto' choisit la meilleure stratégie selon la taille et la dimension des données.","type":"content","url":"/knn#en-pratique","position":55},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Recherche approximative","lvl2":"Passage à l’échelle"},"type":"lvl3","url":"/knn#recherche-approximative","position":56},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Recherche approximative","lvl2":"Passage à l’échelle"},"content":"Quand la recherche exacte est trop coûteuse, on peut accepter des voisins approximatifs. Les méthodes de recherche approximative des plus proches voisins (approximate nearest neighbors, ANN) garantissent de trouver des points qui sont proches, sans garantir qu’ils soient les plus proches.\n\nLe hachage sensible à la localité (locality-sensitive hashing, LSH) projette les points dans un espace de hachage où les points proches ont une forte probabilité de collision. Plusieurs tables de hachage avec des fonctions différentes permettent d’atteindre un bon rappel. La complexité devient sous-linéaire en N, au prix d’une approximation.\n\nDes bibliothèques comme FAISS (Facebook AI Similarity Search) et Annoy (Approximate Nearest Neighbors Oh Yeah) implémentent ces algorithmes et permettent de chercher parmi des milliards de vecteurs. Ces outils sont essentiels pour les systèmes de recommandation et la recherche sémantique à grande échelle, où les représentations vectorielles (embeddings) de documents, images ou produits sont comparées par similarité.\n\nBases de données vectorielles et RAG\n\nLe terme base de données vectorielle (vector database) est devenu courant dans l’industrie. Il désigne essentiellement un système de recherche approximative des plus proches voisins optimisé pour les embeddings de haute dimension. Sous le capot, ce sont les mêmes algorithmes: LSH, graphes de proximité (HNSW), ou quantification de produits.\n\nCes systèmes sont au cœur des architectures RAG (Retrieval-Augmented Generation) utilisées avec les grands modèles de langage. Le principe: plutôt que de tout mémoriser dans les paramètres du modèle, on stocke des documents dans une base vectorielle. À chaque requête, on retrouve les documents pertinents par similarité cosinus sur leurs embeddings, puis on les fournit au modèle comme contexte.\n\nLa similarité cosinus \\text{sim}(\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|} est particulièrement adaptée aux embeddings de texte car elle ignore la magnitude des vecteurs et se concentre sur leur direction, capturant ainsi la similarité sémantique plutôt que la longueur des documents.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(123)\n\n# Generate points in 2D\nn_points = 40\npoints = np.random.randn(n_points, 2)\n\n# Query point\nquery = np.array([0.5, 0.3])\n\n# Find true 5 nearest neighbors\nk = 5\ndistances = np.sqrt(np.sum((points - query)**2, axis=1))\ntrue_nn_idx = np.argsort(distances)[:k]\n\n# Simulate approximate nearest neighbors (miss one, include one wrong)\napprox_nn_idx = np.array([true_nn_idx[0], true_nn_idx[1], true_nn_idx[2], \n                          true_nn_idx[4], np.argsort(distances)[k+1]])\n\n# Points that are in both\ncommon_idx = np.intersect1d(true_nn_idx, approx_nn_idx)\n# Only in true\nonly_true = np.setdiff1d(true_nn_idx, approx_nn_idx)\n# Only in approx\nonly_approx = np.setdiff1d(approx_nn_idx, true_nn_idx)\n\nfig, axes = plt.subplots(1, 2, figsize=(11, 5))\n\n# Left: Exact search\nax = axes[0]\nax.scatter(points[:, 0], points[:, 1], s=50, c='lightgray', alpha=0.7)\nax.scatter(points[true_nn_idx, 0], points[true_nn_idx, 1], s=100, c='C2', \n           edgecolors='black', linewidths=1.5, label=f'{k} plus proches (vrais)')\nax.scatter(query[0], query[1], s=200, c='red', marker='*', zorder=10, label='Requête')\n\n# Draw circle for k-th distance\nkth_dist = distances[true_nn_idx[-1]]\ncircle = plt.Circle(query, kth_dist, fill=False, color='C2', linestyle='--', linewidth=1.5)\nax.add_patch(circle)\n\nax.set_xlim(-3, 3)\nax.set_ylim(-3, 3)\nax.set_aspect('equal')\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_title('Recherche exacte')\nax.legend(loc='upper left', fontsize=9)\n\n# Right: Approximate search\nax = axes[1]\nax.scatter(points[:, 0], points[:, 1], s=50, c='lightgray', alpha=0.7)\n\n# Common points (found by both)\nax.scatter(points[common_idx, 0], points[common_idx, 1], s=100, c='C2', \n           edgecolors='black', linewidths=1.5, label='Trouvés (corrects)')\n\n# Missed by approximate\nax.scatter(points[only_true, 0], points[only_true, 1], s=100, c='C3', \n           edgecolors='black', linewidths=1.5, marker='s', label='Manqués')\n\n# False positives from approximate\nax.scatter(points[only_approx, 0], points[only_approx, 1], s=100, c='C1', \n           edgecolors='black', linewidths=1.5, marker='^', label='Faux positifs')\n\nax.scatter(query[0], query[1], s=200, c='red', marker='*', zorder=10, label='Requête')\n\nax.set_xlim(-3, 3)\nax.set_ylim(-3, 3)\nax.set_aspect('equal')\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_title(f'Recherche approximative (rappel = {len(common_idx)}/{k})')\nax.legend(loc='upper left', fontsize=9)\n\nplt.tight_layout()\n\n\n\n","type":"content","url":"/knn#recherche-approximative","position":57},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Compromis précision-vitesse","lvl2":"Passage à l’échelle"},"type":"lvl3","url":"/knn#compromis-pr-cision-vitesse","position":58},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl3":"Compromis précision-vitesse","lvl2":"Passage à l’échelle"},"content":"Le choix entre recherche exacte et approximative dépend de l’application. Pour un diagnostic médical, une erreur dans l’identification des cas similaires peut avoir des conséquences graves: la recherche exacte est préférable. Pour suggérer des vidéos similaires sur une plateforme de streaming, quelques voisins manqués importent peu si les suggestions restent pertinentes.\n\nMéthode\n\nComplexité requête\n\nExacte\n\nDimension\n\nForce brute\n\nO(Nd)\n\nOui\n\nToute\n\nArbre k-d\n\nO(\\log N) à O(N)\n\nOui\n\nd \\lesssim 20\n\nLSH\n\nO(1) à O(N^{\\rho})\n\nNon\n\nHaute\n\nGraphes de proximité\n\nO(\\log N)\n\nNon\n\nHaute\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulated query times for different methods\nN = np.logspace(2, 7, 50)  # 100 to 10 million points\n\n# Brute force: O(N)\nbrute_force = N * 1e-7\n\n# k-d tree: O(log N) in low dim, degrades to O(N) in high dim\n# We show the ideal low-dim case\nkd_tree = np.log2(N) * 1e-5\n\n# ANN (e.g., HNSW): nearly constant with slight log factor\nann = np.log2(N) * 5e-6 + 1e-4\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nax.loglog(N, brute_force, 'C0-', linewidth=2, label='Force brute $O(N)$')\nax.loglog(N, kd_tree, 'C1-', linewidth=2, label=r'Arbre k-d $O(\\log N)$')\nax.loglog(N, ann, 'C2-', linewidth=2, label=r'ANN $O(\\log N)$')\n\n# Add shaded regions for practical regimes\nax.axvspan(100, 1e4, alpha=0.1, color='C0')\nax.axvspan(1e4, 1e6, alpha=0.1, color='C1')\nax.axvspan(1e6, 1e7, alpha=0.1, color='C2')\n\nax.set_xlabel('Taille de l\\'ensemble $N$')\nax.set_ylabel('Temps par requête (s)')\nax.legend(loc='upper left')\nax.set_xlim(100, 1e7)\nax.grid(True, alpha=0.3, which='both')\n\n# Annotations\nax.annotate('Petit\\nensemble', xy=(500, 1e-3), fontsize=9, ha='center', alpha=0.7)\nax.annotate('Moyen', xy=(1e5, 1e-3), fontsize=9, ha='center', alpha=0.7)\nax.annotate('Grande\\néchelle', xy=(3e6, 1e-3), fontsize=9, ha='center', alpha=0.7)\n\nplt.tight_layout()\n\n\n\nLa figure illustre comment le temps de requête évolue avec la taille de l’ensemble. La force brute devient rapidement prohibitive. Les méthodes approximatives maintiennent des temps de réponse acceptables même pour des millions de points.","type":"content","url":"/knn#compromis-pr-cision-vitesse","position":59},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"Méthodes paramétriques et non paramétriques"},"type":"lvl2","url":"/knn#m-thodes-param-triques-et-non-param-triques","position":60},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"Méthodes paramétriques et non paramétriques"},"content":"Les k-ppv sont une méthode non paramétrique: les données sont le modèle. Il n’y a pas de paramètres appris; les prédictions consultent directement l’ensemble d’entraînement. La complexité du modèle croît avec N.\n\n\n\nNon paramétrique\n\nParamétrique\n\nModèle\n\nLes données\n\nUn vecteur \\boldsymbol{\\theta} \\in \\mathbb{R}^p\n\nComplexité\n\nCroît avec N\n\nFixe\n\nInférence\n\nRequiert les données\n\nRequiert seulement \\boldsymbol{\\theta}\n\nLes méthodes paramétriques distillent l’information dans un vecteur de paramètres de taille fixe. Un réseau de neurones entraîné sur des milliards d’exemples n’a besoin que de ses poids pour faire des prédictions, pas des données d’entraînement.","type":"content","url":"/knn#m-thodes-param-triques-et-non-param-triques","position":61},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"Résumé"},"type":"lvl2","url":"/knn#r-sum","position":62},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"Résumé"},"content":"Les k plus proches voisins classifient un point par vote majoritaire parmi ses k voisins les plus proches. Le paramètre k contrôle le compromis biais-variance. Le choix de la distance encode les hypothèses sur la similarité. Le fléau de la dimensionnalité limite l’efficacité en haute dimension.\n\nLa méthode illustre la tension entre mémorisation et généralisation: avec k=1, l’erreur d’entraînement est nulle mais la généralisation est mauvaise. Elle illustre aussi la distinction entre approches non paramétriques (les données sont le modèle) et paramétriques (un vecteur de paramètres résume les données).\n\nLe chapitre suivant développe l’approche paramétrique: l’apprentissage comme problème d’optimisation, où nous cherchons les paramètres qui minimisent une fonction de perte.","type":"content","url":"/knn#r-sum","position":63},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"Exercices"},"type":"lvl2","url":"/knn#exercices","position":64},{"hierarchy":{"lvl1":"Méthodes non paramétriques","lvl2":"Exercices"},"content":"Exercice 1: Classification manuelle\n\nConsidérez les points d’entraînement suivants en 2D:\n\nPoint\n\nx_1\n\nx_2\n\nClasse\n\nA\n\n0\n\n0\n\n0\n\nB\n\n1\n\n0\n\n0\n\nC\n\n0\n\n1\n\n1\n\nD\n\n2\n\n2\n\n1\n\nE\n\n3\n\n1\n\n1\n\nPour le point requête \\mathbf{x} = (1, 1), identifiez les 3 plus proches voisins avec la distance euclidienne. Quelle est la prédiction du 3-ppv?\n\nRépétez avec k = 1 et k = 5. Les prédictions changent-elles?\n\nCalculez les distances avec la norme \\ell_1 (Manhattan). Les 3 plus proches voisins sont-ils les mêmes?\n\nSolution Exercice 1\n\nDistances euclidiennes depuis \\mathbf{x} = (1, 1):\n\nPoint\n\nDistance \\ell_2\n\nClasse\n\nA\n\n\\sqrt{(1-0)^2 + (1-0)^2} = \\sqrt{2} \\approx 1.41\n\n0\n\nB\n\n\\sqrt{(1-1)^2 + (1-0)^2} = 1\n\n0\n\nC\n\n\\sqrt{(1-0)^2 + (1-1)^2} = 1\n\n1\n\nD\n\n\\sqrt{(1-2)^2 + (1-2)^2} = \\sqrt{2} \\approx 1.41\n\n1\n\nE\n\n\\sqrt{(1-3)^2 + (1-1)^2} = 2\n\n1\n\nLes 3 plus proches: B (0), C (1), puis A ou D (égalité).\n\nSi on prend B, C, A: votes = {0: 2, 1: 1} → prédiction: classe 0\n\nSi on prend B, C, D: votes = {0: 1, 1: 2} → prédiction: classe 1\n\nEffet de k:\n\nk = 1: Plus proche = B ou C (égalité à distance 1). Prédiction dépend du choix.\n\nk = 5: Tous les points. Votes = {0: 2, 1: 3} → prédiction: classe 1\n\nDistances Manhattan (\\ell_1):\n\nPoint\n\nDistance \\ell_1\n\nA\n\n$\n\nB\n\n$\n\nC\n\n$\n\nD\n\n$\n\nE\n\n$\n\nLes 3 plus proches avec \\ell_1: B, C, puis A/D/E (égalité). Les deux plus proches (B, C) sont identiques, mais le troisième peut différer selon le critère de départage.\n\nExercice 2: Effet de la normalisation\n\nUn jeu de données contient deux variables: l’âge (entre 20 et 70 ans) et le revenu annuel (entre 20 000 et 200 000 dollars).\n\nCalculez la distance euclidienne entre les points \\mathbf{x}_1 = (30, 50000) et \\mathbf{x}_2 = (35, 51000).\n\nCalculez la distance entre \\mathbf{x}_1 = (30, 50000) et \\mathbf{x}_3 = (31, 150000).\n\nLaquelle des deux paires est “plus proche”? Ce résultat est-il intuitivement raisonnable?\n\nProposez une transformation des données qui rendrait les deux variables comparables. Recalculez les distances après transformation.\n\nSolution Exercice 2\n\nDistance \\mathbf{x}_1 à \\mathbf{x}_2:d(\\mathbf{x}_1, \\mathbf{x}_2) = \\sqrt{(35-30)^2 + (51000-50000)^2} = \\sqrt{25 + 1000000} \\approx 1000\n\nDistance \\mathbf{x}_1 à \\mathbf{x}_3:d(\\mathbf{x}_1, \\mathbf{x}_3) = \\sqrt{(31-30)^2 + (150000-50000)^2} = \\sqrt{1 + 10^{10}} \\approx 100000\n\nComparaison: Selon la distance euclidienne, (\\mathbf{x}_1, \\mathbf{x}_2) est 100 fois plus proche que (\\mathbf{x}_1, \\mathbf{x}_3).\n\nCe n’est pas raisonnable: la différence de revenu domine complètement. La paire (\\mathbf{x}_1, \\mathbf{x}_2) diffère de 5 ans et 1000, tandis que (\\mathbf{x}_1, \\mathbf{x}_3) diffère de 1 an et 100000. Intuitivement, on pourrait argumenter que 5 ans de différence d’âge est plus significatif qu’1 an.\n\nNormalisation (standardisation):\n\nSoit \\mu_{\\text{âge}} = 45, \\sigma_{\\text{âge}} = 15 et \\mu_{\\text{revenu}} = 110000, \\sigma_{\\text{revenu}} = 60000 (valeurs approximatives).\n\nTransformation: z = (x - \\mu) / \\sigma\n\n\\mathbf{x}_1' = ((30-45)/15, (50000-110000)/60000) = (-1, -1)\n\n\\mathbf{x}_2' = ((35-45)/15, (51000-110000)/60000) = (-0.67, -0.98)\n\n\\mathbf{x}_3' = ((31-45)/15, (150000-110000)/60000) = (-0.93, 0.67)\n\nNouvelles distances:\n\nd(\\mathbf{x}_1', \\mathbf{x}_2') = \\sqrt{(-1+0.67)^2 + (-1+0.98)^2} \\approx 0.33\n\nd(\\mathbf{x}_1', \\mathbf{x}_3') = \\sqrt{(-1+0.93)^2 + (-1-0.67)^2} \\approx 1.68\n\nAprès normalisation, les deux variables contribuent équitablement.\n\nExercice 3: Compromis biais-variance\n\nSoit un problème de régression 1D où la vraie fonction est f(x) = \\sin(2\\pi x) et les observations sont bruitées: y = f(x) + \\varepsilon avec \\varepsilon \\sim \\mathcal{N}(0, 0.1).\n\nGénérez 50 points d’entraînement uniformément répartis sur [0, 1].\n\nImplémentez la régression k-ppv et tracez les prédictions pour k = 1, 5, 20, 50.\n\nCalculez l’erreur quadratique moyenne (MSE) sur un ensemble de test de 200 points pour chaque valeur de k.\n\nTracez le MSE en fonction de k. Quelle valeur de k minimise l’erreur de test?\n\nQue se passe-t-il quand k = N (nombre total de points d’entraînement)?\n\nSolution Exercice 3import numpy as np\n\n# 1. Génération des données\nnp.random.seed(42)\nX_train = np.sort(np.random.uniform(0, 1, 50))\ny_train = np.sin(2 * np.pi * X_train) + np.random.randn(50) * 0.1\n\nX_test = np.linspace(0, 1, 200)\ny_test_true = np.sin(2 * np.pi * X_test)\n\n# 2. Implémentation k-ppv régression\ndef knn_regression(X_train, y_train, X_test, k):\n    predictions = []\n    for x in X_test:\n        distances = np.abs(X_train - x)\n        k_nearest = np.argsort(distances)[:k]\n        predictions.append(np.mean(y_train[k_nearest]))\n    return np.array(predictions)\n\n# 3. & 4. Calcul du MSE pour différents k\nk_values = range(1, 51)\nmse_values = []\nfor k in k_values:\n    y_pred = knn_regression(X_train, y_train, X_test, k)\n    mse = np.mean((y_pred - y_test_true)**2)\n    mse_values.append(mse)\n\nRésultats typiques:\n\nk = 1: MSE élevé (haute variance, la prédiction saute entre points)\n\nk = 5 à 10: MSE minimal (bon compromis)\n\nk = 50: MSE élevé (haut biais)\n\n5. Quand k = N = 50:\nLa prédiction est la moyenne de tous les y_i, soit \\hat{y}(x) = \\bar{y} \\approx 0 (car \\sin est symétrique sur [0,1]). Le modèle ignore complètement x et prédit une constante. C’est le cas extrême de sous-apprentissage (biais maximal, variance nulle).\n\nExercice 4: Fléau de la dimensionnalité\n\nConsidérez N = 1000 points uniformément distribués dans l’hypercube [0, 1]^d.\n\nPour d = 1, 2, 5, 10, 20, 50, 100, calculez la distance moyenne au plus proche voisin parmi ces points. Utilisez la simulation Monte Carlo.\n\nTracez cette distance en fonction de d. Que constatez-vous?\n\nPour capturer les 10 plus proches voisins (soit 1% des données), quel est le rayon de la boule centrée sur un point arbitraire? Calculez ce rayon pour différentes dimensions.\n\nExpliquez pourquoi les k-ppv deviennent inefficaces en haute dimension, même avec beaucoup de données.\n\nSolution Exercice 4\n\n1. & 2. Simulation Monte Carlo:import numpy as np\nfrom scipy.spatial.distance import cdist\n\ndims = [1, 2, 5, 10, 20, 50, 100]\nN = 1000\nmean_nn_distances = []\n\nfor d in dims:\n    X = np.random.uniform(0, 1, (N, d))\n    dists = cdist(X, X)\n    np.fill_diagonal(dists, np.inf)  # ignorer distance à soi-même\n    nn_dists = dists.min(axis=1)\n    mean_nn_distances.append(nn_dists.mean())\n\nRésultats typiques:\n\nd\n\nDistance moyenne au plus proche voisin\n\n1\n\n~0.001\n\n2\n\n~0.02\n\n5\n\n~0.15\n\n10\n\n~0.35\n\n20\n\n~0.55\n\n50\n\n~0.75\n\n100\n\n~0.85\n\nLa distance au plus proche voisin augmente avec d et tend vers la diagonale de l’hypercube.\n\n3. Rayon pour capturer 1% des points:\n\nPour une distribution uniforme dans [0,1]^d, le volume d’une boule de rayon r doit contenir 1% du volume total. Le volume d’une d-boule est V_d(r) = C_d \\cdot r^d où C_d est une constante. Pour capturer une fraction p du volume unitaire: r = p^{1/d}.\n\nd\n\nr = 0.01^{1/d}\n\n1\n\n0.01\n\n2\n\n0.10\n\n10\n\n0.63\n\n100\n\n0.955\n\nEn dimension 100, il faut un rayon couvrant 95.5% de chaque axe pour capturer 1% des données!\n\n4. Pourquoi k-ppv échoue en haute dimension:\n\nLes “voisins” ne sont plus locaux: ils couvrent presque tout l’espace\n\nTous les points deviennent approximativement équidistants (concentration des distances)\n\nLa moyenne locale devient une moyenne globale, perdant toute information sur la structure locale de f(x)\n\nExercice 5: Distances pour texte\n\nConsidérez trois documents représentés par leurs vecteurs de fréquence de mots (bag-of-words) sur un vocabulaire de 5 mots:\n\nDocument\n\nchat\n\nchien\n\nmaison\n\nvoiture\n\narbre\n\nd_1\n\n3\n\n0\n\n1\n\n0\n\n2\n\nd_2\n\n2\n\n1\n\n0\n\n0\n\n1\n\nd_3\n\n0\n\n0\n\n2\n\n3\n\n0\n\nCalculez la distance euclidienne entre chaque paire de documents.\n\nCalculez la similarité cosinus entre chaque paire, puis convertissez en distance (d = 1 - \\text{sim}).\n\nSelon chaque mesure, quels sont les deux documents les plus similaires?\n\nPourquoi la similarité cosinus est-elle souvent préférée pour les documents textuels?\n\nSolution Exercice 5\n\nVecteurs: d_1 = (3, 0, 1, 0, 2), d_2 = (2, 1, 0, 0, 1), d_3 = (0, 0, 2, 3, 0)\n\n1. Distances euclidiennes:d_E(d_1, d_2) = \\sqrt{(3-2)^2 + (0-1)^2 + (1-0)^2 + 0 + (2-1)^2} = \\sqrt{1+1+1+1} = 2d_E(d_1, d_3) = \\sqrt{9 + 0 + 1 + 9 + 4} = \\sqrt{23} \\approx 4.80d_E(d_2, d_3) = \\sqrt{4 + 1 + 4 + 9 + 1} = \\sqrt{19} \\approx 4.36\n\n2. Similarités cosinus:\n\n\\|d_1\\| = \\sqrt{9+0+1+0+4} = \\sqrt{14}, \\|d_2\\| = \\sqrt{4+1+0+0+1} = \\sqrt{6}, \\|d_3\\| = \\sqrt{0+0+4+9+0} = \\sqrt{13}\\text{sim}(d_1, d_2) = \\frac{d_1 \\cdot d_2}{\\|d_1\\| \\|d_2\\|} = \\frac{6+0+0+0+2}{\\sqrt{14}\\sqrt{6}} = \\frac{8}{\\sqrt{84}} \\approx 0.87\\text{sim}(d_1, d_3) = \\frac{0+0+2+0+0}{\\sqrt{14}\\sqrt{13}} = \\frac{2}{\\sqrt{182}} \\approx 0.15\\text{sim}(d_2, d_3) = \\frac{0+0+0+0+0}{\\sqrt{6}\\sqrt{13}} = 0\n\nDistances cosinus: d_C = 1 - \\text{sim}\n\nPaire\n\nd_E\n\nd_C\n\n(d_1, d_2)\n\n2.00\n\n0.13\n\n(d_1, d_3)\n\n4.80\n\n0.85\n\n(d_2, d_3)\n\n4.36\n\n1.00\n\n3. Documents les plus similaires:\n\nDistance euclidienne: (d_1, d_2) ✓\n\nDistance cosinus: (d_1, d_2) ✓\n\nLes deux métriques s’accordent ici.\n\n4. Pourquoi cosinus pour le texte:\n\nInvariance à la longueur: Un document 2x plus long a des fréquences 2x plus grandes, mais le même contenu thématique. Cosinus ignore la norme.\n\nFocus sur l’orientation: Cosinus mesure l’angle entre vecteurs, pas leur magnitude. Deux documents sur le même sujet pointent dans la même direction.\n\nDocuments creux: Les vecteurs textuels sont très creux (la plupart des mots ont fréquence 0). Cosinus gère bien cette sparsité.\n\nExercice 6: Nadaraya-Watson\n\nSoit les données d’entraînement: x = [0, 1, 2, 3] et y = [1, 2, 1.5, 3].\n\nAvec un noyau gaussien K_\\lambda(u) = \\exp(-u^2 / 2\\lambda^2) et \\lambda = 0.5, calculez manuellement la prédiction de Nadaraya-Watson pour x^* = 1.5.\n\nRépétez avec \\lambda = 2. Comment la prédiction change-t-elle?\n\nImplémentez l’estimateur et tracez les prédictions pour \\lambda = 0.2, 0.5, 1, 2 sur l’intervalle [0, 3].\n\nComparez visuellement avec la régression k-ppv pour k = 1, 2, 3. Quelle méthode produit des prédictions plus lisses?\n\nSolution Exercice 6\n\n1. Nadaraya-Watson pour x^* = 1.5 avec \\lambda = 0.5:\n\nDistances: u_i = x^* - x_i = [1.5, 0.5, -0.5, -1.5]\n\nPoids du noyau: K_\\lambda(u) = \\exp(-u^2 / 2\\lambda^2) = \\exp(-u^2 / 0.5)\n\ni\n\nx_i\n\ny_i\n\nu_i\n\nK(u_i)\n\n0\n\n0\n\n1\n\n1.5\n\ne^{-4.5} \\approx 0.011\n\n1\n\n1\n\n2\n\n0.5\n\ne^{-0.5} \\approx 0.607\n\n2\n\n2\n\n1.5\n\n-0.5\n\ne^{-0.5} \\approx 0.607\n\n3\n\n3\n\n3\n\n-1.5\n\ne^{-4.5} \\approx 0.011\n\nSomme des poids: \\sum K = 0.011 + 0.607 + 0.607 + 0.011 = 1.236\n\nPrédiction:\\hat{y} = \\frac{\\sum K(u_i) y_i}{\\sum K(u_i)} = \\frac{0.011 \\times 1 + 0.607 \\times 2 + 0.607 \\times 1.5 + 0.011 \\times 3}{1.236}\n\n= \\frac{0.011 + 1.214 + 0.911 + 0.033}{1.236} = \\frac{2.169}{1.236} \\approx 1.75\n\n2. Avec \\lambda = 2:\n\nPoids: K(u) = \\exp(-u^2 / 8)\n\ni\n\nK(u_i)\n\n0\n\ne^{-0.28} \\approx 0.756\n\n1\n\ne^{-0.03} \\approx 0.969\n\n2\n\ne^{-0.03} \\approx 0.969\n\n3\n\ne^{-0.28} \\approx 0.756\\hat{y} = \\frac{0.756 \\times 1 + 0.969 \\times 2 + 0.969 \\times 1.5 + 0.756 \\times 3}{0.756 + 0.969 + 0.969 + 0.756}= \\frac{0.756 + 1.938 + 1.454 + 2.268}{3.45} = \\frac{6.416}{3.45} \\approx 1.86\n\nAvec \\lambda plus grand, tous les points contribuent plus également, et la prédiction se rapproche de la moyenne globale \\bar{y} = 1.875.\n\n3. & 4. Nadaraya-Watson produit des courbes plus lisses car la transition entre voisins est graduelle (poids continus) plutôt qu’abrupte (k-ppv: poids 0 ou 1).\n\nExercice 7: Complexité computationnelle\n\nVous développez un système de recommandation pour une plateforme avec 10 millions d’utilisateurs. Chaque utilisateur est représenté par un vecteur de 100 dimensions (embeddings).\n\nCombien d’opérations (multiplications et additions) faut-il pour trouver le plus proche voisin d’un utilisateur par force brute?\n\nSi chaque opération prend 1 nanoseconde, quel est le temps de réponse pour une requête?\n\nSi vous devez traiter 1000 requêtes par seconde, cette approche est-elle viable?\n\nUn arbre k-d réduit la complexité à O(\\log N) en basse dimension. Pourquoi cette structure n’est-elle pas efficace pour d = 100?\n\nProposez une stratégie pour ce problème à grande échelle.\n\nSolution Exercice 7\n\n1. Nombre d’opérations:\n\nPour calculer la distance euclidienne entre deux vecteurs de dimension d:\n\nd soustractions\n\nd multiplications (carrés)\n\nd-1 additions\n\n1 racine carrée\n\nSoit environ 3d opérations par distance.\n\nPour N = 10^7 utilisateurs et d = 100:\\text{Opérations} = N \\times 3d = 10^7 \\times 300 = 3 \\times 10^9\n\n2. Temps de réponse:\n\nÀ 1 ns par opération: 3 \\times 10^9 \\times 10^{-9} s = 3 secondes par requête.\n\n3. Viabilité pour 1000 req/s:\n\nNon! Il faudrait 3000 secondes de calcul par seconde, soit 3000 cœurs CPU dédiés juste pour tenir la charge. Ce n’est pas viable économiquement.\n\n4. Pourquoi k-d tree échoue en d = 100:\n\nEn haute dimension, les hyperplans de séparation deviennent inefficaces: la plupart des régions de l’espace sont “proches” de la requête\n\nL’élagage des branches devient rare: on finit par visiter presque tous les nœuds\n\nLa complexité dégénère vers O(N), comme la force brute\n\nRègle empirique: k-d trees efficaces seulement pour d \\lesssim 20\n\n5. Stratégies à grande échelle:\n\nRecherche approximative (ANN): Utiliser FAISS, Annoy, ou ScaNN. Ces bibliothèques utilisent:\n\nQuantification de produit (PQ): Compresser les vecteurs pour réduire la mémoire et accélérer les calculs\n\nGraphes de proximité (HNSW): Naviguer un graphe où les nœuds proches sont connectés\n\nLSH: Hacher les vecteurs pour que les voisins aient des hachages similaires\n\nRéduction de dimension: Projeter les embeddings de 100 à 32 dimensions via PCA avant la recherche\n\nFiltrage en deux étapes:\n\nRecherche approximative rapide pour obtenir 1000 candidats\n\nRéordonnancement exact sur ces 1000 candidats\n\nMise en cache: Pré-calculer et stocker les k-NN des utilisateurs les plus actifs\n\nAvec FAISS optimisé, on peut atteindre ~1ms par requête sur 10M vecteurs, soit 1000 req/s sur un seul serveur.\n\nExercice 8: Élagage dans un arbre k-d\n\nCet exercice teste la compétence clé des arbres k-d: décider si une région peut être élaguée.\n\nContexte: Vous cherchez le plus proche voisin d’une requête q = (5, 3). Votre meilleur candidat actuel est à distance r = 2.5.\n\nQuestion: Pour chaque boîte ci-dessous, calculez la distance minimale de q à la boîte et décidez si vous pouvez élaguer (ignorer) cette région.\n\nBoîte\n\nx_1 min\n\nx_1 max\n\nx_2 min\n\nx_2 max\n\nA\n\n0\n\n2\n\n0\n\n2\n\nB\n\n6\n\n8\n\n2\n\n5\n\nC\n\n3\n\n7\n\n4\n\n6\n\nD\n\n8\n\n10\n\n0\n\n1\n\nRappel: La distance minimale d’un point q à une boîte alignée sur les axes se calcule ainsi:\n\nPour chaque dimension j: si q_j < \\text{min}_j, contribution = (\\text{min}_j - q_j)^2; si q_j > \\text{max}_j, contribution = (q_j - \\text{max}_j)^2; sinon contribution = 0\n\nDistance minimale = \\sqrt{\\text{somme des contributions}}\n\nSolution Exercice 8\n\nRequête: q = (5, 3), rayon actuel: r = 2.5\n\nBoîte A [0,2] \\times [0,2]:\n\nDimension 1: q_1 = 5 > 2 = \\text{max}, contribution = (5-2)^2 = 9\n\nDimension 2: q_2 = 3 > 2 = \\text{max}, contribution = (3-2)^2 = 1\n\nDistance min = \\sqrt{9 + 1} = \\sqrt{10} \\approx 3.16 > 2.5 → Élaguer ✓\n\nBoîte B [6,8] \\times [2,5]:\n\nDimension 1: q_1 = 5 < 6 = \\text{min}, contribution = (6-5)^2 = 1\n\nDimension 2: q_2 = 3 \\in [2,5], contribution = 0\n\nDistance min = \\sqrt{1 + 0} = 1 < 2.5 → Explorer (peut contenir un point plus proche)\n\nBoîte C [3,7] \\times [4,6]:\n\nDimension 1: q_1 = 5 \\in [3,7], contribution = 0\n\nDimension 2: q_2 = 3 < 4 = \\text{min}, contribution = (4-3)^2 = 1\n\nDistance min = \\sqrt{0 + 1} = 1 < 2.5 → Explorer\n\nBoîte D [8,10] \\times [0,1]:\n\nDimension 1: q_1 = 5 < 8 = \\text{min}, contribution = (8-5)^2 = 9\n\nDimension 2: q_2 = 3 > 1 = \\text{max}, contribution = (3-1)^2 = 4\n\nDistance min = \\sqrt{9 + 4} = \\sqrt{13} \\approx 3.61 > 2.5 → Élaguer ✓\n\nRésumé: On élague A et D, on explore B et C.","type":"content","url":"/knn#exercices","position":65},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage"},"type":"lvl1","url":"/tp1-learning-problem","position":0},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage"},"content":"IFT6390 - Fondements de l’apprentissage machine\n\nCe notebook accompagne le \n\nChapitre 2: Le problème d’apprentissage.","type":"content","url":"/tp1-learning-problem","position":1},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl2":"Objectifs"},"type":"lvl2","url":"/tp1-learning-problem#objectifs","position":2},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl2":"Objectifs"},"content":"À la fin de ce TP, vous serez en mesure de:\n\nCalculer l’erreur quadratique moyenne (MSE)\n\nObserver le phénomène de surapprentissage avec des polynômes\n\nComprendre le compromis biais-variance\n\nImplémenter la régularisation Ridge\n\n","type":"content","url":"/tp1-learning-problem#objectifs","position":3},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl2":"Partie 0: Configuration"},"type":"lvl2","url":"/tp1-learning-problem#partie-0-configuration","position":4},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl2":"Partie 0: Configuration"},"content":"Exécutez cette cellule pour importer les bibliothèques nécessaires.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore', message='Polyfit may be poorly conditioned')\n\n# Pour de jolis graphiques\nplt.rcParams['figure.figsize'] = (8, 5)\nplt.rcParams['font.size'] = 12\n\nprint(\"✅ Configuration terminée!\")\n\n\n\n","type":"content","url":"/tp1-learning-problem#partie-0-configuration","position":5},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl2":"Partie 1: Les données de freinage"},"type":"lvl2","url":"/tp1-learning-problem#partie-1-les-donn-es-de-freinage","position":6},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl2":"Partie 1: Les données de freinage"},"content":"Nous utilisons les données classiques d’Ezekiel (1930): la distance de freinage d’un véhicule en fonction de sa vitesse.\n\nQuestion préliminaire: Quelle relation attendez-vous entre vitesse et distance de freinage? Linéaire? Quadratique? Autre?\n\n# Données de freinage (Ezekiel, 1930): vitesse (mph) vs distance d'arrêt (ft)\nspeed = np.array([4, 4, 7, 7, 8, 9, 10, 10, 10, 11, 11, 12, 12, 12, 12, 13, 13, 13, 13, 14,\n                  14, 14, 14, 15, 15, 15, 16, 16, 17, 17, 17, 18, 18, 18, 18, 19, 19, 19,\n                  20, 20, 20, 20, 20, 22, 23, 24, 24, 24, 24, 25], dtype=float)\ndist = np.array([2, 10, 4, 22, 16, 10, 18, 26, 34, 17, 28, 14, 20, 24, 28, 26, 34, 34, 46,\n                 26, 36, 60, 80, 20, 26, 54, 32, 40, 32, 40, 50, 42, 56, 76, 84, 36, 46,\n                 68, 32, 48, 52, 56, 64, 66, 54, 70, 92, 93, 120, 85], dtype=float)\n\nprint(f\"Nombre d'observations: {len(speed)}\")\nprint(f\"Vitesse: min={speed.min():.0f}, max={speed.max():.0f} mph\")\nprint(f\"Distance: min={dist.min():.0f}, max={dist.max():.0f} ft\")\n\n\n\n# Visualisation des données\nplt.figure(figsize=(8, 5))\nplt.scatter(speed, dist, alpha=0.7, s=50)\nplt.xlabel('Vitesse (mph)')\nplt.ylabel('Distance de freinage (ft)')\nplt.title('Données de freinage')\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n","type":"content","url":"/tp1-learning-problem#partie-1-les-donn-es-de-freinage","position":7},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl2":"Partie 2: Ajuster un polynôme"},"type":"lvl2","url":"/tp1-learning-problem#partie-2-ajuster-un-polyn-me","position":8},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl2":"Partie 2: Ajuster un polynôme"},"content":"Nous allons ajuster des polynômes de différents degrés aux données.","type":"content","url":"/tp1-learning-problem#partie-2-ajuster-un-polyn-me","position":9},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl3":"2.1 Ajustement simple","lvl2":"Partie 2: Ajuster un polynôme"},"type":"lvl3","url":"/tp1-learning-problem#id-2-1-ajustement-simple","position":10},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl3":"2.1 Ajustement simple","lvl2":"Partie 2: Ajuster un polynôme"},"content":"\n\n# Ajuster un polynôme de degré 2\ndegree = 2\ncoeffs = np.polyfit(speed, dist, degree)\n\nprint(f\"Coefficients du polynôme de degré {degree}:\")\nfor i, c in enumerate(coeffs):\n    print(f\"  coefficient de x^{degree-i}: {c:.4f}\")\n\n\n\n# Visualiser l'ajustement\nplt.figure(figsize=(8, 5))\nplt.scatter(speed, dist, alpha=0.7, s=50, label='Données')\n\n# Grille pour tracer la courbe\nspeed_grid = np.linspace(0, 30, 100)\ndist_pred = np.polyval(coeffs, speed_grid)\nplt.plot(speed_grid, dist_pred, 'r-', linewidth=2, label=f'Polynôme degré {degree}')\n\nplt.xlabel('Vitesse (mph)')\nplt.ylabel('Distance de freinage (ft)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n","type":"content","url":"/tp1-learning-problem#id-2-1-ajustement-simple","position":11},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl3":"📝 Exercice 1: Calculer l’erreur quadratique moyenne (MSE)","lvl2":"Partie 2: Ajuster un polynôme"},"type":"lvl3","url":"/tp1-learning-problem#id-exercice-1-calculer-lerreur-quadratique-moyenne-mse","position":12},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl3":"📝 Exercice 1: Calculer l’erreur quadratique moyenne (MSE)","lvl2":"Partie 2: Ajuster un polynôme"},"content":"L’erreur quadratique moyenne est définie comme:\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n\nComplétez la fonction ci-dessous:\n\ndef compute_mse(y_true, y_pred):\n    \"\"\"\n    Calcule l'erreur quadratique moyenne.\n    \n    Args:\n        y_true: valeurs réelles (array)\n        y_pred: prédictions (array)\n    \n    Returns:\n        MSE (float)\n    \"\"\"\n    # ============================================\n    # TODO: Complétez cette fonction\n    # Indice: utilisez np.mean() et l'opérateur **2\n    # ============================================\n    \n    mse = None  # <- Remplacez None par votre code\n    \n    return mse\n\n\n\n# Test de votre fonction\npredictions = np.polyval(coeffs, speed)\nmse = compute_mse(dist, predictions)\n\nif mse is not None:\n    print(f\"MSE pour le polynôme de degré {degree}: {mse:.2f}\")\n    \n    # Vérification\n    expected_mse = np.mean((dist - predictions)**2)\n    if np.isclose(mse, expected_mse):\n        print(\"✅ Correct!\")\n    else:\n        print(f\"❌ Attendu: {expected_mse:.2f}\")\nelse:\n    print(\"⚠️ La fonction retourne None. Complétez le code!\")\n\n\n\n💡 Cliquez pour voir la solutiondef compute_mse(y_true, y_pred):\n    mse = np.mean((y_true - y_pred)**2)\n    return mse\n\n","type":"content","url":"/tp1-learning-problem#id-exercice-1-calculer-lerreur-quadratique-moyenne-mse","position":13},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl3":"📝 Exercice 2: Explorer différents degrés","lvl2":"Partie 2: Ajuster un polynôme"},"type":"lvl3","url":"/tp1-learning-problem#id-exercice-2-explorer-diff-rents-degr-s","position":14},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl3":"📝 Exercice 2: Explorer différents degrés","lvl2":"Partie 2: Ajuster un polynôme"},"content":"Modifiez la variable degree ci-dessous et observez:\n\nComment change la courbe?\n\nComment change le MSE?\n\n# ============================================\n# TODO: Essayez différentes valeurs: 1, 2, 5, 10, 15, 20\n# ============================================\ndegree = 2  # <- Modifiez cette valeur!\n\n# Ajustement\ncoeffs = np.polyfit(speed, dist, degree)\npredictions = np.polyval(coeffs, speed)\nmse = np.mean((dist - predictions)**2)\n\n# Visualisation\nplt.figure(figsize=(10, 5))\nplt.scatter(speed, dist, alpha=0.7, s=50, label='Données')\n\nspeed_grid = np.linspace(3, 26, 200)\npred_grid = np.polyval(coeffs, speed_grid)\npred_grid = np.clip(pred_grid, -50, 200)  # Limiter pour la visualisation\n\nplt.plot(speed_grid, pred_grid, 'r-', linewidth=2, label=f'Polynôme degré {degree}')\nplt.xlabel('Vitesse (mph)')\nplt.ylabel('Distance de freinage (ft)')\nplt.title(f'Degré {degree} — MSE = {mse:.1f}')\nplt.legend()\nplt.ylim(-20, 150)\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f\"\\nMSE: {mse:.2f}\")\n\n\n\n\n\n❓ Questions de réflexion:\n\nQuel degré donne le MSE le plus bas?\n\nLe polynôme de degré 20 semble-t-il raisonnable pour prédire à de nouvelles vitesses?\n\nC’est quoi le problème avec minimiser uniquement le MSE sur les données d’entraînement?\n\n","type":"content","url":"/tp1-learning-problem#id-exercice-2-explorer-diff-rents-degr-s","position":15},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl2":"Partie 3: Train/Test Split — Le surapprentissage"},"type":"lvl2","url":"/tp1-learning-problem#partie-3-train-test-split-le-surapprentissage","position":16},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl2":"Partie 3: Train/Test Split — Le surapprentissage"},"content":"Pour détecter le surapprentissage, nous séparons les données en:\n\nEnsemble d’entraînement (70%): pour ajuster le modèle\n\nEnsemble de test (30%): pour évaluer la généralisation\n\n# Séparation train/test\nnp.random.seed(42)  # Pour reproductibilité\nindices = np.random.permutation(len(speed))\nn_train = 35\n\ntrain_idx = indices[:n_train]\ntest_idx = indices[n_train:]\n\nspeed_train, dist_train = speed[train_idx], dist[train_idx]\nspeed_test, dist_test = speed[test_idx], dist[test_idx]\n\nprint(f\"Entraînement: {len(speed_train)} exemples\")\nprint(f\"Test: {len(speed_test)} exemples\")\n\n\n\n# Visualisation de la séparation\nplt.figure(figsize=(8, 5))\nplt.scatter(speed_train, dist_train, alpha=0.7, s=50, label='Entraînement')\nplt.scatter(speed_test, dist_test, alpha=0.7, s=50, marker='s', label='Test')\nplt.xlabel('Vitesse (mph)')\nplt.ylabel('Distance de freinage (ft)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n","type":"content","url":"/tp1-learning-problem#partie-3-train-test-split-le-surapprentissage","position":17},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl3":"📝 Exercice 3: Courbe biais-variance","lvl2":"Partie 3: Train/Test Split — Le surapprentissage"},"type":"lvl3","url":"/tp1-learning-problem#id-exercice-3-courbe-biais-variance","position":18},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl3":"📝 Exercice 3: Courbe biais-variance","lvl2":"Partie 3: Train/Test Split — Le surapprentissage"},"content":"Complétez le code pour calculer l’erreur sur train ET test pour chaque degré:\n\ndegrees = range(1, 16)\ntrain_errors = []\ntest_errors = []\n\nfor deg in degrees:\n    # Ajuster sur l'ensemble d'entraînement\n    coeffs = np.polyfit(speed_train, dist_train, deg)\n    \n    # ============================================\n    # TODO: Calculez les prédictions et erreurs\n    # ============================================\n    \n    # Prédictions sur train\n    pred_train = None  # <- Complétez avec np.polyval(...)\n    \n    # Prédictions sur test\n    pred_test = None  # <- Complétez avec np.polyval(...)\n    \n    # MSE sur train\n    mse_train = None  # <- Complétez\n    \n    # MSE sur test\n    mse_test = None  # <- Complétez\n    \n    train_errors.append(mse_train)\n    test_errors.append(mse_test)\n\n\n\n# Visualisation (exécutez après avoir complété le code ci-dessus)\nif None not in train_errors and None not in test_errors:\n    plt.figure(figsize=(10, 6))\n    plt.plot(degrees, train_errors, 'o-', linewidth=2, markersize=8, label='Erreur entraînement')\n    plt.plot(degrees, test_errors, 's-', linewidth=2, markersize=8, label='Erreur test')\n    plt.yscale('log')\n    plt.xlabel('Degré du polynôme (complexité)')\n    plt.ylabel('MSE (échelle log)')\n    plt.title('Compromis biais-variance')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.xticks(range(1, 16, 2))\n    plt.show()\n    \n    # Trouver le meilleur degré\n    best_deg = degrees[np.argmin(test_errors)]\n    print(f\"\\n🎯 Meilleur degré (selon erreur test): {best_deg}\")\n    print(f\"   MSE train: {train_errors[best_deg-1]:.1f}\")\n    print(f\"   MSE test: {test_errors[best_deg-1]:.1f}\")\nelse:\n    print(\"⚠️ Complétez le code de l'exercice 3!\")\n\n\n\n💡 Cliquez pour voir la solution# Prédictions sur train\npred_train = np.polyval(coeffs, speed_train)\n\n# Prédictions sur test\npred_test = np.polyval(coeffs, speed_test)\n\n# MSE sur train\nmse_train = np.mean((dist_train - pred_train)**2)\n\n# MSE sur test\nmse_test = np.mean((dist_test - pred_test)**2)\n\n","type":"content","url":"/tp1-learning-problem#id-exercice-3-courbe-biais-variance","position":19},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl2":"Partie 4: Régularisation Ridge"},"type":"lvl2","url":"/tp1-learning-problem#partie-4-r-gularisation-ridge","position":20},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl2":"Partie 4: Régularisation Ridge"},"content":"Au lieu de choisir un degré bas, on peut utiliser un polynôme de haut degré avec régularisation.\n\nLa régularisation Ridge ajoute une pénalité sur les coefficients:\\hat{\\boldsymbol{\\theta}}_{\\text{Ridge}} = \\arg\\min_{\\boldsymbol{\\theta}} \\left[ \\sum_{i=1}^{N} (y_i - \\boldsymbol{\\theta}^\\top \\mathbf{x}_i)^2 + \\lambda \\|\\boldsymbol{\\theta}\\|^2 \\right]\n\nLa solution en forme fermée est:\\hat{\\boldsymbol{\\theta}}_{\\text{Ridge}} = (\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n\n","type":"content","url":"/tp1-learning-problem#partie-4-r-gularisation-ridge","position":21},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl3":"📝 Exercice 4: Implémenter Ridge Regression","lvl2":"Partie 4: Régularisation Ridge"},"type":"lvl3","url":"/tp1-learning-problem#id-exercice-4-impl-menter-ridge-regression","position":22},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl3":"📝 Exercice 4: Implémenter Ridge Regression","lvl2":"Partie 4: Régularisation Ridge"},"content":"Complétez la fonction ci-dessous:\n\ndef polynomial_features(x, degree):\n    \"\"\"\n    Crée la matrice de caractéristiques polynomiales.\n    \n    Args:\n        x: vecteur d'entrées (n,)\n        degree: degré du polynôme\n    \n    Returns:\n        X: matrice (n, degree+1) avec colonnes [1, x, x², ..., x^degree]\n    \"\"\"\n    n = len(x)\n    X = np.zeros((n, degree + 1))\n    for d in range(degree + 1):\n        X[:, d] = x ** d\n    return X\n\n\ndef ridge_regression(X, y, lambda_reg):\n    \"\"\"\n    Calcule les coefficients Ridge.\n    \n    Args:\n        X: matrice de features (n, d)\n        y: vecteur cible (n,)\n        lambda_reg: coefficient de régularisation\n    \n    Returns:\n        theta: coefficients (d,)\n    \"\"\"\n    # ============================================\n    # TODO: Implémentez la formule Ridge\n    # theta = (X'X + lambda*I)^(-1) X'y\n    # \n    # Indices:\n    # - X.T pour la transposée\n    # - @ pour le produit matriciel\n    # - np.eye(n) pour la matrice identité n×n\n    # - np.linalg.inv() pour l'inverse\n    # ============================================\n    \n    d = X.shape[1]  # nombre de features\n    \n    theta = None  # <- Remplacez par votre code\n    \n    return theta\n\n\n\n# Test de votre implémentation\ndegree = 10\nlambda_reg = 1.0\n\nX_train = polynomial_features(speed_train, degree)\ntheta = ridge_regression(X_train, dist_train, lambda_reg)\n\nif theta is not None:\n    print(f\"Coefficients Ridge (degré={degree}, λ={lambda_reg}):\")\n    print(f\"  Norme des coefficients: {np.linalg.norm(theta):.2f}\")\n    \n    # Prédictions\n    X_test = polynomial_features(speed_test, degree)\n    pred_test = X_test @ theta\n    mse = np.mean((dist_test - pred_test)**2)\n    print(f\"  MSE test: {mse:.1f}\")\n    print(\"\\n✅ Implémentation correcte!\")\nelse:\n    print(\"⚠️ Complétez la fonction ridge_regression!\")\n\n\n\n💡 Cliquez pour voir la solutiondef ridge_regression(X, y, lambda_reg):\n    d = X.shape[1]\n    I = np.eye(d)\n    theta = np.linalg.inv(X.T @ X + lambda_reg * I) @ X.T @ y\n    return theta\n\n","type":"content","url":"/tp1-learning-problem#id-exercice-4-impl-menter-ridge-regression","position":23},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl3":"📝 Exercice 5: Trouver le meilleur λ","lvl2":"Partie 4: Régularisation Ridge"},"type":"lvl3","url":"/tp1-learning-problem#id-exercice-5-trouver-le-meilleur","position":24},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl3":"📝 Exercice 5: Trouver le meilleur λ","lvl2":"Partie 4: Régularisation Ridge"},"content":"Explorez différentes valeurs de λ pour un polynôme de degré 10:\n\ndegree = 10\nX_train = polynomial_features(speed_train, degree)\nX_test = polynomial_features(speed_test, degree)\n\nlambdas = [0, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\nresults = []\n\nprint(f\"{'λ':>10} | {'MSE Train':>10} | {'MSE Test':>10} | {'||θ||':>10}\")\nprint(\"-\" * 50)\n\nfor lam in lambdas:\n    theta = ridge_regression(X_train, dist_train, lam)\n    if theta is not None:\n        mse_train = np.mean((dist_train - X_train @ theta)**2)\n        mse_test = np.mean((dist_test - X_test @ theta)**2)\n        norm_theta = np.linalg.norm(theta)\n        results.append((lam, mse_train, mse_test, norm_theta))\n        print(f\"{lam:>10.3f} | {mse_train:>10.1f} | {mse_test:>10.1f} | {norm_theta:>10.1f}\")\n\n\n\n# Visualisation de l'effet de λ\nif len(results) > 0:\n    lambdas_plot = [r[0] for r in results]\n    train_plot = [r[1] for r in results]\n    test_plot = [r[2] for r in results]\n    \n    plt.figure(figsize=(10, 5))\n    plt.semilogx([max(l, 1e-4) for l in lambdas_plot], train_plot, 'o-', label='Train')\n    plt.semilogx([max(l, 1e-4) for l in lambdas_plot], test_plot, 's-', label='Test')\n    plt.xlabel('λ (échelle log)')\n    plt.ylabel('MSE')\n    plt.title(f'Effet de la régularisation Ridge (degré={degree})')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.show()\n\n\n\n❓ Questions:\n\nQuelle valeur de λ minimise l’erreur test?\n\nQue se passe-t-il quand λ → 0? Et quand λ → ∞?\n\nComment la norme des coefficients ||θ|| change avec λ?\n\n","type":"content","url":"/tp1-learning-problem#id-exercice-5-trouver-le-meilleur","position":25},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl2":"🎯 Récapitulatif"},"type":"lvl2","url":"/tp1-learning-problem#id-r-capitulatif","position":26},{"hierarchy":{"lvl1":"TP1: Le problème d’apprentissage","lvl2":"🎯 Récapitulatif"},"content":"Dans ce TP, vous avez appris:\n\nMSE: L’erreur quadratique moyenne mesure la qualité des prédictions\n\nSurapprentissage: Un modèle trop complexe mémorise le bruit\n\nErreur train ↓ mais erreur test ↑\n\nCompromis biais-variance:\n\nModèle simple = biais élevé (sous-apprentissage)\n\nModèle complexe = variance élevée (surapprentissage)\n\nRégularisation Ridge: Pénalise les grands coefficients\n\nPermet d’utiliser des modèles complexes sans surapprentissage\n\nλ contrôle la force de la régularisation\n\n📚 Pour aller plus loin: \n\nChapitre complet sur le site du cours","type":"content","url":"/tp1-learning-problem#id-r-capitulatif","position":27},{"hierarchy":{"lvl1":"Le problème d’apprentissage"},"type":"lvl1","url":"/learning-problem","position":0},{"hierarchy":{"lvl1":"Le problème d’apprentissage"},"content":"Objectifs d’apprentissage\n\nÀ la fin de ce chapitre, vous serez en mesure de:\n\nDéfinir formellement le problème d’apprentissage supervisé\n\nDistinguer les tâches de classification et de régression\n\nDéfinir le risque et le risque empirique\n\nExpliquer le principe de minimisation du risque empirique\n\nDériver l’estimateur du maximum de vraisemblance\n\nRelier le maximum de vraisemblance à la divergence de Kullback-Leibler\n\nIdentifier les sources d’écart entre performance mesurée et performance réelle\n\nLe chapitre précédent a présenté les méthodes non paramétriques, comme les k plus proches voisins, qui conservent les données d’entraînement et les consultent au moment de la prédiction. Cette approche est intuitive, mais elle a un coût: les données doivent rester en mémoire, et chaque prédiction requiert de parcourir l’ensemble d’entraînement. Ce chapitre développe une approche différente: plutôt que de garder les données, nous cherchons à les résumer dans un ensemble fixe de paramètres. L’apprentissage devient alors un problème d’optimisation.","type":"content","url":"/learning-problem","position":1},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Apprentissage supervisé"},"type":"lvl2","url":"/learning-problem#apprentissage-supervis","position":2},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Apprentissage supervisé"},"content":"Une ingénieure automobile mesure la distance de freinage d’un véhicule à différentes vitesses. Ses données ressemblent à ceci:\n\nVitesse (mph)\n\nDistance (ft)\n\n4\n\n2\n\n7\n\n4\n\n12\n\n20\n\n18\n\n56\n\n24\n\n93\n\nElle veut prédire la distance de freinage à 30 mph sans faire l’essai. Pour cela, elle cherche une fonction f telle que f(\\text{vitesse}) \\approx \\text{distance} sur ses observations. Si la fonction capture la relation sous-jacente, elle devrait donner une prédiction raisonnable pour des vitesses non mesurées.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Données de freinage (Ezekiel, 1930): vitesse (mph) vs distance d'arrêt (ft)\nspeed = np.array([4, 4, 7, 7, 8, 9, 10, 10, 10, 11, 11, 12, 12, 12, 12, 13, 13, 13, 13, 14,\n                  14, 14, 14, 15, 15, 15, 16, 16, 17, 17, 17, 18, 18, 18, 18, 19, 19, 19,\n                  20, 20, 20, 20, 20, 22, 23, 24, 24, 24, 24, 25], dtype=float)\ndist = np.array([2, 10, 4, 22, 16, 10, 18, 26, 34, 17, 28, 14, 20, 24, 28, 26, 34, 34, 46,\n                 26, 36, 60, 80, 20, 26, 54, 32, 40, 32, 40, 50, 42, 56, 76, 84, 36, 46,\n                 68, 32, 48, 52, 56, 64, 66, 54, 70, 92, 93, 120, 85], dtype=float)\n\nplt.figure(figsize=(6, 4))\nplt.scatter(speed, dist, alpha=0.7, label='Observations')\n\n# Fit quadratic\ncoeffs = np.polyfit(speed, dist, 2)\nspeed_grid = np.linspace(0, 30, 100)\ndist_pred = np.polyval(coeffs, speed_grid)\nplt.plot(speed_grid, dist_pred, 'k--', alpha=0.6, label='Fonction ajustée')\n\n# Prediction at 30 mph\npred_30 = np.polyval(coeffs, 30)\nplt.scatter([30], [pred_30], marker='x', s=80, color='C1', zorder=5, label=f'Prédiction à 30 mph: {pred_30:.0f} ft')\n\nplt.xlabel('Vitesse (mph)')\nplt.ylabel('Distance de freinage (ft)')\nplt.legend()\nplt.tight_layout()\n\n\n\nCe processus est l’ajustement de courbe (curve fitting). Nous avons des paires (entrée, sortie), nous ajustons une fonction, et nous utilisons cette fonction pour prédire. L’apprentissage supervisé généralise cette idée: les entrées peuvent être des vecteurs de dimension quelconque, les sorties peuvent être continues ou discrètes, et les fonctions candidates peuvent être bien plus complexes qu’un polynôme.\n\nFormellement, nous disposons d’un jeu de données \\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N composé de N paires, où chaque \\mathbf{x}_i \\in \\mathcal{X} est une entrée et y_i \\in \\mathcal{Y} est la sortie correspondante. L’objectif est de trouver une fonction f: \\mathcal{X} \\to \\mathcal{Y} qui approxime bien la relation entre entrées et sorties, y compris pour des exemples que nous n’avons pas encore observés.\n\nDans de nombreuses applications, les entrées sont des vecteurs de caractéristiques. Chaque exemple \\mathbf{x}_i \\in \\mathbb{R}^d est un vecteur de dimension d, où chaque composante représente une mesure ou un attribut. Pour prédire le prix d’une maison, les entrées pourraient être la superficie, le nombre de chambres et l’âge du bâtiment. Pour filtrer les pourriels, les entrées pourraient être des fréquences de mots. Pour diagnostiquer une maladie, les entrées pourraient être des résultats d’analyses sanguines.\n\nLorsque la sortie est une valeur continue, nous parlons de régression: f: \\mathbb{R}^d \\to \\mathbb{R} pour une sortie scalaire, ou f: \\mathbb{R}^d \\to \\mathbb{R}^p pour une sortie vectorielle. La distance de freinage, le prix d’une maison, la concentration d’un médicament dans le sang sont des exemples de régression.\n\nLorsque la sortie est une catégorie parmi un ensemble fini, nous parlons de classification. Pour la classification binaire, f: \\mathbb{R}^d \\to \\{0, 1\\}. Pour la classification multiclasse avec m catégories, f: \\mathbb{R}^d \\to \\{0, \\ldots, m-1\\}. Déterminer si un courriel est un pourriel, diagnostiquer une maladie, ou reconnaître un chiffre manuscrit sont des exemples de classification.","type":"content","url":"/learning-problem#apprentissage-supervis","position":3},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Mesurer l’erreur"},"type":"lvl2","url":"/learning-problem#mesurer-lerreur","position":4},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Mesurer l’erreur"},"content":"Pour choisir entre deux fonctions candidates, nous avons besoin d’un critère qui quantifie la qualité des prédictions. Une fonction de perte \\ell: \\mathcal{Y} \\times \\mathcal{Y} \\to \\mathbb{R}_+ mesure l’écart entre une prédiction \\hat{y} et la vraie valeur y. Une perte de zéro indique une prédiction parfaite; plus la perte est grande, plus l’erreur est importante.\n\nPour la régression, nous utilisons généralement la perte quadratique:\\ell_2(y, \\hat{y}) = (y - \\hat{y})^2\n\nCette perte pénalise les grandes erreurs de manière quadratique. Une erreur de 2 coûte quatre fois plus qu’une erreur de 1.\n\nReprenons les données de freinage. Supposons que notre fonction prédise 50 ft pour une vitesse où la vraie distance est 56 ft. La perte quadratique est (56 - 50)^2 = 36. Si elle prédit 70 ft, la perte est (56 - 70)^2 = 196. La perte quadratique pénalise sévèrement les grandes erreurs.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Données de freinage\nspeed = np.array([4, 4, 7, 7, 8, 9, 10, 10, 10, 11, 11, 12, 12, 12, 12, 13, 13, 13, 13, 14,\n                  14, 14, 14, 15, 15, 15, 16, 16, 17, 17, 17, 18, 18, 18, 18, 19, 19, 19,\n                  20, 20, 20, 20, 20, 22, 23, 24, 24, 24, 24, 25], dtype=float)\ndist = np.array([2, 10, 4, 22, 16, 10, 18, 26, 34, 17, 28, 14, 20, 24, 28, 26, 34, 34, 46,\n                 26, 36, 60, 80, 20, 26, 54, 32, 40, 32, 40, 50, 42, 56, 76, 84, 36, 46,\n                 68, 32, 48, 52, 56, 64, 66, 54, 70, 92, 93, 120, 85], dtype=float)\n\ncoeffs = np.polyfit(speed, dist, 2)\npredictions = np.polyval(coeffs, speed)\nresiduals = dist - predictions\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n# Left: predictions vs observations\nax = axes[0]\nax.scatter(speed, dist, alpha=0.7, label='Observations')\nspeed_grid = np.linspace(4, 25, 100)\nax.plot(speed_grid, np.polyval(coeffs, speed_grid), 'k--', alpha=0.6, label='Prédictions')\nfor i in range(len(speed)):\n    ax.plot([speed[i], speed[i]], [dist[i], predictions[i]], 'C1-', alpha=0.5)\nax.set_xlabel('Vitesse (mph)')\nax.set_ylabel('Distance (ft)')\nax.legend()\nax.set_title('Résidus: écarts entre observations et prédictions')\n\n# Right: histogram of squared residuals\nax = axes[1]\nax.hist(residuals**2, bins=15, edgecolor='black', alpha=0.7)\nax.set_xlabel(r'Perte quadratique $(y - \\hat{y})^2$')\nax.set_ylabel('Fréquence')\nax.set_title(f'EQM = {np.mean(residuals**2):.1f}')\n\nplt.tight_layout()\n\n\n\nPour la classification, un choix naturel est la perte 0-1:\\ell_{0-1}(y, \\hat{y}) = \\mathbb{1}_{y \\neq \\hat{y}} = \\begin{cases} 0 & \\text{si } y = \\hat{y} \\\\ 1 & \\text{si } y \\neq \\hat{y} \\end{cases}\n\nCette perte compte simplement les erreurs: elle vaut 1 pour une mauvaise prédiction, 0 sinon.\n\nLe choix de la fonction de perte dépend du problème. En diagnostic médical, manquer une maladie grave (faux négatif) peut avoir des conséquences bien plus importantes que de prescrire un test supplémentaire à un patient sain (faux positif). Une perte asymétrique refléterait cette différence. En régression, si les grandes erreurs sont particulièrement problématiques, la perte quadratique est appropriée; si nous voulons être robustes aux valeurs aberrantes, la perte absolue |y - \\hat{y}| est préférable.","type":"content","url":"/learning-problem#mesurer-lerreur","position":5},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Le risque"},"type":"lvl2","url":"/learning-problem#le-risque","position":6},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Le risque"},"content":"La perte évalue une seule prédiction. Pour évaluer un modèle dans son ensemble, nous voulons mesurer sa performance moyenne sur toutes les données possibles, pas seulement sur les exemples que nous avons observés.","type":"content","url":"/learning-problem#le-risque","position":7},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Pourquoi des variables aléatoires?","lvl2":"Le risque"},"type":"lvl3","url":"/learning-problem#pourquoi-des-variables-al-atoires","position":8},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Pourquoi des variables aléatoires?","lvl2":"Le risque"},"content":"Une question naturelle se pose: si nous ajustons une fonction déterministe f à des données, pourquoi avons-nous besoin de variables aléatoires et d’espérances? La fonction obtenue n’est-elle pas simplement une courbe fixe?\n\nLa réponse tient en un mot: généralisation. Nous ne nous intéressons pas vraiment à la performance sur les données d’entraînement car ces points sont déjà connus. Ce qui compte, c’est la performance sur des données futures que nous n’avons pas encore observées.\n\nConsidérons l’exemple de la distance de freinage. Les 50 mesures dans notre tableau sont un échantillon de toutes les mesures possibles. Si nous retournions sur le terrain et mesurions à nouveau, nous obtiendrions des valeurs légèrement différentes. En effet, le même véhicule à 20 mph ne s’arrête pas exactement à la même distance à chaque essai. Il y a de la variabilité intrinsèque: état de la route, température des freins, réflexes du conducteur.\n\nCette variabilité est capturée par une distribution de probabilité p(x, y). Nos 50 points sont des tirages de cette distribution. La question fondamentale devient alors:\n\nNotre modèle f prédira-t-il bien sur de nouveaux tirages de cette même distribution?\n\nLa fonction f elle-même est déterministe une fois entraînée. Mais son évaluation, savoir si elle prédit bien ou mal, dépend de quelles données futures elle rencontrera. Et ces données futures sont incertaines: elles seront tirées de p(x, y), mais nous ne savons pas lesquelles.\n\nLe risque formalise cette idée: c’est la perte moyenne que subira notre modèle f lorsqu’il sera confronté à des données tirées de p(x, y). C’est une mesure de performance prospective, tournée vers le futur.\n\nModèles déterministes vs stochastiques (et pourquoi on s’en fiche un peu)\n\nIl existe deux façons de raconter la même histoire.\n\nModèle déterministe: on suppose qu’il existe une relation y \\approx f^\\star(x), et que les écarts proviennent de facteurs non modélisés (mesure bruitée, variabilité du monde réel). Ici, f est une fonction déterministe; l’aléatoire vit dans les données que l’on observe et dans celles que l’on observera demain.\n\nModèle stochastique: on suppose plutôt que Y est une variable aléatoire conditionnellement à X=x, via une distribution p(y\\mid x). La \"bonne\" prédiction devient alors une question de moyenne/quantile/probabilité, selon la perte.\n\nDans la pratique, ces deux points de vue sont surtout des langages différents. Le formalisme probabiliste est souvent plus commode: il permet d’exprimer simplement \"la performance moyenne sur des données futures\" via une espérance. Ce chapitre adopte ce langage parce qu’il rend la généralisation et les garanties mathématiques plus propres, sans changer l’objectif final: produire une règle de prédiction utile.","type":"content","url":"/learning-problem#pourquoi-des-variables-al-atoires","position":9},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Définition formelle","lvl2":"Le risque"},"type":"lvl3","url":"/learning-problem#d-finition-formelle","position":10},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Définition formelle","lvl2":"Le risque"},"content":"Le risque d’une fonction f est l’espérance de la perte sur la distribution des données:\\mathcal{R}(f) = \\mathbb{E}_{(\\mathbf{X},Y) \\sim p}\\left[\\ell(Y, f(\\mathbf{X}))\\right] = \\int \\ell(y, f(\\mathbf{x})) \\, p(\\mathbf{x}, y) \\, d\\mathbf{x} \\, dy\n\nDécomposons cette formule étape par étape:\n\n\\mathbb{E}_{(\\mathbf{X},Y) \\sim p}: L’espérance mathématique signifie “moyenne sur tous les exemples possibles”. La notation (\\mathbf{X},Y) \\sim p indique que nous tirons les paires (\\mathbf{x}, y) selon la distribution p(\\mathbf{x}, y) de la nature.\n\n\\ell(Y, f(\\mathbf{X})): Pour chaque exemple aléatoire (\\mathbf{X}, Y), nous calculons la perte entre la vraie valeur Y et la prédiction f(\\mathbf{X}) du modèle.\n\nL’intégrale \\int \\ell(y, f(\\mathbf{x})) \\, p(\\mathbf{x}, y) \\, d\\mathbf{x} \\, dy: Cette intégrale calcule une moyenne pondérée. Pour chaque paire possible (\\mathbf{x}, y), nous multiplions la perte \\ell(y, f(\\mathbf{x})) par la probabilité p(\\mathbf{x}, y) que cette paire apparaisse dans la nature, puis nous sommons (intégrons) sur toutes les paires possibles.","type":"content","url":"/learning-problem#d-finition-formelle","position":11},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Exemple concret","lvl2":"Le risque"},"type":"lvl3","url":"/learning-problem#exemple-concret","position":12},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Exemple concret","lvl2":"Le risque"},"content":"Considérons un problème de classification binaire en 2D. Supposons que \\mathbf{x} \\in [0, 1]^2 et y \\in \\{0, 1\\}. Pour calculer le risque, nous devrions:\n\nDiviser l’espace [0,1]^2 en une grille fine (par exemple, 1000 \\times 1000 points)\n\nPour chaque point \\mathbf{x} de la grille, considérer les deux valeurs possibles de y (0 et 1)\n\nPour chaque combinaison (\\mathbf{x}, y), calculer:\n\nLa probabilité p(\\mathbf{x}, y) que cette combinaison apparaisse\n\nLa perte \\ell(y, f(\\mathbf{x})) si notre modèle prédit f(\\mathbf{x})\n\nFaire la somme pondérée: \\sum_{\\mathbf{x}} \\sum_{y \\in \\{0,1\\}} \\ell(y, f(\\mathbf{x})) \\cdot p(\\mathbf{x}, y)\n\nEn pratique, pour un espace continu, cette somme devient une intégrale sur un domaine continu, ce qui est encore plus complexe à calculer.\n\nVisualisons ceci concrètement. La figure suivante montre un problème de classification binaire où chaque classe suit une distribution gaussienne en 2D. Les contours représentent la densité p(x|y) pour chaque classe. La ligne pointillée est la frontière de décision d’un classificateur linéaire. Les régions ombrées indiquent où le classificateur fait des erreurs: la région rouge correspond aux points de classe 0 classés comme classe 1, et la région bleue correspond aux points de classe 1 classés comme classe 0.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Paramètres du mélange gaussien (classification binaire 2D)\nmu0 = np.array([0.0, 0.0])\nmu1 = np.array([2.0, 1.0])\ncov = np.array([[1.0, 0.3], [0.3, 1.0]])\n\ndef gaussian_pdf(x, mu, cov):\n    \"\"\"PDF d'une gaussienne multivariée.\"\"\"\n    d = len(mu)\n    diff = x - mu\n    cov_inv = np.linalg.inv(cov)\n    mahal = np.einsum('...i,ij,...j->...', diff, cov_inv, diff)\n    return np.exp(-0.5 * mahal) / np.sqrt((2 * np.pi) ** d * np.linalg.det(cov))\n\n# Create grid for visualization\nx_range = np.linspace(-3, 5, 200)\ny_range = np.linspace(-3, 4, 200)\nX_grid, Y_grid = np.meshgrid(x_range, y_range)\npos = np.dstack([X_grid, Y_grid])\n\n# Compute class-conditional densities\np_x_given_0 = gaussian_pdf(pos, mu0, cov)\np_x_given_1 = gaussian_pdf(pos, mu1, cov)\n\n# Joint densities (with equal priors)\nprior = 0.5\np_x_y0 = p_x_given_0 * (1 - prior)\np_x_y1 = p_x_given_1 * prior\n\n# Linear decision boundary (Bayes optimal for equal covariances)\n# w^T x + b = 0 where w = Sigma^{-1}(mu1 - mu0)\ncov_inv = np.linalg.inv(cov)\nw = cov_inv @ (mu1 - mu0)\nb = -0.5 * (mu1 @ cov_inv @ mu1 - mu0 @ cov_inv @ mu0)\n\n# Decision boundary: w[0]*x + w[1]*y + b = 0  =>  y = -(w[0]*x + b)/w[1]\nx_boundary = np.linspace(-3, 5, 100)\ny_boundary = -(w[0] * x_boundary + b) / w[1]\n\n# Classifier prediction: classify as 1 if w^T x + b > 0\npredictions = (w[0] * X_grid + w[1] * Y_grid + b) > 0\n\n# Misclassification regions\n# Class 0 misclassified as 1: true class is 0, but prediction is 1\nmisclass_0 = predictions  # region where we predict 1\n# Class 1 misclassified as 0: true class is 1, but prediction is 0\nmisclass_1 = ~predictions  # region where we predict 0\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Plot class-conditional densities as contours\nlevels = [0.01, 0.05, 0.1, 0.15]\nax.contour(X_grid, Y_grid, p_x_given_0, levels=levels, colors='C0', alpha=0.7)\nax.contour(X_grid, Y_grid, p_x_given_1, levels=levels, colors='C1', alpha=0.7)\n\n# Shade misclassification regions weighted by probability\n# Red: class 0 points incorrectly classified as 1\nerror_region_0 = np.where(misclass_0, p_x_y0, 0)\n# Blue: class 1 points incorrectly classified as 0  \nerror_region_1 = np.where(misclass_1, p_x_y1, 0)\n\nax.contourf(X_grid, Y_grid, error_region_0, levels=[0.001, 0.01, 0.05, 0.1], \n            colors=['#ff000010', '#ff000030', '#ff000050'], extend='max')\nax.contourf(X_grid, Y_grid, error_region_1, levels=[0.001, 0.01, 0.05, 0.1],\n            colors=['#0000ff10', '#0000ff30', '#0000ff50'], extend='max')\n\n# Decision boundary\nax.plot(x_boundary, y_boundary, 'k--', linewidth=2, label='Frontière de décision')\n\n# Class centers\nax.scatter(*mu0, s=100, c='C0', marker='x', linewidths=3, zorder=5, label='Centre classe 0')\nax.scatter(*mu1, s=100, c='C1', marker='x', linewidths=3, zorder=5, label='Centre classe 1')\n\nax.set_xlim(-3, 5)\nax.set_ylim(-3, 4)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.legend(loc='upper left')\nax.set_aspect('equal')\n\nplt.tight_layout()\n\n\n\nLe risque est l’intégrale de la perte sur tout l’espace, pondérée par p(\\mathbf{x}, y). Les régions ombrées contribuent au risque: chaque point dans ces régions est mal classé, et sa contribution dépend de la densité de probabilité à cet endroit. Les régions denses proches de la frontière contribuent le plus au risque.","type":"content","url":"/learning-problem#exemple-concret","position":13},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Pourquoi le risque est important","lvl2":"Le risque"},"type":"lvl3","url":"/learning-problem#pourquoi-le-risque-est-important","position":14},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Pourquoi le risque est important","lvl2":"Le risque"},"content":"Le risque mesure ce que nous obtiendrons en moyenne si nous appliquons f à de nouvelles données tirées de la même distribution. Un modèle avec un faible risque fait de bonnes prédictions en général, pas seulement sur les exemples d’entraînement. C’est exactement ce que nous voulons optimiser: un modèle qui performe bien sur des données jamais vues, pas seulement sur celles qu’il a déjà observées.\n\nCette quantité est ce que nous voulons minimiser. Le problème fondamental est que nous ne connaissons pas la distribution p(\\mathbf{x}, y) de la nature. Nous n’y avons accès qu’indirectement, via un échantillon fini \\mathcal{D}.","type":"content","url":"/learning-problem#pourquoi-le-risque-est-important","position":15},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Le risque empirique"},"type":"lvl2","url":"/learning-problem#le-risque-empirique","position":16},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Le risque empirique"},"content":"Puisque le risque est inaccessible, nous l’approximons par une moyenne sur les données disponibles. Le risque empirique est:\\hat{\\mathcal{R}}(f, \\mathcal{D}) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell(y_i, f(\\mathbf{x}_i))\n\nCette quantité est calculable: c’est la moyenne des pertes sur l’échantillon d’entraînement. Pour la perte 0-1, le risque empirique est le taux d’erreur sur les données d’entraînement. Pour la perte quadratique, c’est l’erreur quadratique moyenne.","type":"content","url":"/learning-problem#le-risque-empirique","position":17},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Pourquoi le risque est-il inaccessible?","lvl2":"Le risque empirique"},"type":"lvl3","url":"/learning-problem#pourquoi-le-risque-est-il-inaccessible","position":18},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Pourquoi le risque est-il inaccessible?","lvl2":"Le risque empirique"},"content":"La nécessité d’utiliser le risque empirique découle de deux obstacles fondamentaux, l’un conceptuel et l’autre computationnel.","type":"content","url":"/learning-problem#pourquoi-le-risque-est-il-inaccessible","position":19},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Obstacle 1: La distribution p(\\mathbf{x}, y) est inconnue","lvl3":"Pourquoi le risque est-il inaccessible?","lvl2":"Le risque empirique"},"type":"lvl4","url":"/learning-problem#obstacle-1-la-distribution-p-mathbf-x-y-est-inconnue","position":20},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Obstacle 1: La distribution p(\\mathbf{x}, y) est inconnue","lvl3":"Pourquoi le risque est-il inaccessible?","lvl2":"Le risque empirique"},"content":"La nature possède une distribution p(\\mathbf{x}, y) qui génère les données, mais nous ne la connaissons pas. Nous n’observons qu’un échantillon fini \\mathcal{D} = \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N tiré de cette distribution.\n\nL’ensemble \\mathcal{D} est une variable aléatoire: si nous répétions l’expérience de collecte de données, nous obtiendrions un échantillon différent. Cette perspective, adoptée notamment dans \n\nMurphy (2022), rappelle que nos conclusions dépendent de l’échantillon particulier que nous avons observé. C’est comme si nous regardions quelques gouttes d’eau d’un océan: nous pouvons analyser ces gouttes, mais un autre prélèvement donnerait des gouttes différentes.\n\nMême si nous tentions d’estimer p(\\mathbf{x}, y) à partir des données (par exemple, via des techniques d’estimation de densité comme les mélanges de gaussiennes ou les estimateurs à noyau), nous n’obtiendrions qu’une approximation \\hat{p}(\\mathbf{x}, y) de la vraie distribution. Cette approximation serait elle-même imparfaite et dépendrait de nos hypothèses sur la forme de la distribution.\n\nLa figure suivante illustre ce problème. À gauche, la vraie distribution p(\\mathbf{x}, y) que la nature utilise pour générer les données (que nous ne connaissons pas). À droite, un échantillon de N = 50 points tirés de cette distribution (ce que nous observons).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Paramètres du mélange gaussien\nmu0, mu1 = np.array([0.0, 0.0]), np.array([2.0, 1.0])\ncov = np.array([[1.0, 0.3], [0.3, 1.0]])\n\ndef gaussian_pdf(x, mu, cov):\n    d = len(mu)\n    diff = x - mu\n    cov_inv = np.linalg.inv(cov)\n    mahal = np.einsum('...i,ij,...j->...', diff, cov_inv, diff)\n    return np.exp(-0.5 * mahal) / np.sqrt((2 * np.pi) ** d * np.linalg.det(cov))\n\n# Générer un échantillon\nrng = np.random.default_rng(42)\nn = 50\nX0 = rng.multivariate_normal(mu0, cov, n // 2)\nX1 = rng.multivariate_normal(mu1, cov, n // 2)\nX = np.vstack([X0, X1])\ny = np.concatenate([np.zeros(n // 2), np.ones(n // 2)])\n\n# Grille pour visualisation\nx_range = np.linspace(-3, 5, 150)\ny_range = np.linspace(-3, 4, 150)\nX_grid, Y_grid = np.meshgrid(x_range, y_range)\npos = np.dstack([X_grid, Y_grid])\n\np_x_given_0 = gaussian_pdf(pos, mu0, cov)\np_x_given_1 = gaussian_pdf(pos, mu1, cov)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Left: True distribution (what nature knows)\nax = axes[0]\nax.contourf(X_grid, Y_grid, p_x_given_0, levels=15, cmap='Blues', alpha=0.6)\nax.contourf(X_grid, Y_grid, p_x_given_1, levels=15, cmap='Oranges', alpha=0.6)\nax.contour(X_grid, Y_grid, p_x_given_0, levels=5, colors='C0', alpha=0.8)\nax.contour(X_grid, Y_grid, p_x_given_1, levels=5, colors='C1', alpha=0.8)\nax.scatter(*mu0, s=100, c='C0', marker='x', linewidths=3, zorder=5)\nax.scatter(*mu1, s=100, c='C1', marker='x', linewidths=3, zorder=5)\nax.set_xlim(-3, 5)\nax.set_ylim(-3, 4)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_title('Distribution vraie $p(x, y)$\\n(inconnue)')\nax.set_aspect('equal')\n\n# Right: Finite sample (what we observe)\nax = axes[1]\nax.scatter(X[y == 0, 0], X[y == 0, 1], c='C0', alpha=0.7, s=50, label='Classe 0')\nax.scatter(X[y == 1, 0], X[y == 1, 1], c='C1', alpha=0.7, s=50, label='Classe 1')\nax.set_xlim(-3, 5)\nax.set_ylim(-3, 4)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_title(f'Échantillon observé $\\\\mathcal{{D}}$\\n($N = {len(X)}$ points)')\nax.legend()\nax.set_aspect('equal')\n\nplt.tight_layout()\n\n\n\nNous ne voyons que les points à droite. La structure continue à gauche, incluant les contours, les densités, ainsi que les régions de haute et basse probabilité, nous est cachée. C’est à partir de ces quelques points que nous devons estimer la performance de notre modèle.","type":"content","url":"/learning-problem#obstacle-1-la-distribution-p-mathbf-x-y-est-inconnue","position":21},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Obstacle 2: L’intégration est computationnellement intractable","lvl3":"Pourquoi le risque est-il inaccessible?","lvl2":"Le risque empirique"},"type":"lvl4","url":"/learning-problem#obstacle-2-lint-gration-est-computationnellement-intractable","position":22},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Obstacle 2: L’intégration est computationnellement intractable","lvl3":"Pourquoi le risque est-il inaccessible?","lvl2":"Le risque empirique"},"content":"Supposons, par un miracle, que nous connaissions exactement p(\\mathbf{x}, y). Pourrions-nous alors calculer le risque \\mathcal{R}(f) = \\int \\ell(y, f(\\mathbf{x})) \\, p(\\mathbf{x}, y) \\, d\\mathbf{x} \\, dy?\n\nLa réponse est généralement non, pour plusieurs raisons:\n\nPour les espaces continus: L’intégrale est une intégrale de grande dimension. Si \\mathbf{x} \\in \\mathbb{R}^d avec d grand (par exemple, d = 1000 pour des images ou d = 10^6 pour des données textuelles), nous devons intégrer sur un espace de dimension d+1.\n\nPour vous rappeler l’idée, en calcul on approche une intégrale en 1D par une somme: on découpe l’intervalle en petites tranches et on additionne des aires de rectangles ou de trapèzes. Par exemple, sur [a,b]:\\int_a^b g(x)\\,dx \\;\\approx\\; \\sum_{m=1}^{M} g(x_m)\\,\\Delta x\n\nCette idée générale, qui consiste à remplacer une intégrale par une somme pondérée de valeurs de g évaluées à des points x_m, s’appelle l’intégration numérique (ou quadrature).\n\nLe problème en apprentissage est que notre intégrale n’est pas en 1D. Si on applique le même raisonnement en dimension d en mettant, disons, M points par dimension, on obtient une grille de taille M^d (et ici d peut être très grand). Le nombre de points à évaluer explose donc exponentiellement avec d. C’est exactement la malédiction de la dimensionnalité.\n\nPour les espaces discrets: Si \\mathbf{x} et y sont discrets mais prennent de nombreuses valeurs, la somme \\sum_{\\mathbf{x}} \\sum_y \\ell(y, f(\\mathbf{x})) \\cdot p(\\mathbf{x}, y) peut avoir un nombre exponentiel de termes. Par exemple, si \\mathbf{x} est un vecteur binaire de dimension d, il y a 2^d valeurs possibles pour \\mathbf{x}. Pour d = 100, cela fait déjà 2^{100} \\approx 10^{30} termes à sommer, ce qui est computationnellement impossible.\n\nIntégration de Monte Carlo: On pourrait penser utiliser l’intégration de Monte Carlo: tirer des échantillons (\\mathbf{x}, y) selon p(\\mathbf{x}, y) et estimer l’intégrale par la moyenne empirique. Mais pour obtenir une estimation précise du risque, nous aurions besoin d’un très grand nombre d’échantillons (potentiellement infini pour une précision parfaite). De plus, cela nécessiterait de pouvoir échantillonner efficacement depuis p(\\mathbf{x}, y), ce qui est lui-même un problème difficile si la distribution est complexe.\n\nLa figure suivante illustre la malédiction de la dimensionnalité. Avec seulement 10 points par dimension pour une quadrature numérique, le nombre total de points d’évaluation explose rapidement.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Number of grid points per dimension\npoints_per_dim = 10\n\n# Dimensions to consider\ndimensions = np.array([1, 2, 3, 5, 10, 20, 50, 100])\n\n# Total grid points = points_per_dim^d\ntotal_points = points_per_dim ** dimensions.astype(float)\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nbars = ax.bar(range(len(dimensions)), total_points, color='steelblue', edgecolor='black')\n\n# Add reference lines\nax.axhline(y=1e9, color='C1', linestyle='--', alpha=0.7, label='1 milliard (limite pratique)')\nax.axhline(y=1e80, color='C3', linestyle=':', alpha=0.7, label='$10^{80}$ (atomes dans l\\'univers)')\n\nax.set_yscale('log')\nax.set_xticks(range(len(dimensions)))\nax.set_xticklabels([f'd={d}' for d in dimensions])\nax.set_xlabel('Dimension de l\\'espace des entrées')\nax.set_ylabel('Nombre de points de grille')\nax.set_title(f'Points nécessaires pour l\\'intégration numérique\\n({points_per_dim} points par dimension)')\nax.legend(loc='upper left')\n\n# Annotate a few bars\nfor i, (d, n) in enumerate(zip(dimensions, total_points)):\n    if d <= 5:\n        ax.annotate(f'$10^{{{d}}}$', (i, n), ha='center', va='bottom', fontsize=9)\n    elif d == 10:\n        ax.annotate(f'$10^{{{10}}}$', (i, n), ha='center', va='bottom', fontsize=9)\n    elif d == 100:\n        ax.annotate(f'$10^{{{100}}}$', (i, n), ha='center', va='bottom', fontsize=9)\n\nax.set_ylim(1, 1e105)\n\nplt.tight_layout()\n\n\n\nEn dimension 10, il faut déjà \n\n1010 points, soit dix milliards. En dimension 100, il en faut \n\n10100, un nombre qui dépasse le nombre d’atomes dans l’univers observable. L’intégration numérique directe est donc impossible pour les problèmes de haute dimension, même si nous connaissions p(\\mathbf{x}, y) exactement.","type":"content","url":"/learning-problem#obstacle-2-lint-gration-est-computationnellement-intractable","position":23},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Le risque empirique comme seule option pratique","lvl2":"Le risque empirique"},"type":"lvl3","url":"/learning-problem#le-risque-empirique-comme-seule-option-pratique","position":24},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Le risque empirique comme seule option pratique","lvl2":"Le risque empirique"},"content":"Face à ces obstacles, le risque empirique est notre seule option calculable. Mais il y a une bonne nouvelle: le risque empirique est une forme d’intégration de Monte Carlo, et Monte Carlo a une propriété remarquable.\n\nMéthode\n\nComplexité\n\nExigence\n\nQuadrature (règles trapézoïdales, etc.)\n\nO(M^d)\n\nConnaître p(\\mathbf{x},y) exactement\n\nMonte Carlo\n\nO(N)\n\nAvoir des échantillons de p(\\mathbf{x},y)\n\nLa complexité de Monte Carlo est indépendante de la dimension d. Elle ne dépend que du nombre d’échantillons N. C’est cette propriété qui rend l’apprentissage possible en haute dimension. De plus, nous n’avons pas besoin de connaître la valeur numérique de p(\\mathbf{x},y): nous avons seulement besoin de pouvoir tirer des échantillons de cette distribution. C’est exactement ce que nos données d’entraînement nous fournissent.\n\nLe risque empirique remplace l’intégrale sur la distribution inconnue par une moyenne sur l’échantillon fini que nous possédons:\\hat{\\mathcal{R}}(f, \\mathcal{D}) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell(y_i, f(\\mathbf{x}_i))\n\nCette formule est directe à évaluer: nous parcourons nos N exemples d’entraînement, calculons la perte pour chacun, et faisons la moyenne.\n\nReprenons les données de freinage. Divisons-les en deux parties: les mesures à vitesses faibles (4-19 mph) pour l’entraînement, et les mesures à vitesses élevées (20-25 mph) pour le test. Le risque empirique sur l’ensemble d’entraînement mesure la qualité de l’ajustement. Le risque empirique sur l’ensemble de test estime la performance sur des vitesses non vues pendant l’entraînement.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Données de freinage\nspeed = np.array([4, 4, 7, 7, 8, 9, 10, 10, 10, 11, 11, 12, 12, 12, 12, 13, 13, 13, 13, 14,\n                  14, 14, 14, 15, 15, 15, 16, 16, 17, 17, 17, 18, 18, 18, 18, 19, 19, 19,\n                  20, 20, 20, 20, 20, 22, 23, 24, 24, 24, 24, 25], dtype=float)\ndist = np.array([2, 10, 4, 22, 16, 10, 18, 26, 34, 17, 28, 14, 20, 24, 28, 26, 34, 34, 46,\n                 26, 36, 60, 80, 20, 26, 54, 32, 40, 32, 40, 50, 42, 56, 76, 84, 36, 46,\n                 68, 32, 48, 52, 56, 64, 66, 54, 70, 92, 93, 120, 85], dtype=float)\n\n# Split: train on low speeds, test on high speeds\ntrain_mask = speed < 20\ntest_mask = speed >= 20\n\nspeed_train, dist_train = speed[train_mask], dist[train_mask]\nspeed_test, dist_test = speed[test_mask], dist[test_mask]\n\n# Fit on training data\ncoeffs = np.polyfit(speed_train, dist_train, 2)\n\n# Compute MSE on train and test\npred_train = np.polyval(coeffs, speed_train)\npred_test = np.polyval(coeffs, speed_test)\nmse_train = np.mean((dist_train - pred_train)**2)\nmse_test = np.mean((dist_test - pred_test)**2)\n\nplt.figure(figsize=(7, 4))\nplt.scatter(speed_train, dist_train, alpha=0.7, label=f'Entraînement (EQM={mse_train:.1f})')\nplt.scatter(speed_test, dist_test, alpha=0.7, marker='s', label=f'Test (EQM={mse_test:.1f})')\n\nspeed_grid = np.linspace(4, 28, 100)\nplt.plot(speed_grid, np.polyval(coeffs, speed_grid), 'k--', alpha=0.6, label='Fonction ajustée')\n\nplt.axvline(x=20, color='gray', linestyle=':', alpha=0.5)\nplt.xlabel('Vitesse (mph)')\nplt.ylabel('Distance (ft)')\nplt.legend()\nplt.tight_layout()\n\n\n\nDans cet exemple, l’EQM (MSE, erreur quadratique moyenne) sur l’ensemble de test est plus élevé que sur l’ensemble d’entraînement. Cet écart est typique: la fonction a été optimisée pour les données d’entraînement, pas pour les données de test.\n\nSous l’hypothèse que les exemples (\\mathbf{x}_i, y_i) sont tirés indépendamment et identiquement distribués (i.i.d.) selon p(\\mathbf{x}, y), le risque empirique est un estimateur non biaisé du vrai risque: \\mathbb{E}[\\hat{\\mathcal{R}}(f, \\mathcal{D})] = \\mathcal{R}(f). Cela signifie qu’en moyenne, sur tous les échantillons possibles, le risque empirique est égal au vrai risque.\n\nPar la loi des grands nombres, lorsque N \\to \\infty, le risque empirique converge vers le vrai risque (presque sûrement). Avec suffisamment de données, si l’échantillon est représentatif de la distribution, le risque empirique devrait être proche du risque.\n\nLa figure suivante illustre cette convergence. Nous utilisons le problème de classification gaussienne pour lequel nous pouvons calculer le vrai risque analytiquement. Chaque courbe montre l’évolution du risque empirique pour un échantillon de taille croissante. Toutes les courbes convergent vers le vrai risque (ligne pointillée), mais avec des fluctuations qui diminuent à mesure que N augmente.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\n\n# Paramètres du mélange gaussien\nmu0, mu1 = np.array([0.0, 0.0]), np.array([2.0, 1.0])\ncov = np.array([[1.0, 0.3], [0.3, 1.0]])\n\ndef gaussian_pdf(x, mu, cov):\n    d = len(mu)\n    diff = x - mu\n    cov_inv = np.linalg.inv(cov)\n    mahal = np.einsum('...i,ij,...j->...', diff, cov_inv, diff)\n    return np.exp(-0.5 * mahal) / np.sqrt((2 * np.pi) ** d * np.linalg.det(cov))\n\n# Compute Bayes-optimal classifier error rate (true risk)\n# For Gaussian classes with equal covariance, the Bayes error is:\n# P(error) = Phi(-d/2) where d is the Mahalanobis distance between means\ncov_inv = np.linalg.inv(cov)\nd_squared = (mu1 - mu0) @ cov_inv @ (mu1 - mu0)\nd = np.sqrt(d_squared)\ntrue_risk = norm.cdf(-d / 2)\n\n# Simulate empirical risk for different sample sizes\nsample_sizes = np.arange(10, 1001, 10)\nn_runs = 20\n\nfig, ax = plt.subplots(figsize=(9, 5))\n\n# Store all runs for confidence band\nall_risks = np.zeros((n_runs, len(sample_sizes)))\n\nfor run in range(n_runs):\n    empirical_risks = []\n    # Generate a large dataset and compute cumulative empirical risk\n    rng = np.random.default_rng(run)\n    n_total = 1000\n    X0 = rng.multivariate_normal(mu0, cov, n_total // 2)\n    X1 = rng.multivariate_normal(mu1, cov, n_total // 2)\n    X = np.vstack([X0, X1])\n    y = np.concatenate([np.zeros(n_total // 2), np.ones(n_total // 2)])\n    perm = rng.permutation(n_total)\n    X, y = X[perm], y[perm]\n    \n    # Bayes-optimal classifier: predict 1 if w^T x + b > 0\n    w = cov_inv @ (mu1 - mu0)\n    b = -0.5 * (mu1 @ cov_inv @ mu1 - mu0 @ cov_inv @ mu0)\n    \n    for n in sample_sizes:\n        X_n, y_n = X[:n], y[:n]\n        predictions = (X_n @ w + b > 0).astype(float)\n        emp_risk = np.mean(predictions != y_n)\n        empirical_risks.append(emp_risk)\n    \n    all_risks[run] = empirical_risks\n    ax.plot(sample_sizes, empirical_risks, 'C0-', alpha=0.15, linewidth=0.8)\n\n# Mean and confidence bands\nmean_risk = np.mean(all_risks, axis=0)\nstd_risk = np.std(all_risks, axis=0)\nax.fill_between(sample_sizes, mean_risk - 2*std_risk, mean_risk + 2*std_risk, \n                alpha=0.3, color='C0', label='Intervalle ± 2 écarts-types')\nax.plot(sample_sizes, mean_risk, 'C0-', linewidth=2, label='Moyenne empirique')\n\n# True risk\nax.axhline(y=true_risk, color='C3', linestyle='--', linewidth=2, \n           label=f'Vrai risque = {true_risk:.3f}')\n\nax.set_xlabel('Taille de l\\'échantillon $N$')\nax.set_ylabel('Risque empirique (taux d\\'erreur)')\nax.set_title('Convergence du risque empirique vers le vrai risque')\nax.legend(loc='upper right')\nax.set_xlim(0, 1000)\nax.set_ylim(0, 0.35)\n\nplt.tight_layout()\n\n\n\nAvec N = 50, le risque empirique peut facilement varier de 0.10 à 0.25 selon l’échantillon. Avec N = 500, la variabilité est beaucoup plus faible. C’est la loi des grands nombres en action: plus l’échantillon est grand, plus l’estimation est précise.","type":"content","url":"/learning-problem#le-risque-empirique-comme-seule-option-pratique","position":25},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Le compromis fondamental","lvl2":"Le risque empirique"},"type":"lvl3","url":"/learning-problem#le-compromis-fondamental","position":26},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Le compromis fondamental","lvl2":"Le risque empirique"},"content":"Cette situation crée un compromis fondamental en apprentissage automatique:\n\nCe que nous voulons minimiser: Le risque \\mathcal{R}(f), qui mesure la performance sur toutes les données possibles\n\nCe que nous pouvons minimiser: Le risque empirique \\hat{\\mathcal{R}}(f, \\mathcal{D}), qui mesure la performance sur nos données d’entraînement\n\nL’écart entre ces deux quantités est au cœur de l’apprentissage automatique. Un modèle peut avoir un risque empirique très faible (il performe bien sur les données d’entraînement) tout en ayant un risque élevé (il performe mal sur de nouvelles données). C’est le problème du surapprentissage, que nous explorerons plus en détail dans le chapitre sur la généralisation.\n\nLa question de savoir quand et à quelle vitesse l’approximation du risque par le risque empirique est fiable relève de la théorie de la généralisation, que nous aborderons au chapitre suivant.","type":"content","url":"/learning-problem#le-compromis-fondamental","position":27},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Minimisation du risque empirique"},"type":"lvl2","url":"/learning-problem#minimisation-du-risque-empirique","position":28},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Minimisation du risque empirique"},"content":"Nous avons maintenant les éléments pour formuler l’apprentissage comme un problème d’optimisation. Nous cherchons la fonction f dans une classe \\mathcal{F} qui minimise le risque:f^\\star = \\arg\\min_{f \\in \\mathcal{F}} \\mathcal{R}(f)\n\nPuisque le risque est inaccessible, nous le remplaçons par le risque empirique:\\hat{f} = \\arg\\min_{f \\in \\mathcal{F}} \\hat{\\mathcal{R}}(f, \\mathcal{D})\n\nCe principe est la minimisation du risque empirique (MRE): choisir la fonction qui fait le moins d’erreurs sur les données d’entraînement, en espérant que cette performance se transfère aux nouvelles données.\n\nLa classe \\mathcal{F} est notre classe d’hypothèses. Elle représente l’ensemble des fonctions que nous sommes prêts à considérer. Le choix de \\mathcal{F} encode nos hypothèses sur la forme de la relation entre entrées et sorties.","type":"content","url":"/learning-problem#minimisation-du-risque-empirique","position":29},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Un premier exemple: les modèles linéaires","lvl2":"Minimisation du risque empirique"},"type":"lvl3","url":"/learning-problem#un-premier-exemple-les-mod-les-lin-aires","position":30},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Un premier exemple: les modèles linéaires","lvl2":"Minimisation du risque empirique"},"content":"Pour rendre ces concepts concrets, considérons la classe la plus simple: les modèles linéaires. Un modèle linéaire suppose que la sortie est une combinaison linéaire des entrées:f(\\mathbf{x}; \\boldsymbol{\\theta}) = \\theta_0 + \\sum_{j=1}^d \\theta_j x_j = \\boldsymbol{\\theta}^\\top \\mathbf{x}\n\noù \\mathbf{x} \\in \\mathbb{R}^{d+1} est le vecteur d’entrée augmenté d’un 1 pour le biais (x_0 = 1), et \\boldsymbol{\\theta} \\in \\mathbb{R}^{d+1} est le vecteur de paramètres contenant le biais \\theta_0 et les poids \\theta_1, \\ldots, \\theta_d.\n\nCette forme est restrictive: elle suppose que la relation entre entrées et sorties est linéaire. Pour les données de freinage, cela signifierait que la distance est proportionnelle à la vitesse, ce qui n’est pas le cas (la relation est plutôt quadratique). Néanmoins, les modèles linéaires sont utiles comme point de départ, et nous verrons comment les étendre pour capturer des relations non linéaires.\n\nAvec cette classe \\mathcal{F} fixée, l’apprentissage consiste à trouver les paramètres \\boldsymbol{\\theta} qui minimisent le risque empirique. Pour la perte quadratique, cela revient à minimiser la somme des carrés des résidus.\n\nMais quand le minimiseur du risque empirique a-t-il un faible risque? Si \\hat{f} minimise \\hat{\\mathcal{R}} et f^\\star minimise \\mathcal{R}, nous voulons que \\mathcal{R}(\\hat{f}) soit proche de \\mathcal{R}(f^\\star). La réponse dépend de la taille de l’échantillon N, de la complexité de la classe \\mathcal{F}, et de propriétés de la distribution p.","type":"content","url":"/learning-problem#un-premier-exemple-les-mod-les-lin-aires","position":31},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Résoudre le problème d’optimisation","lvl2":"Minimisation du risque empirique"},"type":"lvl3","url":"/learning-problem#r-soudre-le-probl-me-doptimisation","position":32},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Résoudre le problème d’optimisation","lvl2":"Minimisation du risque empirique"},"content":"Quand nous écrivons \\arg\\min_{\\boldsymbol{\\theta}}, nous cherchons les paramètres qui rendent la fonction objectif aussi petite que possible. Mais comment trouver ces paramètres en pratique?\n\nLa réponse dépend de la forme du problème:\n\nPour certains problèmes, comme la régression linéaire avec perte quadratique, nous pouvons dériver une solution analytique en posant le gradient égal à zéro et en résolvant le système d’équations résultant.\n\nPour d’autres, nous devons recourir à des algorithmes itératifs (comme la descente de gradient) ou à des solveurs spécialisés (comme la programmation quadratique pour les SVM).","type":"content","url":"/learning-problem#r-soudre-le-probl-me-doptimisation","position":33},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Exemple: solution analytique pour la régression linéaire (MCO)","lvl3":"Résoudre le problème d’optimisation","lvl2":"Minimisation du risque empirique"},"type":"lvl4","url":"/learning-problem#exemple-solution-analytique-pour-la-r-gression-lin-aire-mco","position":34},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Exemple: solution analytique pour la régression linéaire (MCO)","lvl3":"Résoudre le problème d’optimisation","lvl2":"Minimisation du risque empirique"},"content":"Pour illustrer les solutions analytiques, considérons la régression linéaire avec perte quadratique. L’objectif est de minimiser la somme des carrés des résidus:\\text{RSS}(\\boldsymbol{\\theta}) = \\sum_{i=1}^N (y_i - \\boldsymbol{\\theta}^\\top \\mathbf{x}_i)^2 = \\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}\\|_2^2\n\noù \\mathbf{X} est la matrice N \\times (d+1) des entrées (avec une colonne de 1 pour le biais) et \\mathbf{y} est le vecteur des sorties.\n\nEn développant et en calculant le gradient:\\nabla_{\\boldsymbol{\\theta}} \\text{RSS}(\\boldsymbol{\\theta}) = -2\\mathbf{X}^\\top \\mathbf{y} + 2\\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\theta}\n\nEn posant le gradient égal à zéro, nous obtenons les équations normales:\\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\theta} = \\mathbf{X}^\\top \\mathbf{y}\n\nSi la matrice \\mathbf{X}^\\top \\mathbf{X} est inversible, la solution unique est:\\hat{\\boldsymbol{\\theta}}_{\\text{MCO}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n\nCette solution porte le nom d’estimateur des moindres carrés ordinaires (MCO, ou ordinary least squares, OLS). Elle peut être calculée directement sans itération, ce qui en fait un exemple classique de solution analytique.\n\nLe chapitre sur l’optimisation présentera ces méthodes en détail. Pour l’instant, nous nous concentrons sur la formulation des problèmes d’apprentissage comme problèmes d’optimisation, en gardant à l’esprit que des outils existent pour les résoudre.","type":"content","url":"/learning-problem#exemple-solution-analytique-pour-la-r-gression-lin-aire-mco","position":35},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Généralisation","lvl2":"Minimisation du risque empirique"},"type":"lvl3","url":"/learning-problem#g-n-ralisation","position":36},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Généralisation","lvl2":"Minimisation du risque empirique"},"content":"La différence entre le risque et le risque empirique est l’écart de généralisation:\\text{Écart} = \\mathcal{R}(f) - \\hat{\\mathcal{R}}(f; \\mathcal{D}_{\\text{train}})\n\nUn modèle qui minimise le risque empirique peut avoir un risque élevé si cet écart est grand. Ce phénomène est le surapprentissage: le modèle s’ajuste aux particularités de l’échantillon d’entraînement, y compris le bruit, plutôt qu’aux régularités sous-jacentes. L’erreur d’entraînement est faible, mais l’erreur sur de nouvelles données est élevée.\n\nÀ l’inverse, un modèle trop simple peut avoir un risque empirique et un risque tous deux élevés. C’est le sous-apprentissage: le modèle n’a pas la capacité de capturer la structure des données.","type":"content","url":"/learning-problem#g-n-ralisation","position":37},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Extrapolation","lvl3":"Généralisation","lvl2":"Minimisation du risque empirique"},"type":"lvl4","url":"/learning-problem#extrapolation","position":38},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Extrapolation","lvl3":"Généralisation","lvl2":"Minimisation du risque empirique"},"content":"Un cas particulier de mauvaise généralisation est l’extrapolation: prédire pour des entrées en dehors de la plage des données d’entraînement. Même un modèle bien ajusté peut échouer spectaculairement lorsqu’on lui demande de prédire au-delà de ce qu’il a vu.\n\nConsidérons des essais en soufflerie pour mesurer la portance d’une aile à différentes vitesses. Les tests sont effectués entre 20 et 60 m/s. L’ingénieur veut prédire la portance à 100 m/s.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Données de portance aérodynamique (simulées)\nnp.random.seed(42)\nrho, S, C_L = 1.225, 20.0, 0.5\nv_train = np.linspace(20, 60, 8)\nL_true_train = 0.5 * rho * v_train**2 * S * C_L\nL_train = L_true_train + np.random.normal(0, 400, len(v_train))\n\ncoeffs_2 = np.polyfit(v_train, L_train, 2)\ncoeffs_5 = np.polyfit(v_train, L_train, 5)\n\nv_extrap = np.linspace(15, 110, 200)\nL_true_extrap = 0.5 * rho * v_extrap**2 * S * C_L\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\nfor ax, coeffs, deg in zip(axes, [coeffs_2, coeffs_5], [2, 5]):\n    ax.scatter(v_train, L_train, s=50, zorder=5, label='Observations')\n    ax.plot(v_extrap, L_true_extrap, 'g-', alpha=0.5, label='Vraie relation')\n    L_pred = np.polyval(coeffs, v_extrap)\n    ax.plot(v_extrap, L_pred, 'k--', label=f'Polynôme degré {deg}')\n    ax.axvline(60, color='gray', linestyle=':', alpha=0.5)\n    ax.axvspan(60, 110, alpha=0.1, color='red')\n    ax.set_xlabel('Vitesse (m/s)')\n    ax.set_ylabel('Portance (N)')\n    ax.set_title(f'Degré {deg}')\n    ax.legend(loc='upper left')\n    ax.set_ylim(-5000, 80000)\n    ax.text(85, 5000, 'Extrapolation', ha='center', fontsize=10, color='red', alpha=0.7)\n\nplt.tight_layout()\n\n\n\nLe polynôme de degré 2 (qui correspond au vrai modèle physique L \\propto v^2) extrapole correctement. Le polynôme de degré 5, bien qu’il ajuste aussi bien les données d’entraînement, diverge complètement en dehors de la plage observée.","type":"content","url":"/learning-problem#extrapolation","position":39},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Régularisation","lvl2":"Minimisation du risque empirique"},"type":"lvl3","url":"/learning-problem#r-gularisation","position":40},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Régularisation","lvl2":"Minimisation du risque empirique"},"content":"Une manière de contrôler le surapprentissage consiste à pénaliser la complexité du modèle directement dans la fonction objectif. Le risque empirique régularisé est:\\hat{\\mathcal{R}}_\\lambda(\\boldsymbol{\\theta}) = \\hat{\\mathcal{R}}(\\boldsymbol{\\theta}) + \\lambda \\, C(\\boldsymbol{\\theta})\n\noù C(\\boldsymbol{\\theta}) mesure la complexité du modèle et \\lambda \\geq 0 contrôle l’intensité de la pénalisation. Un choix courant est la régularisation \\ell_2 (ou weight decay):C(\\boldsymbol{\\theta}) = \\|\\boldsymbol{\\theta}\\|_2^2 = \\sum_j \\theta_j^2\n\nCette pénalisation pousse les paramètres vers zéro, ce qui a pour effet de lisser la fonction apprise. En régression linéaire, l’ajout de cette pénalité donne la régression ridge:\\hat{\\boldsymbol{\\theta}}_{\\text{ridge}} = \\arg\\min_{\\boldsymbol{\\theta}} \\frac{1}{N}\\sum_{i=1}^N (y_i - \\boldsymbol{\\theta}^\\top \\mathbf{x}_i)^2 + \\lambda \\|\\boldsymbol{\\theta}\\|_2^2\n\nIllustrons l’effet de la régularisation sur le même problème de régression polynomiale. Avec un polynôme de degré 15 et différentes valeurs de \\lambda:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Données de freinage\nspeed = np.array([4, 4, 7, 7, 8, 9, 10, 10, 10, 11, 11, 12, 12, 12, 12, 13, 13, 13, 13, 14,\n                  14, 14, 14, 15, 15, 15, 16, 16, 17, 17, 17, 18, 18, 18, 18, 19, 19, 19,\n                  20, 20, 20, 20, 20, 22, 23, 24, 24, 24, 24, 25], dtype=float)\ndist = np.array([2, 10, 4, 22, 16, 10, 18, 26, 34, 17, 28, 14, 20, 24, 28, 26, 34, 34, 46,\n                 26, 36, 60, 80, 20, 26, 54, 32, 40, 32, 40, 50, 42, 56, 76, 84, 36, 46,\n                 68, 32, 48, 52, 56, 64, 66, 54, 70, 92, 93, 120, 85], dtype=float)\n\n# Train/test split\nnp.random.seed(42)\nindices = np.random.permutation(len(speed))\ntrain_idx, test_idx = indices[:35], indices[35:]\nspeed_train, dist_train = speed[train_idx], dist[train_idx]\nspeed_test, dist_test = speed[test_idx], dist[test_idx]\n\n# Build polynomial features (degree 15)\ndegree = 15\ndef poly_features(x, deg):\n    return np.vstack([x**i for i in range(deg+1)]).T\n\nX_train = poly_features(speed_train, degree)\nX_test = poly_features(speed_test, degree)\n\n# Ridge regression for different lambda values\nlambdas = [0, 1e-6, 1e-3, 1]\nfig, axes = plt.subplots(2, 2, figsize=(10, 8))\n\nfor ax, lam in zip(axes.flat, lambdas):\n    # Ridge solution: (X^T X + lambda I)^{-1} X^T y\n    I = np.eye(X_train.shape[1])\n    I[0, 0] = 0  # Don't regularize bias\n    w = np.linalg.solve(X_train.T @ X_train + lam * I, X_train.T @ dist_train)\n    \n    # Predictions\n    pred_train = X_train @ w\n    pred_test = X_test @ w\n    mse_train = np.mean((dist_train - pred_train)**2)\n    mse_test = np.mean((dist_test - pred_test)**2)\n    \n    # Plot\n    ax.scatter(speed_train, dist_train, alpha=0.6, s=30, label='Entraînement')\n    ax.scatter(speed_test, dist_test, alpha=0.6, s=30, marker='s', label='Test')\n    \n    speed_grid = np.linspace(3, 26, 200)\n    X_grid = poly_features(speed_grid, degree)\n    pred_grid = X_grid @ w\n    pred_grid = np.clip(pred_grid, -50, 200)\n    ax.plot(speed_grid, pred_grid, 'k-', alpha=0.7)\n    \n    ax.set_xlim(3, 26)\n    ax.set_ylim(-20, 150)\n    ax.set_xlabel('Vitesse (mph)')\n    ax.set_ylabel('Distance (ft)')\n    ax.set_title(f'$\\\\lambda$ = {lam}: Entr. EQM={mse_train:.1f}, Test EQM={mse_test:.1f}')\n    if lam == 0:\n        ax.legend()\n\nplt.tight_layout()\n\n\n\nSans régularisation (\\lambda = 0), le polynôme de degré 15 oscille fortement. Avec une régularisation modérée (\\lambda = 10^{-3}), les oscillations sont atténuées et l’erreur de test diminue. Avec une régularisation trop forte (\\lambda = 1), le modèle devient trop contraint et sous-apprend.","type":"content","url":"/learning-problem#r-gularisation","position":41},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Solution analytique de la régression ridge","lvl3":"Régularisation","lvl2":"Minimisation du risque empirique"},"type":"lvl4","url":"/learning-problem#solution-analytique-de-la-r-gression-ridge","position":42},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Solution analytique de la régression ridge","lvl3":"Régularisation","lvl2":"Minimisation du risque empirique"},"content":"Comme pour les moindres carrés ordinaires, la régression ridge admet une solution analytique. L’objectif régularisé est:\\text{RSS}_\\lambda(\\boldsymbol{\\theta}) = \\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}\\|_2^2 + \\lambda \\|\\boldsymbol{\\theta}\\|_2^2\n\nEn développant et en calculant le gradient:\\nabla_{\\boldsymbol{\\theta}} \\text{RSS}_\\lambda(\\boldsymbol{\\theta}) = -2\\mathbf{X}^\\top \\mathbf{y} + 2\\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\theta} + 2\\lambda \\boldsymbol{\\theta} = -2\\mathbf{X}^\\top \\mathbf{y} + 2(\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I}) \\boldsymbol{\\theta}\n\nEn posant le gradient égal à zéro, nous obtenons les équations normales régularisées:(\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I}) \\boldsymbol{\\theta} = \\mathbf{X}^\\top \\mathbf{y}\n\nLa solution est:\\hat{\\boldsymbol{\\theta}}_{\\text{ridge}} = (\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n\nComparons avec la solution MCO: \\hat{\\boldsymbol{\\theta}}_{\\text{MCO}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}. La seule différence est l’ajout du terme \\lambda \\mathbf{I} à la matrice \\mathbf{X}^\\top \\mathbf{X}.","type":"content","url":"/learning-problem#solution-analytique-de-la-r-gression-ridge","position":43},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl5":"Solution via décomposition en valeurs singulières (SVD)","lvl4":"Solution analytique de la régression ridge","lvl3":"Régularisation","lvl2":"Minimisation du risque empirique"},"type":"lvl5","url":"/learning-problem#solution-via-d-composition-en-valeurs-singuli-res-svd","position":44},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl5":"Solution via décomposition en valeurs singulières (SVD)","lvl4":"Solution analytique de la régression ridge","lvl3":"Régularisation","lvl2":"Minimisation du risque empirique"},"content":"Les solutions MCO et Ridge peuvent également être exprimées en utilisant la décomposition en valeurs singulières (SVD) de la matrice \\mathbf{X}. Cette approche offre une interprétation géométrique et révèle pourquoi la régularisation fonctionne.\n\nLa SVD décompose \\mathbf{X} en trois matrices:\\mathbf{X} = \\mathbf{U} \\mathbf{D} \\mathbf{V}^\\top\n\noù:\n\n\\mathbf{U} est une matrice N \\times d dont les colonnes \\mathbf{u}_j sont orthonormales (directions dans l’espace des observations)\n\n\\mathbf{D} est une matrice diagonale d \\times d contenant les valeurs singulières d_1 \\geq d_2 \\geq \\cdots \\geq d_d \\geq 0\n\n\\mathbf{V} est une matrice d \\times d dont les colonnes \\mathbf{v}_j sont orthonormales (directions principales dans l’espace des coefficients)\n\nSolution MCO via SVD: En substituant cette décomposition dans la solution MCO:\\hat{\\boldsymbol{\\theta}}_{\\text{MCO}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y} = (\\mathbf{V} \\mathbf{D}^2 \\mathbf{V}^\\top)^{-1} \\mathbf{V} \\mathbf{D} \\mathbf{U}^\\top \\mathbf{y} = \\mathbf{V} \\mathbf{D}^{-1} \\mathbf{U}^\\top \\mathbf{y}\n\nCe qui s’écrit sous forme de somme:\\hat{\\boldsymbol{\\theta}}_{\\text{MCO}} = \\sum_{j=1}^d \\frac{\\mathbf{u}_j^\\top \\mathbf{y}}{d_j} \\mathbf{v}_j\n\nSolution Ridge via SVD: Pour Ridge, on peut montrer que:\\hat{\\boldsymbol{\\theta}}_{\\text{ridge}} = \\sum_{j=1}^d \\frac{d_j^2}{d_j^2 + \\lambda} \\frac{\\mathbf{u}_j^\\top \\mathbf{y}}{d_j} \\mathbf{v}_j\n\nLa différence avec MCO est le facteur de rétrécissement \\frac{d_j^2}{d_j^2 + \\lambda} qui multiplie chaque terme. Ce facteur est toujours inférieur à 1, ce qui “rétrécit” chaque composante vers zéro.\n\nInterprétation géométrique: Les directions principales \\mathbf{v}_j définissent les axes d’une ellipse de confiance dans l’espace des coefficients. Les longueurs des demi-axes sont proportionnelles à 1/d_j pour MCO. Avec Ridge, elles deviennent proportionnelles à \\frac{d_j}{d_j^2 + \\lambda} = \\frac{1}{d_j} \\cdot \\frac{d_j^2}{d_j^2 + \\lambda}: l’ellipse se rétrécit, et plus rapidement le long des directions associées aux petites valeurs singulières.\n\nAvantages numériques: La SVD est plus stable numériquement que l’inversion directe de \\mathbf{X}^\\top \\mathbf{X}, surtout quand cette matrice est mal conditionnée. Les algorithmes SVD gèrent mieux les cas où certaines valeurs singulières sont très petites.","type":"content","url":"/learning-problem#solution-via-d-composition-en-valeurs-singuli-res-svd","position":45},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Pourquoi \\lambda \\mathbf{I} aide","lvl3":"Régularisation","lvl2":"Minimisation du risque empirique"},"type":"lvl4","url":"/learning-problem#pourquoi-lambda-mathbf-i-aide","position":46},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Pourquoi \\lambda \\mathbf{I} aide","lvl3":"Régularisation","lvl2":"Minimisation du risque empirique"},"content":"Ce terme diagonal a plusieurs effets bénéfiques:\n\nAmélioration du conditionnement: La matrice \\mathbf{X}^\\top \\mathbf{X} peut être mal conditionnée (ses valeurs propres varient sur plusieurs ordres de grandeur) ou même singulière. L’ajout de \\lambda \\mathbf{I} augmente toutes les valeurs propres de \\lambda, rendant la matrice inversible et mieux conditionnée.\n\nRétrécissement des coefficients (shrinkage): Comme nous l’avons vu dans la section SVD ci-dessus, la solution Ridge s’écrit:\\hat{\\boldsymbol{\\theta}}_{\\text{ridge}} = \\sum_{j=1}^d \\frac{d_j^2}{d_j^2 + \\lambda} \\frac{\\mathbf{u}_j^\\top \\mathbf{y}}{d_j} \\mathbf{v}_j\n\nLe facteur de rétrécissement \\frac{d_j^2}{d_j^2 + \\lambda} est toujours inférieur à 1, ce qui “rétrécit” chaque composante vers zéro. L’effet est différencié selon les directions:\n\nPour une grande valeur singulière d_j (fort signal), le facteur \\frac{d_j^2}{d_j^2 + \\lambda} reste proche de 1 même pour des valeurs modérées de \\lambda. La direction est peu affectée.\n\nPour une petite valeur singulière d_j (faible signal), le facteur \\frac{d_j^2}{d_j^2 + \\lambda} décroît rapidement avec \\lambda. La direction est fortement pénalisée.\n\nPour visualiser ce rétrécissement et comprendre son effet, examinons un exemple concret. L’animation suivante montre simultanément trois perspectives sur la régularisation Ridge: les données et la droite ajustée, le paysage de perte avec la contrainte, et les facteurs de rétrécissement.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib.patches import Circle\nfrom IPython.display import Image\n\n# Générer des données de régression simple\nnp.random.seed(42)\nn = 30\n\n# Une seule caractéristique pour visualisation claire\nx = np.random.uniform(-2, 2, n)\n# Relation linéaire avec bruit\ntheta_true = 1.5\ny = theta_true * x + np.random.normal(0, 0.8, n)\n\n# Ajouter une caractéristique corrélée (pour créer de la colinéarité)\nx2 = 0.9 * x + 0.3 * np.random.randn(n)\n\n# Matrice de design avec les deux caractéristiques\nX = np.column_stack([x, x2])\n\n# Solution MCO\ntheta_ols = np.linalg.lstsq(X, y, rcond=None)[0]\n\n# SVD pour analyse\nU, d_svd, Vt = np.linalg.svd(X, full_matrices=False)\nV = Vt.T\n\n# Fonction pour calculer la solution Ridge\ndef ridge_solution(X, y, lam):\n    n_features = X.shape[1]\n    return np.linalg.solve(X.T @ X + lam * np.eye(n_features), X.T @ y)\n\n# Préparer la grille pour les contours RSS\ntheta1_range = np.linspace(-0.5, 3, 100)\ntheta2_range = np.linspace(-1.5, 2, 100)\nT1, T2 = np.meshgrid(theta1_range, theta2_range)\n\n# Calculer RSS pour chaque point de la grille\nRSS = np.zeros_like(T1)\nfor i in range(T1.shape[0]):\n    for j in range(T1.shape[1]):\n        theta = np.array([T1[i, j], T2[i, j]])\n        residuals = y - X @ theta\n        RSS[i, j] = np.sum(residuals**2)\n\n# Créer la figure avec trois panneaux\nfig = plt.figure(figsize=(15, 5))\n\n# === Panneau 1: Données et droite ajustée ===\nax1 = fig.add_subplot(1, 3, 1)\n\n# Données\nax1.scatter(x, y, c='tab:blue', s=50, alpha=0.7, label='Données', zorder=3)\n\n# Grille pour tracer les droites\nx_grid = np.linspace(-2.5, 2.5, 100)\n\n# Droite MCO (fixe) - on utilise seulement theta1 car x et x2 sont très corrélés\n# La prédiction effective est environ (theta1 + 0.9*theta2) * x\nslope_ols = theta_ols[0] + 0.9 * theta_ols[1]  # Pente effective\ny_ols = slope_ols * x_grid\nax1.plot(x_grid, y_ols, 'k-', linewidth=2, alpha=0.7, label='MCO')\n\n# Droite Ridge (animée)\nline_ridge, = ax1.plot([], [], '-', color='tab:orange', linewidth=2.5, label='Ridge')\n\n# Ligne horizontale (prédiction = moyenne, lambda infini)\ny_mean = np.mean(y)\nax1.axhline(y_mean, color='gray', linestyle=':', alpha=0.5, label=f'Moyenne ($\\\\lambda \\\\to \\\\infty$)')\n\nax1.set_xlabel('$x$')\nax1.set_ylabel('$y$')\nax1.set_title('Données et droite de régression')\nax1.legend(loc='upper left', fontsize=9)\nax1.grid(True, alpha=0.3)\nax1.set_xlim(-2.5, 2.5)\nax1.set_ylim(-4, 5)\n\n# Texte pour les coefficients\ncoef_text = ax1.text(0.98, 0.02, '', transform=ax1.transAxes, fontsize=10, \n                     ha='right', va='bottom',\n                     bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\n# === Panneau 2: Paysage de perte ===\nax2 = fig.add_subplot(1, 3, 2)\n\n# Contours RSS (ellipses centrées sur OLS)\nlevels = np.percentile(RSS.flatten(), [5, 15, 30, 50, 70, 85, 95])\ncontours = ax2.contour(T1, T2, RSS, levels=levels, colors='gray', alpha=0.6)\nax2.clabel(contours, inline=True, fontsize=8, fmt='%.0f')\n\n# Solution MCO (fixe)\nax2.plot(theta_ols[0], theta_ols[1], 'ko', markersize=12, label='MCO', zorder=5)\nax2.annotate('MCO', xy=(theta_ols[0], theta_ols[1]), \n             xytext=(theta_ols[0] + 0.2, theta_ols[1] + 0.2),\n             fontsize=11, ha='left')\n\n# Origine = coefficients nuls (prédiction constante)\nax2.plot(0, 0, 'k+', markersize=15, markeredgewidth=2, zorder=4)\nax2.annotate('$\\\\boldsymbol{\\\\theta} = 0$\\n(pente nulle)', xy=(0, 0), \n             xytext=(-0.4, -1.2), fontsize=9, ha='center', color='gray')\n\n# Cercle de contrainte Ridge (animé)\ncircle_ridge = Circle((0, 0), radius=np.linalg.norm(theta_ols), \n                       fill=False, edgecolor='tab:orange', linewidth=2.5, \n                       linestyle='-', alpha=0.8, zorder=3)\nax2.add_patch(circle_ridge)\n\n# Solution Ridge (animée)\npoint_ridge, = ax2.plot([], [], 'o', color='tab:orange', markersize=10, \n                        label='Ridge', zorder=6)\n\n# Chemin de régularisation\nlambda_path = np.logspace(-3, 1.5, 50)\ntheta_path = np.array([ridge_solution(X, y, l) for l in lambda_path])\nax2.plot(theta_path[:, 0], theta_path[:, 1], 'tab:orange', linewidth=1.5, \n         alpha=0.4, linestyle='--', label='Chemin')\n\nax2.set_xlabel('$\\\\theta_1$')\nax2.set_ylabel('$\\\\theta_2$')\nax2.set_title('Paysage RSS et contrainte $\\\\|\\\\boldsymbol{\\\\theta}\\\\|^2$')\nax2.legend(loc='upper right', fontsize=9)\nax2.grid(True, alpha=0.3)\nax2.set_xlim(-0.5, 3)\nax2.set_ylim(-1.5, 2)\nax2.set_aspect('equal')\n\n# Texte pour lambda\nlambda_text = ax2.text(0.02, 0.98, '', transform=ax2.transAxes, fontsize=11, \n                       va='top', ha='left',\n                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.9))\n\n# === Panneau 3: Facteurs de rétrécissement ===\nax3 = fig.add_subplot(1, 3, 3)\n\nlambda_range = np.linspace(0, 10, 200)\nshrink1_curve = d_svd[0]**2 / (d_svd[0]**2 + lambda_range)\nshrink2_curve = d_svd[1]**2 / (d_svd[1]**2 + lambda_range)\n\nax3.plot(lambda_range, shrink1_curve, 'b-', linewidth=2, \n         label=f'Direction forte ($d_1={d_svd[0]:.1f}$)')\nax3.plot(lambda_range, shrink2_curve, 'r-', linewidth=2, \n         label=f'Direction faible ($d_2={d_svd[1]:.2f}$)')\n\n# Zone de surapprentissage et sous-apprentissage\nax3.axvspan(0, 0.5, alpha=0.1, color='red', label='Surapprentissage')\nax3.axvspan(5, 10, alpha=0.1, color='blue', label='Sous-apprentissage')\n\npoint_shrink1, = ax3.plot([], [], 'bo', markersize=10, zorder=3)\npoint_shrink2, = ax3.plot([], [], 'ro', markersize=10, zorder=3)\n\nax3.axhline(1.0, color='gray', linestyle='--', alpha=0.5)\nax3.set_xlabel('$\\\\lambda$')\nax3.set_ylabel('Facteur de rétrécissement')\nax3.set_title('Rétrécissement par direction SVD')\nax3.legend(loc='center right', fontsize=8)\nax3.grid(True, alpha=0.3)\nax3.set_xlim(0, 10)\nax3.set_ylim(0, 1.1)\n\nshrink_text = ax3.text(0.02, 0.5, '', transform=ax3.transAxes, fontsize=10, \n                       va='center', ha='left',\n                       bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nplt.tight_layout()\n\n# Fonction d'animation\ndef animate(frame):\n    if frame < 80:\n        lam = (frame / 80) * 10\n    else:\n        lam = 10\n    \n    # Solution Ridge\n    theta_ridge = ridge_solution(X, y, lam)\n    \n    # Panneau 1: Mettre à jour la droite\n    slope_ridge = theta_ridge[0] + 0.9 * theta_ridge[1]\n    y_ridge = slope_ridge * x_grid\n    line_ridge.set_data(x_grid, y_ridge)\n    coef_text.set_text(f'Pente MCO: {slope_ols:.2f}\\nPente Ridge: {slope_ridge:.2f}')\n    \n    # Panneau 2: Mettre à jour le cercle et le point\n    norm_ridge = np.linalg.norm(theta_ridge)\n    circle_ridge.set_radius(norm_ridge)\n    point_ridge.set_data([theta_ridge[0]], [theta_ridge[1]])\n    lambda_text.set_text(f'$\\\\lambda = {lam:.1f}$')\n    \n    # Panneau 3: Mettre à jour les points de rétrécissement\n    shrink1 = d_svd[0]**2 / (d_svd[0]**2 + lam)\n    shrink2 = d_svd[1]**2 / (d_svd[1]**2 + lam)\n    point_shrink1.set_data([lam], [shrink1])\n    point_shrink2.set_data([lam], [shrink2])\n    shrink_text.set_text(f'Facteur dir. 1: {shrink1:.2f}\\nFacteur dir. 2: {shrink2:.2f}')\n    \n    return (line_ridge, point_ridge, circle_ridge, lambda_text, \n            point_shrink1, point_shrink2, coef_text, shrink_text)\n\n# Créer l'animation\nanim = FuncAnimation(fig, animate, frames=90, interval=80, blit=False, repeat=True)\nanim.save('_static/ridge_geometry.gif', writer='pillow', fps=12, dpi=100)\nplt.close()\n\n# Afficher le GIF\nImage(filename='_static/ridge_geometry.gif')\n\n\n\nL’animation relie trois perspectives sur la régularisation Ridge lorsque \\lambda augmente de 0 à 10:\n\nPanneau de gauche — Données et ajustement: Les points bleus sont les données d’entraînement. La droite noire est l’ajustement MCO (\\lambda = 0), la droite orange est l’ajustement Ridge. À mesure que \\lambda augmente, la pente de la droite Ridge diminue, se rapprochant de la ligne horizontale (prédiction constante égale à la moyenne). C’est le rétrécissement vers zéro: Ridge “tire” les coefficients vers l’origine, ce qui réduit la pente.\n\nPanneau central — Paysage de perte: Chaque point de ce plan représente un choix de coefficients (\\theta_1, \\theta_2). Les contours gris montrent la fonction de coût RSS: plus on est proche du point noir (MCO), plus l’erreur sur les données d’entraînement est faible. L’ellipse est allongée car x_1 et x_2 sont corrélées (colinéarité). L’origine \\boldsymbol{\\theta} = (0, 0) correspond à une pente nulle (prédiction constante). Le cercle orange représente la contrainte Ridge \\|\\boldsymbol{\\theta}\\|_2 \\leq c: plus \\lambda est grand, plus le cercle est petit, forçant la solution à se rapprocher de l’origine. La solution Ridge (point orange) se déplace le long du chemin de régularisation, compromis entre minimiser la RSS et rester proche de zéro.\n\nPanneau de droite — Rétrécissement différencié: La direction “forte” (grande valeur singulière d_1, où les données sont dispersées) est peu affectée par la régularisation. La direction “faible” (petite valeur singulière d_2, direction de colinéarité) est rétrécit beaucoup plus rapidement. C’est le cœur de l’effet Ridge: pénaliser davantage les directions où le signal est faible et l’estimation instable.\n\nL’intuition géométrique est la suivante: quand les données sont colinéaires, l’ellipse RSS est très allongée. De petites perturbations dans les données causent de grands déplacements de la solution MCO le long de l’axe allongé. La contrainte Ridge “coupe” cette ellipse avec un cercle, forçant une solution plus proche de l’origine et donc plus stable.\n\nStabilité numérique: Quand \\mathbf{X}^\\top \\mathbf{X} est presque singulière, de petites perturbations dans les données causent de grandes variations dans \\hat{\\boldsymbol{\\theta}}_{\\text{MCO}}. La régularisation réduit cette sensibilité.","type":"content","url":"/learning-problem#pourquoi-lambda-mathbf-i-aide","position":47},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Classes de modèles et expansion de caractéristiques"},"type":"lvl2","url":"/learning-problem#classes-de-mod-les-et-expansion-de-caract-ristiques","position":48},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Classes de modèles et expansion de caractéristiques"},"content":"Les exemples de régularisation ci-dessus utilisaient des caractéristiques polynomiales: au lieu de prédire y directement à partir de x, nous avons construit des caractéristiques [1, x, x^2, \\ldots, x^{15}] et appliqué un modèle linéaire dans cet espace étendu. Cette technique s’appelle l’expansion de caractéristiques et mérite d’être formalisée.","type":"content","url":"/learning-problem#classes-de-mod-les-et-expansion-de-caract-ristiques","position":49},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Trois familles de modèles","lvl2":"Classes de modèles et expansion de caractéristiques"},"type":"lvl3","url":"/learning-problem#trois-familles-de-mod-les","position":50},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Trois familles de modèles","lvl2":"Classes de modèles et expansion de caractéristiques"},"content":"Situons les modèles linéaires dans une hiérarchie plus large. Nous distinguons trois familles de complexité croissante:\n\nModèles linéaires: f(\\mathbf{x}; \\boldsymbol{\\theta}) = \\boldsymbol{\\theta}^\\top \\mathbf{x} + b. La sortie est une combinaison linéaire des entrées. Simple, interprétable, mais limité aux relations linéaires.\n\nModèles à expansion de caractéristiques: f(\\mathbf{x}; \\boldsymbol{\\theta}) = \\boldsymbol{\\theta}^\\top \\boldsymbol{\\phi}(\\mathbf{x}) + b, où \\boldsymbol{\\phi}: \\mathbb{R}^d \\to \\mathbb{R}^D est une transformation non linéaire fixée à l’avance (par exemple, polynomiale). Le modèle reste linéaire dans les paramètres \\boldsymbol{\\theta}, ce qui facilite l’optimisation, mais peut capturer des relations non linéaires en \\mathbf{x}. L’espace de redescription a souvent une dimension D \\gg d.\n\nRéseaux de neurones: f(\\mathbf{x}; \\boldsymbol{\\theta}) = f_K(f_{K-1}(\\cdots f_1(\\mathbf{x}; \\boldsymbol{\\theta}_1); \\boldsymbol{\\theta}_{K-1}); \\boldsymbol{\\theta}_K). Une composition de K fonctions non linéaires, chacune avec ses propres paramètres. Contrairement aux modèles à expansion fixe, les réseaux de neurones apprennent la représentation \\boldsymbol{\\phi} en même temps que les paramètres \\boldsymbol{\\theta}.\n\nCette progression capture l’évolution historique du domaine: des modèles linéaires classiques aux méthodes à noyaux (expansion implicite), puis aux réseaux profonds qui apprennent leurs propres représentations. Nous verrons les réseaux de neurones en détail dans les chapitres suivants; concentrons-nous ici sur les deux premières familles.","type":"content","url":"/learning-problem#trois-familles-de-mod-les","position":51},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Expansion de caractéristiques","lvl2":"Classes de modèles et expansion de caractéristiques"},"type":"lvl3","url":"/learning-problem#expansion-de-caract-ristiques","position":52},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Expansion de caractéristiques","lvl2":"Classes de modèles et expansion de caractéristiques"},"content":"Pour capturer des relations non linéaires tout en gardant un modèle linéaire dans les paramètres, nous transformons les entrées. En régression polynomiale, nous appliquons une fonction \\phi: \\mathbb{R} \\to \\mathbb{R}^{k+1}:\\phi(x) = [1, x, x^2, \\ldots, x^k]\n\nLa prédiction devient f(x; \\boldsymbol{\\theta}) = \\boldsymbol{\\theta}^\\top \\phi(x). Le modèle est polynomial en x mais linéaire en \\boldsymbol{\\theta}, ce qui permet d’utiliser les mêmes algorithmes d’optimisation (MCO, Ridge).\n\nLe degré k contrôle la capacité du modèle. Avec k = 1, nous avons une droite. Avec k élevé, le polynôme peut osciller pour passer par tous les points d’entraînement. Avec k = N - 1, nous pouvons interpoler exactement les N points: le risque empirique atteint zéro. Mais un polynôme qui passe exactement par les points d’entraînement n’a aucune raison de bien prédire les nouveaux points.\n\nIllustrons ce phénomène avec les données de freinage. Nous ajustons des polynômes de degrés 1, 2, 5 et 15, et comparons leurs erreurs sur les ensembles d’entraînement et de test.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\n\n# Suppress polyfit warnings for high-degree polynomials (expected for this demo)\nwarnings.filterwarnings('ignore', message='Polyfit may be poorly conditioned')\n\n# Données de freinage\nspeed = np.array([4, 4, 7, 7, 8, 9, 10, 10, 10, 11, 11, 12, 12, 12, 12, 13, 13, 13, 13, 14,\n                  14, 14, 14, 15, 15, 15, 16, 16, 17, 17, 17, 18, 18, 18, 18, 19, 19, 19,\n                  20, 20, 20, 20, 20, 22, 23, 24, 24, 24, 24, 25], dtype=float)\ndist = np.array([2, 10, 4, 22, 16, 10, 18, 26, 34, 17, 28, 14, 20, 24, 28, 26, 34, 34, 46,\n                 26, 36, 60, 80, 20, 26, 54, 32, 40, 32, 40, 50, 42, 56, 76, 84, 36, 46,\n                 68, 32, 48, 52, 56, 64, 66, 54, 70, 92, 93, 120, 85], dtype=float)\n\n# Train/test split\nnp.random.seed(42)\nindices = np.random.permutation(len(speed))\ntrain_idx, test_idx = indices[:35], indices[35:]\nspeed_train, dist_train = speed[train_idx], dist[train_idx]\nspeed_test, dist_test = speed[test_idx], dist[test_idx]\n\ndegrees_to_plot = [1, 2, 5, 15]\ndegrees_eval = range(1, 16)\nfig, axes = plt.subplots(2, 2, figsize=(10, 8))\n\n# Pre-compute all errors for the summary plot later\nall_train_errors = []\nall_test_errors = []\nfor deg in degrees_eval:\n    coeffs = np.polyfit(speed_train, dist_train, deg)\n    all_train_errors.append(np.mean((dist_train - np.polyval(coeffs, speed_train))**2))\n    all_test_errors.append(np.mean((dist_test - np.polyval(coeffs, speed_test))**2))\n\nfor ax, deg in zip(axes.flat, degrees_to_plot):\n    # Fit polynomial\n    coeffs = np.polyfit(speed_train, dist_train, deg)\n    \n    # Predictions\n    pred_train = np.polyval(coeffs, speed_train)\n    pred_test = np.polyval(coeffs, speed_test)\n    \n    # MSE\n    mse_train = np.mean((dist_train - pred_train)**2)\n    mse_test = np.mean((dist_test - pred_test)**2)\n    \n    # Plot\n    ax.scatter(speed_train, dist_train, alpha=0.6, s=30, label='Entraînement')\n    ax.scatter(speed_test, dist_test, alpha=0.6, s=30, marker='s', label='Test')\n    \n    speed_grid = np.linspace(3, 26, 200)\n    pred_grid = np.polyval(coeffs, speed_grid)\n    # Clip extreme predictions for visualization\n    pred_grid = np.clip(pred_grid, -50, 200)\n    ax.plot(speed_grid, pred_grid, 'k-', alpha=0.7)\n    \n    ax.set_xlim(3, 26)\n    ax.set_ylim(-20, 150)\n    ax.set_xlabel('Vitesse (mph)')\n    ax.set_ylabel('Distance (ft)')\n    ax.set_title(f'Degré {deg}: Entr. EQM={mse_train:.1f}, Test EQM={mse_test:.1f}')\n    if deg == 1:\n        ax.legend()\n\nplt.tight_layout()\n\n\n\nLe polynôme de degré 1 (droite) ne capture pas la courbure des données: c’est du sous-apprentissage. Le polynôme de degré 2 capture bien la relation quadratique. Le polynôme de degré 5 commence à osciller. Le polynôme de degré 15 passe près de tous les points d’entraînement, mais ses oscillations produisent des prédictions absurdes entre les points: c’est du surapprentissage.\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax.plot(degrees_eval, all_train_errors, 'o-', linewidth=2, label='Erreur entraînement')\nax.plot(degrees_eval, all_test_errors, 's-', linewidth=2, label='Erreur test')\n\n# Utiliser une échelle logarithmique car l'erreur de test explose\nax.set_yscale('log')\n\nax.set_xlabel('Degré du polynôme (complexité)')\nax.set_ylabel('EQM (échelle log)')\nax.set_xticks(range(1, 16, 2))\nax.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\nax.legend()\n\nax.set_title('Compromis biais-variance')\nplt.tight_layout()\n\n\n\nL’erreur d’entraînement diminue avec le degré du polynôme. L’erreur de test diminue d’abord (quand le modèle gagne en expressivité), puis augmente (quand le modèle commence à mémoriser le bruit). Le meilleur modèle se trouve à l’intersection de ces deux tendances. La régularisation Ridge, vue précédemment, est une alternative au choix du degré: elle permet d’utiliser un modèle de haute capacité tout en contrôlant le surapprentissage.","type":"content","url":"/learning-problem#expansion-de-caract-ristiques","position":53},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Intuition géométrique: pourquoi la dimension supérieure aide","lvl2":"Classes de modèles et expansion de caractéristiques"},"type":"lvl3","url":"/learning-problem#intuition-g-om-trique-pourquoi-la-dimension-sup-rieure-aide","position":54},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Intuition géométrique: pourquoi la dimension supérieure aide","lvl2":"Classes de modèles et expansion de caractéristiques"},"content":"L’expansion de caractéristiques semble être un simple changement de variables, mais elle cache une idée géométrique profonde. Pour comprendre pourquoi projeter les données dans un espace de dimension supérieure permet de capturer des relations non linéaires, examinons d’abord le cas de la régression, puis celui de la classification.","type":"content","url":"/learning-problem#intuition-g-om-trique-pourquoi-la-dimension-sup-rieure-aide","position":55},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Le plan caché derrière la parabole","lvl3":"Intuition géométrique: pourquoi la dimension supérieure aide","lvl2":"Classes de modèles et expansion de caractéristiques"},"type":"lvl4","url":"/learning-problem#le-plan-cach-derri-re-la-parabole","position":56},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Le plan caché derrière la parabole","lvl3":"Intuition géométrique: pourquoi la dimension supérieure aide","lvl2":"Classes de modèles et expansion de caractéristiques"},"content":"Considérons une régression quadratique: f(x) = \\theta_0 + \\theta_1 x + \\theta_2 x^2. Cette fonction est non linéaire en x (c’est une parabole), mais linéaire dans les paramètres \\boldsymbol{\\theta} = (\\theta_0, \\theta_1, \\theta_2). Que signifie cette distinction géométriquement?\n\nIntroduisons l’espace des caractéristiques \\phi(x) = (1, x, x^2). Chaque valeur de x correspond à un point dans \\mathbb{R}^3. Ces points ne sont pas dispersés arbitrairement: ils vivent sur une courbe particulière, la courbe des moments (moment curve), qui ressemble à une rampe tordue.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Créer la courbe des moments: (x, x², x³) pour visualisation\n# On utilise (1, x, x²) mais on projette sur (x, x², y) pour la visualisation\nfig = plt.figure(figsize=(12, 5))\n\n# Gauche: les fonctions de base\nax1 = fig.add_subplot(121)\nx = np.linspace(-2, 2, 100)\nax1.plot(x, np.ones_like(x), 'b-', linewidth=2, label=r'$\\phi_0(x) = 1$')\nax1.plot(x, x, 'orange', linewidth=2, label=r'$\\phi_1(x) = x$')\nax1.plot(x, x**2, 'g-', linewidth=2, label=r'$\\phi_2(x) = x^2$')\nax1.axhline(0, color='gray', linewidth=0.5)\nax1.axvline(0, color='gray', linewidth=0.5)\nax1.set_xlabel('$x$')\nax1.set_ylabel('$\\phi_j(x)$')\nax1.set_title('Les fonctions de base')\nax1.legend()\nax1.set_ylim(-2.5, 4.5)\nax1.grid(True, alpha=0.3)\n\n# Droite: combinaisons linéaires\nax2 = fig.add_subplot(122)\nx = np.linspace(-2, 2, 100)\n\n# Différentes combinaisons de coefficients\ncombinations = [\n    ((1, 0, 0), 'Constante: $1$'),\n    ((0, 1, 0), 'Linéaire: $x$'),\n    ((0, 0, 1), 'Quadratique: $x^2$'),\n    ((1, -0.5, 0.5), 'Combinaison: $1 - 0.5x + 0.5x^2$'),\n]\n\ncolors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\nfor (theta0, theta1, theta2), label in combinations:\n    y = theta0 + theta1 * x + theta2 * x**2\n    ax2.plot(x, y, linewidth=2, label=label)\n\nax2.axhline(0, color='gray', linewidth=0.5)\nax2.axvline(0, color='gray', linewidth=0.5)\nax2.set_xlabel('$x$')\nax2.set_ylabel('$f(x)$')\nax2.set_title('Combinaisons linéaires des fonctions de base')\nax2.legend(loc='upper center')\nax2.set_ylim(-2.5, 4.5)\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\n\n\n\n\n\nLes fonctions de base \\{1, x, x^2\\} sont les “ingrédients” du modèle. La régression polynomiale cherche les coefficients \\theta_0, \\theta_1, \\theta_2 qui mélangent ces ingrédients de façon optimale. Chaque combinaison produit une courbe différente, mais toutes sont des paraboles (ou des cas dégénérés: droites, constantes).\n\nVoici l’insight géométrique clé: dans l’espace (x, x^2, y), le modèle y = \\theta_0 + \\theta_1 x + \\theta_2 x^2 définit un plan. La parabole que nous voyons dans le graphique (x, y) est simplement la projection de ce plan sur notre espace de visualisation.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import Image\n\n# Générer des données avec relation quadratique\nnp.random.seed(42)\nn_points = 40\nx_data = np.random.uniform(-1.8, 1.8, n_points)\ny_true = 0.5 + 0.3 * x_data + 0.8 * x_data**2\ny_data = y_true + np.random.normal(0, 0.3, n_points)\n\n# Ajuster le modèle quadratique\nX_design = np.column_stack([np.ones(n_points), x_data, x_data**2])\ntheta = np.linalg.lstsq(X_design, y_data, rcond=None)[0]\n\n# Créer l'animation\nfig = plt.figure(figsize=(9, 7))\nax = fig.add_subplot(111, projection='3d')\n\n# Grille pour le plan de régression\nx_grid = np.linspace(-2, 2, 20)\nx2_grid = np.linspace(0, 4, 20)\nX_plane, X2_plane = np.meshgrid(x_grid, x2_grid)\nY_plane = theta[0] + theta[1] * X_plane + theta[2] * X2_plane\n\n# Surface parabolique z = x²\nx_surf = np.linspace(-2, 2, 30)\nX_surf, Y_surf_temp = np.meshgrid(x_surf, np.linspace(-1, 5, 30))\nZ_surf = X_surf**2\n\ndef init():\n    ax.clear()\n    return []\n\ndef animate(frame):\n    ax.clear()\n    \n    # Animation en trois phases\n    if frame < 15:\n        # Phase 1: Vue 2D (de côté, cachant x²)\n        elev = 0\n        azim = 0\n        show_plane = False\n        show_surface = False\n        title = 'Vue 2D: régression quadratique'\n    elif frame < 35:\n        # Phase 2: Rotation révélant la 3ème dimension\n        progress = (frame - 15) / 20\n        elev = progress * 25\n        azim = progress * 45\n        show_plane = False\n        show_surface = True\n        title = 'Rotation: découverte de la dimension $x^2$...'\n    else:\n        # Phase 3: Vue 3D avec le plan\n        elev = 25\n        azim = 45 + (frame - 35) * 2\n        show_plane = True\n        show_surface = True\n        title = r'Le plan $y = \\theta_0 + \\theta_1 x + \\theta_2 x^2$ dans lespace 3D'\n    \n    ax.view_init(elev=elev, azim=azim)\n    \n    # Surface parabolique (rampe z = x²)\n    if show_surface:\n        ax.plot_surface(X_surf, Z_surf, Y_surf_temp, alpha=0.1, color='gray')\n    \n    # Points de données dans l'espace 3D: (x, x², y)\n    ax.scatter(x_data, x_data**2, y_data, c='tab:blue', s=50, alpha=0.8, \n               label='Données', depthshade=True)\n    \n    # Plan de régression\n    if show_plane:\n        ax.plot_surface(X_plane, X2_plane, Y_plane, alpha=0.4, color='tab:orange',\n                       label='Plan de régression')\n    \n    # Courbe de régression sur la surface z = x²\n    x_curve = np.linspace(-1.8, 1.8, 100)\n    y_curve = theta[0] + theta[1] * x_curve + theta[2] * x_curve**2\n    ax.plot(x_curve, x_curve**2, y_curve, 'r-', linewidth=3, \n            label='Courbe ajustée')\n    \n    ax.set_xlabel('$x$')\n    ax.set_ylabel('$x^2$')\n    ax.set_zlabel('$y$')\n    ax.set_xlim(-2, 2)\n    ax.set_ylim(0, 4)\n    ax.set_zlim(-1, 5)\n    ax.set_title(title, fontsize=11)\n    \n    return []\n\nanim = FuncAnimation(fig, animate, init_func=init, frames=70, interval=100, blit=True)\nanim.save('_static/regression_plane_3d.gif', writer='pillow', fps=10, dpi=100)\nplt.close()\n\n# Afficher le GIF\nImage(filename='_static/regression_plane_3d.gif')\n\n\n\nL’animation révèle la structure cachée de la régression polynomiale:\n\nVue 2D initiale: On voit les données et la parabole ajustée, comme dans un graphique classique.\n\nRotation: En faisant pivoter la vue, on découvre que chaque point (x_i, y_i) vit en réalité dans un espace 3D aux coordonnées (x_i, x_i^2, y_i). La surface grise représente la “rampe” z = x^2 sur laquelle tous les points sont contraints de vivre.\n\nLe plan de régression: Le modèle y = \\theta_0 + \\theta_1 x + \\theta_2 x^2 est un plan (en orange) dans cet espace 3D. Ce plan est choisi pour minimiser les distances verticales aux points.\n\nLa courbe ajustée: La parabole rouge est l’intersection du plan avec la rampe z = x^2. C’est ce que nous voyons quand nous projetons le tout sur le plan (x, y).\n\nCette perspective unifie le “linéaire dans les paramètres” et le “non linéaire en x”:\n\nLinéaire dans les paramètres: Le modèle est un plan (objet linéaire) dans l’espace des caractéristiques.\n\nNon linéaire en x: La contrainte z = x^2 force les données à vivre sur une surface courbe, et l’intersection du plan avec cette surface produit une courbe.","type":"content","url":"/learning-problem#le-plan-cach-derri-re-la-parabole","position":57},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"De la régression à la classification","lvl3":"Intuition géométrique: pourquoi la dimension supérieure aide","lvl2":"Classes de modèles et expansion de caractéristiques"},"type":"lvl4","url":"/learning-problem#de-la-r-gression-la-classification","position":58},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"De la régression à la classification","lvl3":"Intuition géométrique: pourquoi la dimension supérieure aide","lvl2":"Classes de modèles et expansion de caractéristiques"},"content":"La même intuition s’applique en classification, avec une différence: au lieu de chercher un plan qui ajuste les données, on cherche un hyperplan qui les sépare. Voyons comment l’expansion de caractéristiques transforme des données non linéairement séparables en données linéairement séparables.","type":"content","url":"/learning-problem#de-la-r-gression-la-classification","position":59},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"De 1D à 2D: séparer l’inséparable","lvl3":"Intuition géométrique: pourquoi la dimension supérieure aide","lvl2":"Classes de modèles et expansion de caractéristiques"},"type":"lvl4","url":"/learning-problem#de-1d-2d-s-parer-lins-parable","position":60},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"De 1D à 2D: séparer l’inséparable","lvl3":"Intuition géométrique: pourquoi la dimension supérieure aide","lvl2":"Classes de modèles et expansion de caractéristiques"},"content":"Considérons des points sur une droite, répartis en deux classes: les points bleus au centre, les points orange aux extrémités. Aucun seuil unique ne peut séparer ces deux classes.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\n# Classe bleue: points au centre\nx_blue = np.random.uniform(-0.5, 0.5, 15)\n# Classe orange: points aux extrémités\nx_orange = np.concatenate([np.random.uniform(-1.5, -0.8, 8), \n                           np.random.uniform(0.8, 1.5, 8)])\n\nfig, ax = plt.subplots(figsize=(10, 2))\nax.scatter(x_blue, np.zeros_like(x_blue), c='tab:blue', s=80, label='Classe A', zorder=3)\nax.scatter(x_orange, np.zeros_like(x_orange), c='tab:orange', s=80, label='Classe B', zorder=3)\nax.axhline(0, color='gray', linewidth=0.5, zorder=1)\nax.set_xlim(-2, 2)\nax.set_ylim(-0.5, 0.5)\nax.set_xlabel('$x$')\nax.set_yticks([])\nax.legend(loc='upper right')\nax.set_title('Données 1D: aucun seuil ne sépare les deux classes')\nplt.tight_layout()\n\n\n\nAppliquons maintenant l’expansion \\phi(x) = (x, x^2). Chaque point est projeté sur une parabole dans l’espace 2D. Les points proches de zéro (classe bleue) restent bas, tandis que les points éloignés de zéro (classe orange) montent.\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Gauche: espace original avec tentative de séparation\nax = axes[0]\nax.scatter(x_blue, np.zeros_like(x_blue), c='tab:blue', s=80, zorder=3)\nax.scatter(x_orange, np.zeros_like(x_orange), c='tab:orange', s=80, zorder=3)\nax.axhline(0, color='gray', linewidth=0.5, zorder=1)\nax.axvline(0.3, color='red', linestyle='--', linewidth=2, label='Seuil?')\nax.set_xlim(-2, 2)\nax.set_ylim(-0.5, 0.5)\nax.set_xlabel('$x$')\nax.set_yticks([])\nax.set_title('Espace original: pas de séparation linéaire')\nax.legend()\n\n# Droite: espace transformé\nax = axes[1]\nax.scatter(x_blue, x_blue**2, c='tab:blue', s=80, zorder=3, label='Classe A')\nax.scatter(x_orange, x_orange**2, c='tab:orange', s=80, zorder=3, label='Classe B')\n\n# Ligne de séparation dans l'espace transformé\nx_line = np.linspace(-2, 2, 100)\nthreshold = 0.6\nax.axhline(threshold, color='green', linestyle='-', linewidth=2, label='Frontière linéaire')\nax.fill_between(x_line, 0, threshold, alpha=0.1, color='blue')\nax.fill_between(x_line, threshold, 2.5, alpha=0.1, color='orange')\n\n# Parabole de référence\nx_curve = np.linspace(-1.6, 1.6, 100)\nax.plot(x_curve, x_curve**2, 'k--', alpha=0.3, linewidth=1)\n\nax.set_xlim(-2, 2)\nax.set_ylim(-0.1, 2.5)\nax.set_xlabel('$x$')\nax.set_ylabel('$x^2$')\nax.set_title(r'Espace transformé $\\phi(x) = (x, x^2)$: séparation linéaire!')\nax.legend()\n\nplt.tight_layout()\n\n\n\nDans l’espace transformé, une simple droite horizontale sépare les deux classes. Cette droite correspond, dans l’espace original, à deux seuils: x^2 < 0.6, soit |x| < \\sqrt{0.6} \\approx 0.77. L’expansion de caractéristiques a transformé une frontière de décision non linéaire (un intervalle) en une frontière linéaire (une droite).","type":"content","url":"/learning-problem#de-1d-2d-s-parer-lins-parable","position":61},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"De 2D à 3D: soulever pour séparer","lvl3":"Intuition géométrique: pourquoi la dimension supérieure aide","lvl2":"Classes de modèles et expansion de caractéristiques"},"type":"lvl4","url":"/learning-problem#de-2d-3d-soulever-pour-s-parer","position":62},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"De 2D à 3D: soulever pour séparer","lvl3":"Intuition géométrique: pourquoi la dimension supérieure aide","lvl2":"Classes de modèles et expansion de caractéristiques"},"content":"Passons à un exemple plus visuel. Considérons deux classes disposées en cercles concentriques: la classe bleue forme un disque central, la classe orange forme un anneau extérieur. Aucune droite ne peut séparer ces deux régions.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(123)\n\n# Classe bleue: disque central\nn_blue = 50\nr_blue = np.random.uniform(0, 0.7, n_blue)\ntheta_blue = np.random.uniform(0, 2*np.pi, n_blue)\nx_blue = r_blue * np.cos(theta_blue)\ny_blue = r_blue * np.sin(theta_blue)\n\n# Classe orange: anneau extérieur\nn_orange = 70\nr_orange = np.random.uniform(1.0, 1.5, n_orange)\ntheta_orange = np.random.uniform(0, 2*np.pi, n_orange)\nx_orange = r_orange * np.cos(theta_orange)\ny_orange = r_orange * np.sin(theta_orange)\n\nfig, ax = plt.subplots(figsize=(6, 6))\nax.scatter(x_blue, y_blue, c='tab:blue', s=40, alpha=0.7, label='Classe A')\nax.scatter(x_orange, y_orange, c='tab:orange', s=40, alpha=0.7, label='Classe B')\n\n# Montrer qu'une droite ne peut pas séparer\ntheta_line = np.pi/4\nx_line = np.linspace(-2, 2, 100)\ny_line = np.tan(theta_line) * x_line\nax.plot(x_line, y_line, 'r--', linewidth=2, alpha=0.7, label='Droite?')\n\nax.set_xlim(-2, 2)\nax.set_ylim(-2, 2)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_aspect('equal')\nax.legend()\nax.set_title('Cercles concentriques: pas de séparation linéaire en 2D')\nplt.tight_layout()\n\n\n\nAppliquons l’expansion \\phi(x_1, x_2) = (x_1, x_2, x_1^2 + x_2^2). La troisième coordonnée est le carré de la distance à l’origine: z = r^2. Les points proches du centre (petit r) sont “soulevés” moins haut que les points éloignés (grand r).\n\nL’animation suivante montre cette transformation. En faisant pivoter la vue, on voit que les données, une fois projetées dans l’espace 3D, deviennent séparables par un plan horizontal.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom IPython.display import Image\n\nnp.random.seed(123)\n\n# Classe bleue: disque central\nn_blue = 50\nr_blue = np.random.uniform(0, 0.7, n_blue)\ntheta_blue = np.random.uniform(0, 2*np.pi, n_blue)\nx_blue = r_blue * np.cos(theta_blue)\ny_blue = r_blue * np.sin(theta_blue)\nz_blue = x_blue**2 + y_blue**2\n\n# Classe orange: anneau extérieur  \nn_orange = 70\nr_orange = np.random.uniform(1.0, 1.5, n_orange)\ntheta_orange = np.random.uniform(0, 2*np.pi, n_orange)\nx_orange = r_orange * np.cos(theta_orange)\ny_orange = r_orange * np.sin(theta_orange)\nz_orange = x_orange**2 + y_orange**2\n\n# Créer la figure\nfig = plt.figure(figsize=(8, 6))\nax = fig.add_subplot(111, projection='3d')\n\n# Plan de séparation\nz_sep = 0.75\nxx, yy = np.meshgrid(np.linspace(-1.8, 1.8, 10), np.linspace(-1.8, 1.8, 10))\nzz = np.ones_like(xx) * z_sep\n\ndef init():\n    ax.clear()\n    return []\n\ndef animate(frame):\n    ax.clear()\n    \n    # Élévation: commence à 90° (vue de dessus), descend à 25°\n    if frame < 20:\n        elev = 90 - frame * 3.25  # 90 -> 25\n        azim = 45\n    else:\n        elev = 25\n        azim = 45 + (frame - 20) * 4  # rotation horizontale\n    \n    ax.view_init(elev=elev, azim=azim)\n    \n    # Points\n    ax.scatter(x_blue, y_blue, z_blue, c='tab:blue', s=30, alpha=0.8, label='Classe A')\n    ax.scatter(x_orange, y_orange, z_orange, c='tab:orange', s=30, alpha=0.8, label='Classe B')\n    \n    # Plan de séparation (apparaît après la descente)\n    if frame >= 15:\n        alpha_plane = min(0.3, (frame - 15) * 0.06)\n        ax.plot_surface(xx, yy, zz, alpha=alpha_plane, color='green')\n    \n    ax.set_xlabel('$x_1$')\n    ax.set_ylabel('$x_2$')\n    ax.set_zlabel('$x_1^2 + x_2^2$')\n    ax.set_xlim(-1.8, 1.8)\n    ax.set_ylim(-1.8, 1.8)\n    ax.set_zlim(0, 2.5)\n    \n    # Titre dynamique\n    if frame < 10:\n        ax.set_title('Vue de dessus: cercles concentriques', fontsize=11)\n    elif frame < 20:\n        ax.set_title('Rotation: on découvre la structure 3D...', fontsize=11)\n    else:\n        ax.set_title(r'Espace $\\phi(x_1,x_2) = (x_1, x_2, x_1^2+x_2^2)$: un plan sépare!', fontsize=11)\n    \n    return []\n\nanim = FuncAnimation(fig, animate, init_func=init, frames=60, interval=100, blit=True)\nanim.save('_static/feature_expansion_3d.gif', writer='pillow', fps=10, dpi=100)\nplt.close()\n\n# Afficher le GIF\nImage(filename='_static/feature_expansion_3d.gif')\n\n\n\nL’animation montre comment l’expansion de caractéristiques “déplie” la géométrie des données. Vue de dessus, la structure est celle des cercles concentriques originaux. Mais la troisième dimension z = x_1^2 + x_2^2 sépare verticalement les deux classes: le disque central reste bas, l’anneau extérieur monte. Un plan horizontal (en vert) suffit alors à séparer les classes.\n\nCe plan horizontal z = 0.75 correspond, dans l’espace original 2D, à un cercle de rayon \\sqrt{0.75} \\approx 0.87. La frontière de décision linéaire en 3D devient une frontière circulaire en 2D.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(123)\n\n# Recréer les données (même seed que l'animation)\nn_blue = 50\nr_blue = np.random.uniform(0, 0.7, n_blue)\ntheta_blue = np.random.uniform(0, 2*np.pi, n_blue)\nx_blue = r_blue * np.cos(theta_blue)\ny_blue = r_blue * np.sin(theta_blue)\n\nn_orange = 70\nr_orange = np.random.uniform(1.0, 1.5, n_orange)\ntheta_orange = np.random.uniform(0, 2*np.pi, n_orange)\nx_orange = r_orange * np.cos(theta_orange)\ny_orange = r_orange * np.sin(theta_orange)\n\nfig, ax = plt.subplots(figsize=(6, 6))\nax.scatter(x_blue, y_blue, c='tab:blue', s=40, alpha=0.7, label='Classe A')\nax.scatter(x_orange, y_orange, c='tab:orange', s=40, alpha=0.7, label='Classe B')\n\n# Cercle de décision (projection du plan z = 0.75)\ntheta_circle = np.linspace(0, 2*np.pi, 100)\nr_decision = np.sqrt(0.75)\nax.plot(r_decision * np.cos(theta_circle), r_decision * np.sin(theta_circle), \n        'g-', linewidth=2.5, label=f'Frontière: $r = {r_decision:.2f}$')\n\nax.set_xlim(-2, 2)\nax.set_ylim(-2, 2)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_aspect('equal')\nax.legend()\nax.set_title('Frontière de décision projetée en 2D')\nplt.tight_layout()\n\n\n\n","type":"content","url":"/learning-problem#de-2d-3d-soulever-pour-s-parer","position":63},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Le principe unificateur","lvl3":"Intuition géométrique: pourquoi la dimension supérieure aide","lvl2":"Classes de modèles et expansion de caractéristiques"},"type":"lvl4","url":"/learning-problem#le-principe-unificateur","position":64},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Le principe unificateur","lvl3":"Intuition géométrique: pourquoi la dimension supérieure aide","lvl2":"Classes de modèles et expansion de caractéristiques"},"content":"Ces exemples de régression et de classification illustrent le même principe géométrique:\n\n\n\nRégression\n\nClassification\n\nObjectif\n\nAjuster les données\n\nSéparer les classes\n\nDans l’espace original\n\nCourbe (parabole, etc.)\n\nFrontière courbe (cercle, etc.)\n\nDans l’espace des caractéristiques\n\nHyperplan d’ajustement\n\nHyperplan séparateur\n\nLa courbe/frontière est...\n\nL’intersection du plan avec la surface \\phi(x)\n\nLa projection de l’hyperplan\n\nL’expansion de caractéristiques transforme un problème non linéaire en un problème linéaire dans un espace de dimension supérieure. Les modèles linéaires, simples à optimiser et à analyser, deviennent alors suffisants pour capturer des structures complexes.\n\nEn augmentant la dimension de l’espace de représentation, nous augmentons la capacité du modèle. Pour la classification, un résultat classique de géométrie affirme que N points en position générale sont presque sûrement séparables par un hyperplan si la dimension de l’espace est au moins N. Pour la régression, un polynôme de degré N-1 peut interpoler exactement N points.\n\nMais cette flexibilité a un coût: plus l’espace est grand, plus le modèle risque de mémoriser les particularités des données d’entraînement plutôt que d’apprendre la structure sous-jacente. C’est le compromis biais-variance, et c’est pourquoi la régularisation (vue précédemment) est si importante pour les modèles à haute capacité.","type":"content","url":"/learning-problem#le-principe-unificateur","position":65},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Évaluation et choix de modèle","lvl2":"Classes de modèles et expansion de caractéristiques"},"type":"lvl3","url":"/learning-problem#id-valuation-et-choix-de-mod-le","position":66},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Évaluation et choix de modèle","lvl2":"Classes de modèles et expansion de caractéristiques"},"content":"En pratique, nous estimons le risque par le risque empirique sur un ensemble de test \\mathcal{D}_{\\text{test}} disjoint de l’ensemble d’entraînement. Un troisième ensemble, l’ensemble de validation, sert à choisir parmi plusieurs modèles ou à régler des hyperparamètres. L’ensemble de test doit rester intact jusqu’à l’évaluation finale, pour fournir une estimation non biaisée.\n\nCette séparation est importante. Si nous utilisons l’ensemble de test pour faire des choix (quel modèle garder, quelle valeur d’hyperparamètre utiliser), l’estimation de performance sur ce même ensemble devient optimiste.","type":"content","url":"/learning-problem#id-valuation-et-choix-de-mod-le","position":67},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Hyperparamètres et validation","lvl3":"Évaluation et choix de modèle","lvl2":"Classes de modèles et expansion de caractéristiques"},"type":"lvl4","url":"/learning-problem#hyperparam-tres-et-validation","position":68},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Hyperparamètres et validation","lvl3":"Évaluation et choix de modèle","lvl2":"Classes de modèles et expansion de caractéristiques"},"content":"De nombreux modèles ont des hyperparamètres: des choix qui doivent être faits avant l’entraînement et qui ne sont pas appris à partir des données. Le degré k d’un polynôme, le nombre de voisins dans les k plus proches voisins, ou le coefficient de régularisation \\lambda sont des exemples d’hyperparamètres.\n\nUn hyperparamètre mal choisi peut mener au surapprentissage (modèle trop complexe) ou au sous-apprentissage (modèle trop simple). La méthode standard pour choisir un hyperparamètre est la validation: on réserve une partie des données (typiquement 20%) comme ensemble de validation, on entraîne le modèle pour plusieurs valeurs de l’hyperparamètre, et on retient celle qui minimise l’erreur sur l’ensemble de validation.\n\nPlus formellement, pour un hyperparamètre h, définissons le risque de validation:\\hat{\\mathcal{R}}^{\\text{val}}_h = \\hat{\\mathcal{R}}\\left(\\hat{f}_h(\\mathcal{D}_{\\text{train}}), \\mathcal{D}_{\\text{valid}}\\right)\n\noù \\hat{f}_h(\\mathcal{D}_{\\text{train}}) est le modèle entraîné avec l’hyperparamètre h. La recherche par grille consiste à évaluer ce risque pour un ensemble de valeurs candidates et à retenir:h^* = \\arg\\min_{h \\in \\{h_1, \\ldots, h_K\\}} \\hat{\\mathcal{R}}^{\\text{val}}_h\n\nUne fois h^* choisi, on peut ré-entraîner le modèle sur l’ensemble des données (entraînement + validation) pour obtenir le modèle final.","type":"content","url":"/learning-problem#hyperparam-tres-et-validation","position":69},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Validation croisée","lvl3":"Évaluation et choix de modèle","lvl2":"Classes de modèles et expansion de caractéristiques"},"type":"lvl4","url":"/learning-problem#validation-crois-e","position":70},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Validation croisée","lvl3":"Évaluation et choix de modèle","lvl2":"Classes de modèles et expansion de caractéristiques"},"content":"Quand les données sont peu nombreuses, réserver 20% pour la validation peut être coûteux. La validation croisée offre une alternative.\n\nL’idée est de partitionner les données en K blocs. Pour chaque bloc k, on entraîne le modèle sur les K-1 autres blocs et on évalue sur le bloc k. Le risque de validation croisée est la moyenne des K évaluations:\\hat{\\mathcal{R}}^{\\text{cv}}_h = \\frac{1}{K} \\sum_{k=1}^K \\hat{\\mathcal{R}}\\left(\\hat{f}_h(\\mathcal{D}_{-k}), \\mathcal{D}_k\\right)\n\noù \\mathcal{D}_{-k} désigne toutes les données sauf le bloc k.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(10, 3))\n\nK = 5\nfor fold in range(K):\n    for k in range(K):\n        if k == fold:\n            ax.barh(fold, 1, left=k, color='C1', edgecolor='black', linewidth=1, label='Validation' if fold == 0 else '')\n        else:\n            ax.barh(fold, 1, left=k, color='C0', edgecolor='black', linewidth=1, label='Entraînement' if fold == 0 and k == 0 else '')\n\nax.set_yticks(range(K))\nax.set_yticklabels([f'Itération {k+1}' for k in range(K)])\nax.set_xticks(np.arange(K) + 0.5)\nax.set_xticklabels([f'Bloc {k+1}' for k in range(K)])\nax.set_xlabel('Blocs de données')\nax.legend(loc='upper right')\nax.set_title(f'Validation croisée à {K} blocs')\nax.set_xlim(0, K)\n\nplt.tight_layout()\n\n\n\nLe cas particulier K = N (un bloc par exemple) est appelé validation croisée leave-one-out. Elle utilise au maximum les données disponibles, mais est coûteuse en calcul. En pratique, K = 5 ou K = 10 offrent un bon compromis.\n\nMise en garde: la fuite d’information\n\nLes outils modernes de génération de code peuvent produire des pipelines d’apprentissage automatique complets en quelques minutes. Mais ces pipelines peuvent contenir des erreurs subtiles qui mènent à des résultats trop beaux pour être vrais.\n\nUn exemple: un praticien utilise un assistant de programmation pour construire un modèle prédictif. L’erreur d’entraînement passe de 0.20 à 0.01 en quelques itérations. Mais en examinant le code, il découvre que le modèle utilise des caractéristiques qui ne seraient pas disponibles au moment du déploiement.\n\nCe phénomène s’appelle la fuite d’information. Le modèle ne généralise pas: il triche. Les métriques d’entraînement sont excellentes, mais le modèle échouera en déploiement.\n\nVotre rôle: auditer les pipelines, vérifier que les caractéristiques utilisées seront disponibles en production, et maintenir une séparation stricte entre les données d’entraînement et de test.","type":"content","url":"/learning-problem#validation-crois-e","position":71},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Biais inductifs","lvl2":"Classes de modèles et expansion de caractéristiques"},"type":"lvl3","url":"/learning-problem#biais-inductifs","position":72},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Biais inductifs","lvl2":"Classes de modèles et expansion de caractéristiques"},"content":"Il n’existe pas de modèle universel qui fonctionne optimalement pour tous les problèmes. Ce résultat, connu sous le nom de théorème du no free lunch, affirme qu’un algorithme d’apprentissage qui performe bien sur une classe de problèmes performe nécessairement moins bien sur d’autres.\n\nTout modèle encode des biais inductifs: des hypothèses implicites ou explicites sur la structure du problème. La régression linéaire suppose que la relation entre entrées et sorties est linéaire. Les k plus proches voisins supposent que les points proches dans l’espace des entrées ont des sorties similaires. Les modèles plus complexes, comme les réseaux de neurones, encodent d’autres hypothèses sur la structure des données.\n\nCes hypothèses sont nécessaires pour que l’apprentissage soit possible. Sans elles, nous n’aurions aucune raison de croire que la performance sur l’échantillon d’entraînement prédit la performance sur de nouvelles données. Le choix du modèle et de ses hypothèses est une décision que l’algorithme ne peut pas prendre seul; elle requiert une connaissance du domaine.","type":"content","url":"/learning-problem#biais-inductifs","position":73},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Fonctions de perte de substitution"},"type":"lvl2","url":"/learning-problem#fonctions-de-perte-de-substitution","position":74},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Fonctions de perte de substitution"},"content":"La perte 0-1 pose un problème pratique. Les méthodes d’optimisation itératives, comme la descente de gradient, requièrent que la fonction objectif soit différentiable. Or la perte 0-1 est constante par morceaux: sa dérivée est nulle presque partout et indéfinie aux points de discontinuité.\n\nNous contournons ce problème en utilisant des fonctions de perte de substitution: des approximations convexes et différentiables de la perte originale.\n\nPour la classification binaire, plutôt que de prédire directement une classe, les modèles produisent souvent un score s = f(\\mathbf{x}) (un nombre réel). La prédiction de classe se fait ensuite en prenant le signe de ce score: si s > 0, on prédit la classe +1; si s < 0, on prédit la classe -1. La valeur absolue de s mesure la confiance: plus |s| est grand, plus le modèle est confiant dans sa prédiction.\n\nPour la classification binaire avec y \\in \\{-1, +1\\}, la perte logistique est:\\ell_{\\text{log}}(y, s) = \\log(1 + e^{-y \\cdot s})\n\noù s = f(\\mathbf{x}) est le score produit par le modèle. Cette fonction est convexe et différentiable partout. Lorsque y et s ont le même signe (prédiction correcte avec confiance), la perte est faible. Lorsqu’ils ont des signes opposés (erreur), la perte croît linéairement avec l’amplitude de l’erreur.\n\nLa perte à charnière (hinge loss) est utilisée dans les machines à vecteurs de support:\\ell_{\\text{hinge}}(y, s) = \\max(0, 1 - y \\cdot s)\n\nCette fonction est convexe mais non différentiable au point y \\cdot s = 1. Elle est nulle lorsque la prédiction est correcte avec une marge suffisante (y \\cdot s \\geq 1), et croît linéairement sinon.\n\nCes deux fonctions majorent la perte 0-1: pour tout y et s, nous avons \\ell_{0-1} \\leq \\ell_{\\text{log}} et \\ell_{0-1} \\leq \\ell_{\\text{hinge}}. Minimiser ces substituts garantit donc un certain contrôle sur la perte originale.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Margin: y * s (positive = correct prediction, negative = error)\nmargin = np.linspace(-3, 3, 500)\n\n# 0-1 loss: 1 if margin < 0, else 0\nloss_01 = (margin < 0).astype(float)\n\n# Logistic loss: log(1 + exp(-margin))\nloss_log = np.log(1 + np.exp(-margin))\n\n# Hinge loss: max(0, 1 - margin)\nloss_hinge = np.maximum(0, 1 - margin)\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nax.plot(margin, loss_01, 'k-', linewidth=2, label='Perte 0-1')\nax.plot(margin, loss_log, 'C0-', linewidth=2, label='Perte logistique')\nax.plot(margin, loss_hinge, 'C1-', linewidth=2, label='Perte à charnière')\n\nax.axvline(0, color='gray', linestyle=':', alpha=0.5)\nax.axhline(1, color='gray', linestyle=':', alpha=0.3)\n\nax.set_xlabel(r'Marge $y \\cdot s$')\nax.set_ylabel('Perte')\nax.set_xlim(-3, 3)\nax.set_ylim(-0.1, 4)\nax.legend()\nax.set_title('Fonctions de perte de substitution comme bornes supérieures convexes')\n\n# Annotate regions\nax.text(-1.5, 3.5, 'Erreur\\n(prédiction incorrecte)', ha='center', fontsize=9, color='gray')\nax.text(1.5, 0.3, 'Correct\\n(prédiction juste)', ha='center', fontsize=9, color='gray')\n\nplt.tight_layout()\n\n\n\nLa figure montre les trois fonctions de perte en fonction de la marge y \\cdot s. Une marge positive indique une prédiction correcte (le signe de s correspond à y), une marge négative indique une erreur. La perte 0-1 est discontinue au point y \\cdot s = 0. Les pertes logistique et à charnière sont continues et convexes, ce qui permet d’utiliser des méthodes d’optimisation par gradient. Elles majorent partout la perte 0-1.","type":"content","url":"/learning-problem#fonctions-de-perte-de-substitution","position":75},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Le cadre probabiliste"},"type":"lvl2","url":"/learning-problem#le-cadre-probabiliste","position":76},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Le cadre probabiliste"},"content":"Jusqu’ici, nous avons choisi des fonctions de perte de manière ad hoc: la perte quadratique semble raisonnable pour la régression, la perte logistique pour la classification. Mais d’où viennent ces choix? Existe-t-il un principe unificateur?\n\nLe cadre probabiliste offre une réponse: plutôt que de choisir une perte arbitraire, nous modélisons explicitement comment les données ont été générées. Cette section présente d’abord le cadre général de l’inférence bayésienne, puis développe deux approches concrètes: le maximum de vraisemblance (EMV) et le maximum a posteriori (MAP).","type":"content","url":"/learning-problem#le-cadre-probabiliste","position":77},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Le cadre bayésien","lvl2":"Le cadre probabiliste"},"type":"lvl3","url":"/learning-problem#le-cadre-bay-sien","position":78},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Le cadre bayésien","lvl2":"Le cadre probabiliste"},"content":"La statistique bayésienne propose un cadre général pour l’estimation de paramètres. Au lieu d’estimer un point unique, elle caractérise notre incertitude sur les paramètres par une distribution de probabilité.\n\nLe théorème de Bayes nous dit comment mettre à jour nos croyances sur les paramètres \\boldsymbol{\\theta} après avoir observé des données \\mathcal{D}:p(\\boldsymbol{\\theta} | \\mathcal{D}) = \\frac{p(\\boldsymbol{\\theta}) \\, p(\\mathcal{D} | \\boldsymbol{\\theta})}{p(\\mathcal{D})}\n\nChaque terme a un nom et un rôle précis:\n\np(\\boldsymbol{\\theta} | \\mathcal{D}) est la distribution a posteriori: notre croyance sur \\boldsymbol{\\theta} après avoir vu les données\n\np(\\boldsymbol{\\theta}) est la distribution a priori: notre croyance sur \\boldsymbol{\\theta} avant d’observer les données\n\np(\\mathcal{D} | \\boldsymbol{\\theta}) est la vraisemblance: la probabilité des données pour un choix de paramètres\n\np(\\mathcal{D}) = \\int p(\\boldsymbol{\\theta}') p(\\mathcal{D} | \\boldsymbol{\\theta}') d\\boldsymbol{\\theta}' est la vraisemblance marginale: une constante de normalisation\n\nL’a priori encode notre connaissance préalable. Pour une pièce de monnaie, nous pourrions croire que \\theta est probablement proche de 0.5. L’a posteriori combine cette croyance avec l’évidence des données.","type":"content","url":"/learning-problem#le-cadre-bay-sien","position":79},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Le prédicteur de Bayes optimal","lvl2":"Le cadre probabiliste"},"type":"lvl3","url":"/learning-problem#le-pr-dicteur-de-bayes-optimal","position":80},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Le prédicteur de Bayes optimal","lvl2":"Le cadre probabiliste"},"content":"Commençons par un idéal théorique. Si nous connaissions la vraie distribution conjointe p(\\mathbf{x}, y), quelle fonction f minimiserait le risque?\\mathcal{R}(f) = \\mathbb{E}_{p(\\mathbf{x}, y)}[\\ell(y, f(\\mathbf{x}))] = \\int \\int \\ell(y, f(\\mathbf{x})) \\, p(y | \\mathbf{x}) \\, p(\\mathbf{x}) \\, dy \\, d\\mathbf{x}\n\nPuisque p(\\mathbf{x}) est toujours positif, minimiser cette intégrale revient à minimiser, pour chaque \\mathbf{x}, l’espérance conditionnelle de la perte. Le prédicteur de Bayes optimal est donc:f^*(\\mathbf{x}) = \\arg\\min_{\\hat{y}} \\mathbb{E}_{p(y|\\mathbf{x})}[\\ell(y, \\hat{y})]\n\nLa réponse dépend de la fonction de perte choisie.\n\nPour la perte quadratique \\ell(y, \\hat{y}) = (y - \\hat{y})^2, développons l’espérance:\\mathbb{E}[(y - \\hat{y})^2 | \\mathbf{x}] = \\mathbb{E}[y^2 | \\mathbf{x}] - 2\\hat{y}\\mathbb{E}[y | \\mathbf{x}] + \\hat{y}^2\n\nC’est une fonction quadratique en \\hat{y}. En dérivant et en posant la dérivée égale à zéro:\\frac{\\partial}{\\partial \\hat{y}} \\mathbb{E}[(y - \\hat{y})^2 | \\mathbf{x}] = -2\\mathbb{E}[y | \\mathbf{x}] + 2\\hat{y} = 0 \\quad \\Rightarrow \\quad \\hat{y}^* = \\mathbb{E}[y | \\mathbf{x}]\n\nLe prédicteur optimal est la moyenne conditionnelle.\n\nPour la perte 0-1 en classification \\ell(y, \\hat{y}) = \\mathbf{1}[y \\neq \\hat{y}], l’espérance est:\\mathbb{E}[\\mathbf{1}[y \\neq \\hat{y}] | \\mathbf{x}] = P(y \\neq \\hat{y} | \\mathbf{x}) = 1 - P(y = \\hat{y} | \\mathbf{x})\n\nMinimiser cette quantité revient à maximiser P(y = \\hat{y} | \\mathbf{x}), donc à choisir la classe la plus probable:\\hat{y}^* = \\arg\\max_c \\, p(y = c | \\mathbf{x})\n\nLe prédicteur optimal est le mode conditionnel. Chaque perte définit son propre prédicteur optimal.\n\nCe prédicteur est un repère théorique: aucun algorithme ne peut faire mieux, car il suppose l’accès à la vraie distribution. La différence entre le risque d’un prédicteur appris et ce risque de Bayes mesure ce que nous perdons en ne connaissant pas la vraie distribution.","type":"content","url":"/learning-problem#le-pr-dicteur-de-bayes-optimal","position":81},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Prédiction bayésienne et distribution prédictive a posteriori","lvl2":"Le cadre probabiliste"},"type":"lvl3","url":"/learning-problem#pr-diction-bay-sienne-et-distribution-pr-dictive-a-posteriori","position":82},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Prédiction bayésienne et distribution prédictive a posteriori","lvl2":"Le cadre probabiliste"},"content":"En pratique, nous ne connaissons pas p(y|\\mathbf{x}). Nous avons un modèle paramétrique p(y|\\mathbf{x}, \\boldsymbol{\\theta}) et une distribution a posteriori p(\\boldsymbol{\\theta}|\\mathcal{D}) sur les paramètres. L’approche bayésienne complète consiste à moyenner les prédictions sur tous les paramètres possibles, pondérés par leur probabilité a posteriori:p(y|\\mathbf{x}, \\mathcal{D}) = \\int p(y|\\mathbf{x}, \\boldsymbol{\\theta}) \\, p(\\boldsymbol{\\theta}|\\mathcal{D}) \\, d\\boldsymbol{\\theta}\n\nCette distribution prédictive a posteriori intègre l’incertitude sur les paramètres. Elle ne s’engage pas sur une valeur unique de \\boldsymbol{\\theta}, mais considère toutes les valeurs plausibles.\n\nLe problème: cette intégrale est rarement calculable analytiquement. Elle nécessite de sommer sur un espace de paramètres de grande dimension, ce qui est coûteux ou impossible en pratique. C’est pourquoi nous recourons souvent à des estimateurs ponctuels: plutôt que d’intégrer sur tous les \\boldsymbol{\\theta}, nous en choisissons un seul, comme l’EMV ou le MAP.","type":"content","url":"/learning-problem#pr-diction-bay-sienne-et-distribution-pr-dictive-a-posteriori","position":83},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Utilité du modèle probabiliste","lvl2":"Le cadre probabiliste"},"type":"lvl3","url":"/learning-problem#utilit-du-mod-le-probabiliste","position":84},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Utilité du modèle probabiliste","lvl2":"Le cadre probabiliste"},"content":"Si nous finissons souvent par utiliser un estimateur ponctuel, pourquoi adopter le cadre probabiliste? Plusieurs raisons:\n\nJustifier la fonction de perte: La perte quadratique découle naturellement de l’hypothèse de bruit gaussien. La perte logarithmique vient du principe de maximum de vraisemblance. Le cadre probabiliste explique pourquoi ces choix sont raisonnables.\n\nQuantifier l’incertitude: Au-delà de la prédiction ponctuelle \\hat{y} = f(\\mathbf{x}; \\hat{\\boldsymbol{\\theta}}), nous pouvons donner un intervalle de prédiction. Sous un modèle gaussien, y a environ 95% de chances de tomber dans [f(\\mathbf{x}) - 2\\sigma, f(\\mathbf{x}) + 2\\sigma].\n\nComparer des modèles: La vraisemblance marginale p(\\mathcal{D}) permet de comparer des modèles de complexités différentes, pénalisant automatiquement les modèles trop complexes.\n\nOuvrir la porte à l’inférence complète: Quand les ressources le permettent (méthodes de Monte Carlo, inférence variationnelle), nous pouvons approximer la distribution prédictive complète plutôt que de nous limiter à un point.","type":"content","url":"/learning-problem#utilit-du-mod-le-probabiliste","position":85},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"type":"lvl3","url":"/learning-problem#maximum-de-vraisemblance","position":86},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"content":"Le maximum de vraisemblance est la première approche concrète dans le cadre probabiliste: nous cherchons les paramètres qui rendent nos observations les plus probables.","type":"content","url":"/learning-problem#maximum-de-vraisemblance","position":87},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Construction de la vraisemblance","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"type":"lvl4","url":"/learning-problem#construction-de-la-vraisemblance","position":88},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Construction de la vraisemblance","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"content":"Supposons que nous avons un modèle paramétrique p(y|\\mathbf{x}; \\boldsymbol{\\theta}) qui, pour chaque entrée \\mathbf{x} et choix de paramètres \\boldsymbol{\\theta}, définit une distribution sur les sorties possibles y. Par exemple, en régression, ce pourrait être une gaussienne centrée sur f(\\mathbf{x}; \\boldsymbol{\\theta}).\n\nConsidérons un seul exemple (\\mathbf{x}_1, y_1). Pour des paramètres \\boldsymbol{\\theta} fixés, nous pouvons évaluer p(y_1 | \\mathbf{x}_1; \\boldsymbol{\\theta}): la probabilité (ou densité) que le modèle assigne à l’observation y_1. Si cette valeur est élevée, les paramètres \\boldsymbol{\\theta} “expliquent bien” cette observation. Si elle est faible, y_1 est une valeur improbable sous ce modèle.\n\nAvec deux exemples indépendants (\\mathbf{x}_1, y_1) et (\\mathbf{x}_2, y_2), la probabilité conjointe est le produit:p(y_1, y_2 | \\mathbf{x}_1, \\mathbf{x}_2; \\boldsymbol{\\theta}) = p(y_1 | \\mathbf{x}_1; \\boldsymbol{\\theta}) \\cdot p(y_2 | \\mathbf{x}_2; \\boldsymbol{\\theta})\n\nAvec N exemples indépendants, nous obtenons la vraisemblance:\\mathcal{L}(\\boldsymbol{\\theta}) = \\prod_{i=1}^N p(y_i | \\mathbf{x}_i; \\boldsymbol{\\theta})\n\nCette quantité est une fonction de \\boldsymbol{\\theta}. Elle répond à la question: pour ce choix de paramètres, quelle est la probabilité d’avoir observé exactement ces données?","type":"content","url":"/learning-problem#construction-de-la-vraisemblance","position":89},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Pourquoi maximiser?","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"type":"lvl4","url":"/learning-problem#pourquoi-maximiser","position":90},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Pourquoi maximiser?","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"content":"Si \\mathcal{L}(\\boldsymbol{\\theta}_A) > \\mathcal{L}(\\boldsymbol{\\theta}_B), alors les données observées sont plus probables sous \\boldsymbol{\\theta}_A que sous \\boldsymbol{\\theta}_B. Les paramètres \\boldsymbol{\\theta}_A rendent les observations moins “surprenantes”.\n\nL’estimateur du maximum de vraisemblance (EMV, ou MLE pour maximum likelihood estimator en anglais) choisit les paramètres qui maximisent cette probabilité:\\hat{\\boldsymbol{\\theta}}_{\\text{EMV}} = \\arg\\max_{\\boldsymbol{\\theta}} \\mathcal{L}(\\boldsymbol{\\theta}) = \\arg\\max_{\\boldsymbol{\\theta}} \\prod_{i=1}^N p(y_i | \\mathbf{x}_i; \\boldsymbol{\\theta})\n\nC’est le choix de paramètres sous lequel nos données sont les plus “attendues”.","type":"content","url":"/learning-problem#pourquoi-maximiser","position":91},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Du produit à la somme","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"type":"lvl4","url":"/learning-problem#du-produit-la-somme","position":92},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Du produit à la somme","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"content":"En pratique, multiplier N probabilités (souvent petites) pose des problèmes numériques: le résultat devient rapidement trop petit pour être représenté par un ordinateur. Le logarithme résout ce problème: il transforme le produit en somme et, comme c’est une fonction croissante, il ne change pas le maximiseur:\\log \\mathcal{L}(\\boldsymbol{\\theta}) = \\sum_{i=1}^N \\log p(y_i | \\mathbf{x}_i; \\boldsymbol{\\theta})\n\nPour l’optimisation, nous préférons minimiser plutôt que maximiser (par convention). La log-vraisemblance négative (NLV, ou NLL pour negative log-likelihood en anglais) est notre fonction objectif:\\text{NLV}(\\boldsymbol{\\theta}) = -\\sum_{i=1}^N \\log p(y_i | \\mathbf{x}_i; \\boldsymbol{\\theta})\n\nRemarquez la structure: c’est une somme sur les exemples d’une quantité -\\log p(y_i | \\mathbf{x}_i; \\boldsymbol{\\theta}) qui dépend de chaque observation. Cette quantité joue le rôle d’une fonction de perte. Le maximum de vraisemblance est donc un cas particulier de la minimisation du risque empirique, où la perte est définie par le modèle probabiliste lui-même.","type":"content","url":"/learning-problem#du-produit-la-somme","position":93},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Régression avec bruit gaussien: d’où vient la perte quadratique?","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"type":"lvl4","url":"/learning-problem#r-gression-avec-bruit-gaussien-do-vient-la-perte-quadratique","position":94},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Régression avec bruit gaussien: d’où vient la perte quadratique?","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"content":"Appliquons ce principe à la régression. Le modèle de génération des données suppose que la sortie observée est la prédiction “vraie” du modèle, corrompue par un bruit aléatoire gaussien:y = f(\\mathbf{x}; \\boldsymbol{\\theta}) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\n\nCe modèle dit que si nous connaissions les vrais paramètres \\boldsymbol{\\theta} et que nous mesurions y pour un \\mathbf{x} donné, nous obtiendrions f(\\mathbf{x}; \\boldsymbol{\\theta}) plus ou moins \\sigma la plupart du temps.\n\nLa distribution conditionnelle qui en découle est:p(y|\\mathbf{x}; \\boldsymbol{\\theta}) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - f(\\mathbf{x}; \\boldsymbol{\\theta}))^2}{2\\sigma^2}\\right)\n\nCalculons la log-vraisemblance négative:\\text{NLV}(\\boldsymbol{\\theta}) = -\\sum_{i=1}^N \\log p(y_i | \\mathbf{x}_i; \\boldsymbol{\\theta}) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y_i - f(\\mathbf{x}_i; \\boldsymbol{\\theta}))^2 + \\frac{N}{2}\\log(2\\pi\\sigma^2)\n\nLe second terme ne dépend pas de \\boldsymbol{\\theta}. Minimiser la NLV revient donc exactement à minimiser la somme des erreurs quadratiques.\n\nC’est un résultat fondamental: la perte quadratique n’est pas un choix arbitraire. Elle découle naturellement de l’hypothèse que les erreurs de mesure suivent une loi gaussienne. Le maximum de vraisemblance sous bruit gaussien coïncide avec les moindres carrés.\n\nDans ce modèle, nous avons supposé que la variance \\sigma^2 est constante pour toutes les entrées \\mathbf{x}. C’est ce qu’on appelle la régression homoscédastique (du grec homos, même, et skedasis, dispersion). C’est l’hypothèse standard en régression linéaire.\n\nEn pratique, l’incertitude peut varier selon l’entrée. Par exemple, les mesures à haute vitesse peuvent être plus bruitées que celles à basse vitesse. La régression hétéroscédastique modélise cette variation en faisant dépendre la variance de \\mathbf{x}:p(y|\\mathbf{x}; \\boldsymbol{\\theta}) = \\mathcal{N}(y | f_\\mu(\\mathbf{x}; \\boldsymbol{\\theta}), f_\\sigma(\\mathbf{x}; \\boldsymbol{\\theta})^2)\n\noù f_\\mu prédit la moyenne et f_\\sigma prédit l’écart-type. Ce modèle est plus flexible mais requiert d’apprendre des paramètres supplémentaires.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nfrom scipy.stats import norm\nfrom IPython.display import HTML\n\n# Générer des données synthétiques\nnp.random.seed(42)\nN = 100\nx_data = np.random.uniform(0.5, 9.5, N)\nf_mu = lambda x: 0.5 * x + 1\n\n# Homoscédastique: variance constante\nsigma_homo = 0.7\ny_homo = f_mu(x_data) + np.random.normal(0, sigma_homo, N)\n\n# Hétéroscédastique: variance croissante\nf_sigma = lambda x: 0.3 + 0.12 * x\ny_hetero = f_mu(x_data) + np.random.normal(0, f_sigma(x_data))\n\n# Configuration de la figure\nfig, axes = plt.subplots(1, 2, figsize=(11, 4.5))\nx_line = np.linspace(0, 10, 100)\ny_pdf_range = np.linspace(-3, 9, 200)\nscale = 2.5  # échelle pour afficher les PDFs\n\ndef init():\n    for ax in axes:\n        ax.clear()\n    return []\n\ndef animate(frame):\n    x_current = 0.5 + frame * 9 / 59  # balayer de 0.5 à 9.5\n    \n    for idx, (ax, y_data, title, color, get_sigma) in enumerate([\n        (axes[0], y_homo, 'Régression homoscédastique', 'steelblue', lambda x: sigma_homo),\n        (axes[1], y_hetero, 'Régression hétéroscédastique', 'coral', f_sigma)\n    ]):\n        ax.clear()\n        \n        # Données et ligne de régression\n        ax.scatter(x_data, y_data, alpha=0.4, s=20, c='gray', zorder=1)\n        ax.plot(x_line, f_mu(x_line), 'k-', linewidth=2, zorder=2)\n        \n        # Gaussienne à la position actuelle\n        mu = f_mu(x_current)\n        sigma = get_sigma(x_current)\n        pdf = norm.pdf(y_pdf_range, mu, sigma)\n        \n        # Afficher la gaussienne \"horizontalement\"\n        ax.fill_betweenx(y_pdf_range, x_current, x_current + scale * pdf, \n                         alpha=0.5, color=color, zorder=3)\n        ax.plot(x_current + scale * pdf, y_pdf_range, color=color, linewidth=2, zorder=4)\n        \n        # Ligne verticale indiquant la position\n        ax.axvline(x_current, color=color, linestyle='--', alpha=0.5, linewidth=1)\n        \n        # Point sur la courbe de régression\n        ax.scatter([x_current], [mu], color='black', s=50, zorder=5)\n        \n        # Bande ±2σ\n        ax.fill_between([x_current - 0.1, x_current + 0.1], \n                        [mu - 2*sigma, mu - 2*sigma], \n                        [mu + 2*sigma, mu + 2*sigma],\n                        alpha=0.2, color=color, zorder=0)\n        \n        ax.set_xlim(-0.5, 12)\n        ax.set_ylim(-2, 8)\n        ax.set_xlabel(r'$x$', fontsize=11)\n        ax.set_ylabel(r'$y$', fontsize=11)\n        sigma_label = r'$\\sigma^2$ constant' if idx == 0 else r'$\\sigma^2(x)$ variable'\n        ax.set_title(f'{title}\\n{sigma_label}', fontsize=11)\n    \n    fig.tight_layout()\n    return []\n\nanim = FuncAnimation(fig, animate, init_func=init, frames=60, interval=80, blit=True)\nanim.save('_static/regression_scedasticity.gif', writer='pillow', fps=12, dpi=100)\nplt.close()\n\n# Afficher le GIF\nfrom IPython.display import Image\nImage(filename='_static/regression_scedasticity.gif')\n\n\n\nPourquoi la gaussienne est-elle verticale?\n\nLa gaussienne représente p(y|x): la distribution de y sachant x. En régression standard, on suppose que x est mesuré sans erreur et que seul y est bruité. L’objectif des moindres carrés ordinaires minimise donc les distances verticales:\\sum_{i=1}^N (y_i - f(x_i))^2\n\nSi les deux variables avaient de l’incertitude, on utiliserait la régression orthogonale (total least squares). Dans ce cas, on minimise les distances perpendiculaires à la droite:\\sum_{i=1}^N \\frac{(y_i - \\theta_0 - \\theta_1 x_i)^2}{1 + \\theta_1^2}\n\nLe dénominateur 1 + \\theta_1^2 convertit la distance verticale en distance perpendiculaire. Ce modèle est approprié quand x et y sont tous deux des mesures bruitées, par exemple deux instruments mesurant la même quantité physique.\n\nL’animation illustre la différence fondamentale entre les deux modèles. À chaque position x, la distribution conditionnelle p(y|x) est une gaussienne (la “cloche” colorée) centrée sur la courbe de régression f_\\mu(x). Dans le cas homoscédastique (gauche), la cloche garde la même largeur partout. Dans le cas hétéroscédastique (droite), la largeur varie avec x. Ici, l’incertitude augmente vers la droite, ce qui se traduit par une dispersion plus grande des points.","type":"content","url":"/learning-problem#r-gression-avec-bruit-gaussien-do-vient-la-perte-quadratique","position":95},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Régression linéaire homoscédastique","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"type":"lvl4","url":"/learning-problem#r-gression-lin-aire-homosc-dastique","position":96},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Régression linéaire homoscédastique","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"content":"Appliquons maintenant le maximum de vraisemblance au cas le plus courant: la régression linéaire homoscédastique. Le modèle probabiliste est:p(y | \\mathbf{x}; \\boldsymbol{\\theta}, \\sigma^2) = \\mathcal{N}(y | \\boldsymbol{\\theta}^\\top \\mathbf{x}, \\sigma^2)\n\nLa fonction de moyenne est linéaire: f_\\mu(\\mathbf{x}; \\boldsymbol{\\theta}) = \\boldsymbol{\\theta}^\\top \\mathbf{x} = \\sum_{j=0}^d \\theta_j x_j, où nous avons absorbé le biais en posant x_0 = 1. Le vecteur \\boldsymbol{\\theta} \\in \\mathbb{R}^{d+1} contient donc le biais \\theta_0 et les poids \\theta_1, \\ldots, \\theta_d. La variance \\sigma^2 est constante.","type":"content","url":"/learning-problem#r-gression-lin-aire-homosc-dastique","position":97},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl5":"Formulation matricielle","lvl4":"Régression linéaire homoscédastique","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"type":"lvl5","url":"/learning-problem#formulation-matricielle","position":98},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl5":"Formulation matricielle","lvl4":"Régression linéaire homoscédastique","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"content":"Avec N observations \\{(\\mathbf{x}_i, y_i)\\}_{i=1}^N, nous pouvons écrire le modèle sous forme matricielle. Définissons la matrice de conception (design matrix) \\mathbf{X} \\in \\mathbb{R}^{N \\times (d+1)} dont chaque ligne contient une observation augmentée d’un 1 pour le biais:\\mathbf{X} = \\begin{pmatrix} 1 & x_{11} & x_{12} & \\cdots & x_{1d} \\\\ 1 & x_{21} & x_{22} & \\cdots & x_{2d} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 1 & x_{N1} & x_{N2} & \\cdots & x_{Nd} \\end{pmatrix}, \\quad \\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_N \\end{pmatrix}, \\quad \\boldsymbol{\\theta} = \\begin{pmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_d \\end{pmatrix}\n\nLe vecteur des prédictions est \\hat{\\mathbf{y}} = \\mathbf{X}\\boldsymbol{\\theta}, et le vecteur des résidus est \\mathbf{r} = \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}.\n\nLa somme des carrés des résidus (residual sum of squares, RSS) s’écrit:\\text{RSS}(\\boldsymbol{\\theta}) = \\sum_{i=1}^N (y_i - \\boldsymbol{\\theta}^\\top \\mathbf{x}_i)^2 = \\|\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}\\|_2^2 = (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta})^\\top (\\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta})\n\nComme nous l’avons vu, minimiser la NLV sous bruit gaussien homoscédastique revient à minimiser le RSS (ou de manière équivalente, l’EQM = RSS/N).","type":"content","url":"/learning-problem#formulation-matricielle","position":99},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl5":"Solution analytique: les équations normales","lvl4":"Régression linéaire homoscédastique","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"type":"lvl5","url":"/learning-problem#solution-analytique-les-quations-normales","position":100},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl5":"Solution analytique: les équations normales","lvl4":"Régression linéaire homoscédastique","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"content":"Pour trouver le minimum, nous calculons le gradient du RSS par rapport à \\boldsymbol{\\theta} et l’égalons à zéro. En développant:\\text{RSS}(\\boldsymbol{\\theta}) = \\mathbf{y}^\\top \\mathbf{y} - 2\\boldsymbol{\\theta}^\\top \\mathbf{X}^\\top \\mathbf{y} + \\boldsymbol{\\theta}^\\top \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\theta}\n\nLe gradient est:\\nabla_{\\boldsymbol{\\theta}} \\text{RSS}(\\boldsymbol{\\theta}) = -2\\mathbf{X}^\\top \\mathbf{y} + 2\\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\theta}\n\nEn posant ce gradient égal à zéro, nous obtenons les équations normales:\\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\theta} = \\mathbf{X}^\\top \\mathbf{y}\n\nSi la matrice \\mathbf{X}^\\top \\mathbf{X} est inversible, la solution unique est:\\hat{\\boldsymbol{\\theta}}_{\\text{EMV}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}\n\nCette solution porte le nom d’estimateur des moindres carrés ordinaires (MCO, ou ordinary least squares, OLS). Elle est exactement équivalente à l’EMV sous l’hypothèse de bruit gaussien homoscédastique.","type":"content","url":"/learning-problem#solution-analytique-les-quations-normales","position":101},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl5":"Conditions d’existence de la solution","lvl4":"Régression linéaire homoscédastique","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"type":"lvl5","url":"/learning-problem#conditions-dexistence-de-la-solution","position":102},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl5":"Conditions d’existence de la solution","lvl4":"Régression linéaire homoscédastique","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"content":"La matrice \\mathbf{X}^\\top \\mathbf{X} est de taille (d+1) \\times (d+1). Elle est inversible si et seulement si \\mathbf{X} est de rang plein colonne, c’est-à-dire si les colonnes de \\mathbf{X} sont linéairement indépendantes. Cela requiert:\n\nPlus d’observations que de paramètres: N \\geq d + 1\n\nPas de colinéarité parfaite: aucune caractéristique ne doit être une combinaison linéaire exacte des autres\n\nQuand \\mathbf{X}^\\top \\mathbf{X} est mal conditionnée (presque singulière), de petites perturbations dans les données peuvent causer de grandes variations dans la solution. C’est l’un des problèmes que la régularisation permet de résoudre.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Données synthétiques: relation linéaire avec bruit\nnp.random.seed(42)\nN = 30\nx = np.random.uniform(0, 10, N)\ny_true = 2.5 * x + 3  # vraie relation: y = 2.5x + 3\ny = y_true + np.random.normal(0, 3, N)  # ajout de bruit gaussien\n\n# Construction de la matrice de conception (avec colonne de 1 pour le biais)\nX = np.column_stack([np.ones(N), x])\n\n# Solution OLS: w = (X^T X)^{-1} X^T y\nXtX = X.T @ X\nXty = X.T @ y\nw_ols = np.linalg.solve(XtX, Xty)\nb_ols, slope_ols = w_ols[0], w_ols[1]\n\n# Prédictions\nx_grid = np.linspace(0, 10, 100)\ny_pred = b_ols + slope_ols * x_grid\n\n# Visualisation\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n# Gauche: données et droite ajustée\nax = axes[0]\nax.scatter(x, y, alpha=0.7, label='Observations')\nax.plot(x_grid, y_pred, 'k-', linewidth=2, label=f'MCO: $y = {slope_ols:.2f}x + {b_ols:.2f}$')\nax.plot(x_grid, 2.5 * x_grid + 3, 'g--', alpha=0.5, label='Vraie relation')\nax.set_xlabel('$x$')\nax.set_ylabel('$y$')\nax.legend()\nax.set_title('Régression linéaire par moindres carrés')\n\n# Droite: résidus\nax = axes[1]\ny_fitted = b_ols + slope_ols * x\nresiduals = y - y_fitted\nax.stem(x, residuals, basefmt=' ', linefmt='C0-', markerfmt='C0o')\nax.axhline(0, color='gray', linestyle='-', alpha=0.3)\nax.set_xlabel('$x$')\nax.set_ylabel(r'Résidu $r_i = y_i - \\hat{y}_i$')\nax.set_title(f'Résidus (RSS = {np.sum(residuals**2):.1f})')\n\nplt.tight_layout()\n\n\n\nLa figure de gauche montre les données et la droite ajustée par moindres carrés. La figure de droite montre les résidus: les écarts entre les observations et les prédictions. L’EMV minimise la somme des carrés de ces résidus.","type":"content","url":"/learning-problem#conditions-dexistence-de-la-solution","position":103},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Exemple: pharmacocinétique","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"type":"lvl4","url":"/learning-problem#exemple-pharmacocin-tique","position":104},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Exemple: pharmacocinétique","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"content":"L’EMV s’applique à des modèles non linéaires. Considérons la concentration d’un médicament dans le sang après administration orale. Les données suivantes proviennent d’une étude sur la théophylline, un bronchodilatateur:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize\n\n# Données pharmacocinétiques: théophylline, sujet 1 (Boeckmann et al., 1994)\ntime = np.array([0, 0.25, 0.57, 1.12, 2.02, 3.82, 5.10, 7.03, 9.05, 12.12, 24.37])\nconc = np.array([0.74, 2.84, 6.57, 10.50, 9.66, 8.58, 8.36, 7.47, 6.89, 5.94, 3.28])\n\n# Model: C(t) = C0 * exp(-k * t) for t > t_peak\n# We'll fit on the decay phase (after peak)\npeak_idx = np.argmax(conc)\nt_decay = time[peak_idx:]\nc_decay = conc[peak_idx:]\n\n# MLE: minimize NLL under Gaussian noise\ndef neg_log_likelihood(params, t, c):\n    C0, k, sigma = params\n    if sigma <= 0 or k <= 0 or C0 <= 0:\n        return np.inf\n    pred = C0 * np.exp(-k * (t - t[0]))\n    nll = 0.5 * len(t) * np.log(2 * np.pi * sigma**2)\n    nll += 0.5 * np.sum((c - pred)**2) / sigma**2\n    return nll\n\n# Initial guess and optimization\nx0 = [c_decay[0], 0.1, 1.0]\nresult = minimize(neg_log_likelihood, x0, args=(t_decay, c_decay), method='Nelder-Mead')\nC0_mle, k_mle, sigma_mle = result.x\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n# Left: data and fit\nax = axes[0]\nax.scatter(time, conc, s=50, zorder=5, label='Observations')\nt_grid = np.linspace(time[peak_idx], 25, 100)\nax.plot(t_grid, C0_mle * np.exp(-k_mle * (t_grid - time[peak_idx])), 'k--', \n        label=f'EMV: $C_0$={C0_mle:.1f}, $k$={k_mle:.2f}')\nax.axvline(time[peak_idx], color='gray', linestyle=':', alpha=0.5)\nax.set_xlabel('Temps (h)')\nax.set_ylabel('Concentration (mg/L)')\nax.legend()\nax.set_title('Concentration plasmatique de théophylline')\n\n# Right: residuals\nax = axes[1]\npred_decay = C0_mle * np.exp(-k_mle * (t_decay - t_decay[0]))\nresiduals = c_decay - pred_decay\nax.stem(t_decay, residuals, basefmt=' ')\nax.axhline(0, color='gray', linestyle='-', alpha=0.3)\nax.set_xlabel('Temps (h)')\nax.set_ylabel('Résidu (mg/L)')\nax.set_title(rf'$\\sigma$ estimé: {sigma_mle:.2f} mg/L')\n\nplt.tight_layout()\n\n\n\nLe modèle C(t) = C_0 e^{-kt} décrit la décroissance exponentielle après le pic de concentration. Les paramètres C_0 (concentration initiale) et k (constante d’élimination) sont estimés par maximum de vraisemblance sous l’hypothèse d’un bruit gaussien. Cette approche est identique à celle des moindres carrés, mais elle fournit également une estimation de l’écart-type du bruit \\sigma.","type":"content","url":"/learning-problem#exemple-pharmacocin-tique","position":105},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Classification binaire","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"type":"lvl4","url":"/learning-problem#classification-binaire","position":106},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Classification binaire","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"content":"La perte 0-1 pour la classification est discontinue, ce qui empêche l’utilisation de méthodes de gradient. La fonction sigmoïde \\sigma(z) = 1/(1 + e^{-z}) contourne ce problème: c’est une approximation lisse de la fonction échelon (step function). Elle transforme n’importe quel score réel en une valeur dans l’intervalle (0, 1), que nous pouvons interpréter comme une probabilité.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n\n# Create figure\nfig, ax = plt.subplots(figsize=(8, 5))\n\n# Define x range\nz = np.linspace(-4, 4, 200)\n\n# Step function (Heaviside)\nstep = (z >= 0).astype(float)\n\n# Sigmoid function with temperature parameter\ndef sigmoid(z, alpha=1):\n    return 1 / (1 + np.exp(-alpha * z))\n\n# Initialize plot\nline_step, = ax.plot(z, step, 'k--', linewidth=2, label='Fonction échelon', alpha=0.7)\nline_sigmoid, = ax.plot([], [], 'b-', linewidth=2, label='Sigmoïde $\\\\sigma(\\\\alpha z)$')\nax.axhline(0.5, color='gray', linestyle=':', alpha=0.5, linewidth=1)\nax.axvline(0, color='gray', linestyle=':', alpha=0.5, linewidth=1)\nax.set_xlim(-4, 4)\nax.set_ylim(-0.1, 1.1)\nax.set_xlabel('$z$')\nax.set_ylabel('$\\\\sigma(\\\\alpha z)$')\nax.set_title('Approximation de la fonction échelon par la sigmoïde')\nax.legend(loc='best')\nax.grid(True, alpha=0.3)\n\n# Animation function\ndef animate(frame):\n    # Alpha increases from 0.5 to 10\n    alpha = 0.5 + (frame / 100) * 9.5\n    y = sigmoid(z, alpha)\n    line_sigmoid.set_data(z, y)\n    ax.set_title(f'Approximation de la fonction échelon par la sigmoïde ($\\\\alpha = {alpha:.2f}$)')\n    return line_sigmoid,\n\n# Create animation\nanim = FuncAnimation(fig, animate, frames=100, interval=50, blit=True, repeat=True)\nanim.save('_static/sigmoid_approximation.gif', writer='pillow', fps=20, dpi=100)\nplt.close()\n\n# Afficher le GIF\nfrom IPython.display import Image\nImage(filename='_static/sigmoid_approximation.gif')\n\n\n\nL’animation montre comment la sigmoïde \\sigma(\\alpha z) se rapproche de la fonction échelon lorsque le paramètre \\alpha augmente. Pour \\alpha = 1, la sigmoïde est douce; pour \\alpha grand, elle devient presque aussi abrupte que la fonction échelon, tout en restant différentiable.\n\nCette interprétation probabiliste n’est pas qu’une astuce numérique. Elle correspond exactement à modéliser Y | \\mathbf{X} par une distribution de Bernoulli dont le paramètre dépend de l’entrée.\n\nPour la classification binaire avec y \\in \\{0, 1\\}, nous modélisons la probabilité de la classe positive par:p(y = 1 | \\mathbf{x}; \\boldsymbol{\\theta}) = \\sigma(f(\\mathbf{x}; \\boldsymbol{\\theta})) = \\frac{1}{1 + e^{-f(\\mathbf{x}; \\boldsymbol{\\theta})}}\n\noù \\sigma est la fonction sigmoïde et f(\\mathbf{x}; \\boldsymbol{\\theta}) est le logit (ou log-odds), le score brut du modèle avant transformation. Le logit est le logarithme du rapport des probabilités: \\log \\frac{p(y=1|\\mathbf{x})}{p(y=0|\\mathbf{x})} = \\log \\frac{p}{1-p}. La distribution conditionnelle suit une loi de Bernoulli:p(y|\\mathbf{x}; \\boldsymbol{\\theta}) = \\sigma(f(\\mathbf{x}; \\boldsymbol{\\theta}))^y (1 - \\sigma(f(\\mathbf{x}; \\boldsymbol{\\theta})))^{1-y}\n\nLa log-vraisemblance négative est:\\text{NLV}(\\boldsymbol{\\theta}) = -\\sum_{i=1}^N \\left[ y_i \\log \\sigma(f(\\mathbf{x}_i; \\boldsymbol{\\theta})) + (1-y_i) \\log(1 - \\sigma(f(\\mathbf{x}_i; \\boldsymbol{\\theta}))) \\right]\n\nCette quantité est l’entropie croisée binaire. Elle correspond à la perte logistique, à une reparamétrisation près.","type":"content","url":"/learning-problem#classification-binaire","position":107},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Classification multiclasse","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"type":"lvl4","url":"/learning-problem#classification-multiclasse","position":108},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Classification multiclasse","lvl3":"Maximum de vraisemblance","lvl2":"Le cadre probabiliste"},"content":"Pour la classification avec C classes (C > 2), nous généralisons le modèle binaire en utilisant la distribution catégorielle (ou multinomiale). Au lieu de modéliser une seule probabilité p(y=1|\\mathbf{x}), nous modélisons un vecteur de probabilités \\boldsymbol{\\pi}(\\mathbf{x}) = [\\pi_1(\\mathbf{x}), \\ldots, \\pi_C(\\mathbf{x})] où \\pi_c(\\mathbf{x}) = p(y=c|\\mathbf{x}) et \\sum_{c=1}^C \\pi_c(\\mathbf{x}) = 1.\n\nPour transformer les scores bruts du modèle en probabilités, nous utilisons la fonction softmax:\\pi_c(\\mathbf{x}; \\boldsymbol{\\theta}) = \\frac{\\exp(f_c(\\mathbf{x}; \\boldsymbol{\\theta}))}{\\sum_{j=1}^C \\exp(f_j(\\mathbf{x}; \\boldsymbol{\\theta}))}\n\noù f_c(\\mathbf{x}; \\boldsymbol{\\theta}) est le score pour la classe c. La fonction softmax généralise la sigmoïde au cas multiclasse: elle transforme C scores réels en un vecteur de probabilités qui somme à 1.\n\nLa distribution conditionnelle suit une loi catégorielle:p(y|\\mathbf{x}; \\boldsymbol{\\theta}) = \\prod_{c=1}^C \\pi_c(\\mathbf{x}; \\boldsymbol{\\theta})^{\\mathbf{1}[y = c]}\n\noù \\mathbf{1}[y = c] vaut 1 si y = c et 0 sinon. En utilisant l’encodage one-hot \\mathbf{y} = [\\mathbf{1}[y=1], \\ldots, \\mathbf{1}[y=C]]^\\top, cette expression devient:p(y|\\mathbf{x}; \\boldsymbol{\\theta}) = \\prod_{c=1}^C \\pi_c(\\mathbf{x}; \\boldsymbol{\\theta})^{y_c}\n\nLa log-vraisemblance négative est:\\text{NLV}(\\boldsymbol{\\theta}) = -\\sum_{i=1}^N \\sum_{c=1}^C y_{ic} \\log \\pi_c(\\mathbf{x}_i; \\boldsymbol{\\theta})\n\noù y_{ic} = \\mathbf{1}[y_i = c]. Cette quantité est l’entropie croisée multiclasse. Elle généralise l’entropie croisée binaire au cas où il y a plus de deux classes.\n\nPour la classification binaire avec C=2, le softmax se réduit à la sigmoïde. En effet, si nous définissons s = f_1(\\mathbf{x}) - f_2(\\mathbf{x}), alors:\\pi_1 = \\frac{e^{f_1}}{e^{f_1} + e^{f_2}} = \\frac{1}{1 + e^{-(f_1 - f_2)}} = \\sigma(s)\n\nLe modèle binaire et le modèle multiclasse partagent donc la même structure probabiliste, avec la distribution catégorielle comme généralisation naturelle de la distribution de Bernoulli.","type":"content","url":"/learning-problem#classification-multiclasse","position":109},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Maximum a posteriori","lvl2":"Le cadre probabiliste"},"type":"lvl3","url":"/learning-problem#maximum-a-posteriori","position":110},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Maximum a posteriori","lvl2":"Le cadre probabiliste"},"content":"Plutôt que de travailler avec la distribution a posteriori complète (ce qui peut être coûteux), nous pouvons chercher son mode: la valeur des paramètres la plus probable a posteriori. C’est l’estimateur du maximum a posteriori (MAP):\\hat{\\boldsymbol{\\theta}}_{\\text{MAP}} = \\arg\\max_{\\boldsymbol{\\theta}} p(\\boldsymbol{\\theta} | \\mathcal{D}) = \\arg\\max_{\\boldsymbol{\\theta}} p(\\boldsymbol{\\theta}) \\, p(\\mathcal{D} | \\boldsymbol{\\theta})\n\nLe dénominateur p(\\mathcal{D}) ne dépend pas de \\boldsymbol{\\theta} et peut être ignoré pour l’optimisation. En passant au logarithme:\\hat{\\boldsymbol{\\theta}}_{\\text{MAP}} = \\arg\\max_{\\boldsymbol{\\theta}} \\left[ \\log p(\\mathcal{D} | \\boldsymbol{\\theta}) + \\log p(\\boldsymbol{\\theta}) \\right]\n\nCette expression révèle une structure familière. Si nous posons C(\\boldsymbol{\\theta}) = -\\log p(\\boldsymbol{\\theta}), nous obtenons:\\hat{\\boldsymbol{\\theta}}_{\\text{MAP}} = \\arg\\min_{\\boldsymbol{\\theta}} \\left[ \\text{NLV}(\\boldsymbol{\\theta}) + C(\\boldsymbol{\\theta}) \\right]\n\nC’est exactement la forme du risque empirique régularisé. La régularisation correspond à l’ajout d’un a priori sur les paramètres. Le terme de régularisation C(\\boldsymbol{\\theta}) est le logarithme négatif de la distribution a priori.","type":"content","url":"/learning-problem#maximum-a-posteriori","position":111},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Le maximum de vraisemblance comme cas particulier","lvl3":"Maximum a posteriori","lvl2":"Le cadre probabiliste"},"type":"lvl4","url":"/learning-problem#le-maximum-de-vraisemblance-comme-cas-particulier","position":112},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Le maximum de vraisemblance comme cas particulier","lvl3":"Maximum a posteriori","lvl2":"Le cadre probabiliste"},"content":"Que se passe-t-il si nous n’avons aucune préférence a priori sur les paramètres? Cela correspond à un a priori uniforme (ou constant): p(\\boldsymbol{\\theta}) = \\text{constante}.\n\nDans ce cas, \\log p(\\boldsymbol{\\theta}) est une constante qui n’affecte pas l’optimisation, et le MAP se réduit à l’EMV:\\hat{\\boldsymbol{\\theta}}_{\\text{MAP}} = \\hat{\\boldsymbol{\\theta}}_{\\text{EMV}} \\quad \\text{quand } p(\\boldsymbol{\\theta}) = \\text{constante}\n\nL’EMV est donc un cas particulier du MAP: celui où nous supposons implicitement que toutes les valeurs de paramètres sont également plausibles avant d’observer les données. Cette perspective unifie les deux approches dans un même cadre.","type":"content","url":"/learning-problem#le-maximum-de-vraisemblance-comme-cas-particulier","position":113},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Limites de l’a priori uniforme","lvl3":"Maximum a posteriori","lvl2":"Le cadre probabiliste"},"type":"lvl4","url":"/learning-problem#limites-de-la-priori-uniforme","position":114},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Limites de l’a priori uniforme","lvl3":"Maximum a posteriori","lvl2":"Le cadre probabiliste"},"content":"L’a priori uniforme (et donc l’EMV) peut être problématique quand les données sont peu nombreuses. Considérons l’estimation de la probabilité \\theta qu’une pièce tombe sur face.\n\nSupposons que nous lancions la pièce 3 fois et obtenions 3 faces. L’estimateur du maximum de vraisemblance pour une distribution de Bernoulli est:\\hat{\\theta}_{\\text{EMV}} = \\frac{N_1}{N_0 + N_1} = \\frac{3}{0 + 3} = 1\n\noù N_1 est le nombre de faces et N_0 le nombre de piles. Cette estimation dit que la probabilité d’obtenir face est de 100%. Si nous utilisions ce modèle pour prédire de futurs lancers, nous prédirons toujours face, ce qui est peu plausible pour une vraie pièce.\n\nLe problème est que l’EMV (avec son a priori uniforme implicite) dispose de suffisamment de flexibilité pour reproduire parfaitement les données d’entraînement, même quand celles-ci sont peu nombreuses ou non représentatives. Un a priori informatif peut atténuer ce problème.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n# MLE vs MAP for Bernoulli with few observations\nfig, axes = plt.subplots(1, 3, figsize=(12, 4))\n\n# Different sample sizes\nsamples_list = [\n    [1, 1, 1],           # 3 heads\n    [1, 1, 1, 0],        # 3 heads, 1 tail\n    [1, 1, 1, 0, 0, 1, 1, 0, 1, 1]  # 7 heads, 3 tails\n]\n\ntheta_grid = np.linspace(0.001, 0.999, 200)\n\nfor ax, samples in zip(axes, samples_list):\n    n1 = sum(samples)  # heads\n    n0 = len(samples) - n1  # tails\n    \n    # MLE\n    theta_mle = n1 / (n0 + n1)\n    \n    # Likelihood (unnormalized)\n    likelihood = theta_grid**n1 * (1 - theta_grid)**n0\n    likelihood = likelihood / likelihood.max()\n    \n    ax.plot(theta_grid, likelihood, 'b-', linewidth=2, label='Vraisemblance')\n    ax.axvline(theta_mle, color='b', linestyle='--', alpha=0.7,\n               label=f'EMV: {theta_mle:.2f}')\n    ax.axvline(0.5, color='gray', linestyle=':', alpha=0.5, label=r'$\\theta = 0.5$')\n    \n    ax.set_xlabel(r'$\\theta$')\n    ax.set_ylabel('Vraisemblance (normalisée)')\n    ax.set_title(f'{n1} faces, {n0} piles (N={len(samples)})')\n    ax.legend(fontsize=8)\n    ax.set_xlim(0, 1)\n\nplt.tight_layout()\n\n\n\nLa figure montre la vraisemblance pour différents échantillons. Avec seulement 3 observations (toutes faces), la vraisemblance est maximale à \\theta = 1. En augmentant la taille de l’échantillon, l’estimation devient plus raisonnable. Voyons comment un a priori non uniforme peut aider.","type":"content","url":"/learning-problem#limites-de-la-priori-uniforme","position":115},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Exemple: lissage de Laplace","lvl3":"Maximum a posteriori","lvl2":"Le cadre probabiliste"},"type":"lvl4","url":"/learning-problem#exemple-lissage-de-laplace","position":116},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Exemple: lissage de Laplace","lvl3":"Maximum a posteriori","lvl2":"Le cadre probabiliste"},"content":"Revenons à notre exemple de la pièce de monnaie. Utilisons un a priori Beta sur \\theta:p(\\theta) = \\text{Beta}(\\theta | a, b) \\propto \\theta^{a-1} (1-\\theta)^{b-1}\n\nLes paramètres a et b contrôlent la forme de l’a priori. Pour a = b = 2, l’a priori favorise des valeurs de \\theta proches de 0.5.\n\nLe logarithme de l’a posteriori (vraisemblance plus a priori) est:\\log p(\\theta | \\mathcal{D}) \\propto N_1 \\log \\theta + N_0 \\log(1-\\theta) + (a-1) \\log \\theta + (b-1) \\log(1-\\theta)\n\nEn dérivant et en résolvant, l’estimateur MAP est:\\hat{\\theta}_{\\text{MAP}} = \\frac{N_1 + a - 1}{N_1 + N_0 + a + b - 2}\n\nAvec a = b = 2 et nos 3 observations de faces:\\hat{\\theta}_{\\text{MAP}} = \\frac{3 + 2 - 1}{3 + 0 + 2 + 2 - 2} = \\frac{4}{5} = 0.8\n\nCette estimation est plus raisonnable que l’EMV \\hat{\\theta}_{\\text{EMV}} = 1. L’a priori “tire” l’estimation vers des valeurs moins extrêmes.\n\nLe choix a = b = 2 correspond au lissage de Laplace (ou add-one smoothing): c’est comme si nous avions observé une face et une pile supplémentaires avant de commencer. Cette technique est particulièrement utile quand certains événements n’ont jamais été observés dans les données.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\ntheta_grid = np.linspace(0.001, 0.999, 200)\n\n# Left: Prior, likelihood, posterior\nax = axes[0]\nn1, n0 = 3, 0  # 3 heads, 0 tails\na, b = 2, 2    # Beta prior parameters\n\n# Prior\nprior = stats.beta.pdf(theta_grid, a, b)\nprior = prior / prior.max()\n\n# Likelihood\nlikelihood = theta_grid**n1 * (1 - theta_grid)**n0\nlikelihood = likelihood / likelihood.max()\n\n# Posterior (Beta(a + n1, b + n0))\nposterior = stats.beta.pdf(theta_grid, a + n1, b + n0)\nposterior = posterior / posterior.max()\n\nax.plot(theta_grid, prior, 'g-', linewidth=2, label='A priori Beta(2,2)')\nax.plot(theta_grid, likelihood, 'b--', linewidth=2, label='Vraisemblance')\nax.plot(theta_grid, posterior, 'r-', linewidth=2, label='A posteriori')\n\ntheta_mle = n1 / (n1 + n0)\ntheta_map = (n1 + a - 1) / (n1 + n0 + a + b - 2)\nax.axvline(theta_mle, color='b', linestyle=':', alpha=0.7, label=f'EMV: {theta_mle:.2f}')\nax.axvline(theta_map, color='r', linestyle=':', alpha=0.7, label=f'MAP: {theta_map:.2f}')\n\nax.set_xlabel(r'$\\theta$')\nax.set_ylabel('Densité (normalisée)')\nax.set_title('3 faces, 0 pile')\nax.legend(fontsize=8)\nax.set_xlim(0, 1)\n\n# Right: Effect of different priors\nax = axes[1]\npriors = [(1, 1, 'Uniforme'), (2, 2, 'Beta(2,2)'), (5, 5, 'Beta(5,5)')]\n\nfor a, b, label in priors:\n    theta_map = (n1 + a - 1) / (n1 + n0 + a + b - 2)\n    posterior = stats.beta.pdf(theta_grid, a + n1, b + n0)\n    posterior = posterior / posterior.max()\n    ax.plot(theta_grid, posterior, linewidth=2, label=f'{label}: MAP={theta_map:.2f}')\n\nax.axvline(1.0, color='gray', linestyle='--', alpha=0.5, label='EMV: 1.00')\nax.set_xlabel(r'$\\theta$')\nax.set_ylabel('A posteriori (normalisé)')\nax.set_title('Effet de différents a priori')\nax.legend(fontsize=8)\nax.set_xlim(0, 1)\n\nplt.tight_layout()\n\n\n\nLa figure de gauche montre comment l’a posteriori combine l’a priori et la vraisemblance. L’a priori Beta(2,2) “tire” l’estimation vers 0.5, résultant en un MAP de 0.8 au lieu de l’EMV de 1.0. La figure de droite montre l’effet de différents a priori: plus l’a priori est fort (variance faible), plus l’estimation est proche de 0.5.","type":"content","url":"/learning-problem#exemple-lissage-de-laplace","position":117},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Régression ridge = MAP avec prior gaussien","lvl3":"Maximum a posteriori","lvl2":"Le cadre probabiliste"},"type":"lvl4","url":"/learning-problem#r-gression-ridge-map-avec-prior-gaussien","position":118},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Régression ridge = MAP avec prior gaussien","lvl3":"Maximum a posteriori","lvl2":"Le cadre probabiliste"},"content":"Appliquons maintenant ce cadre bayésien à la régression linéaire. Si nous plaçons un a priori gaussien isotrope sur les paramètres:p(\\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{\\theta} | \\mathbf{0}, \\sigma_\\theta^2 \\mathbf{I})\n\ncet a priori exprime la croyance que les paramètres sont probablement proches de zéro, avec une incertitude contrôlée par \\sigma_\\theta^2.\n\nLe logarithme négatif de cet a priori est:-\\log p(\\boldsymbol{\\theta}) = \\frac{1}{2\\sigma_\\theta^2} \\|\\boldsymbol{\\theta}\\|_2^2 + \\text{constante}\n\nL’estimateur MAP devient:\\hat{\\boldsymbol{\\theta}}_{\\text{MAP}} = \\arg\\min_{\\boldsymbol{\\theta}} \\left[ \\text{NLV}(\\boldsymbol{\\theta}) + \\frac{1}{2\\sigma_\\theta^2}\\|\\boldsymbol{\\theta}\\|_2^2 \\right]\n\nC’est exactement la régression ridge, avec \\lambda = 1/(2\\sigma_\\theta^2). Cette correspondance nous donne une interprétation de l’hyperparamètre:\n\nGrande valeur de \\lambda (petite variance \\sigma_\\theta^2): forte croyance que les paramètres sont proches de zéro\n\nPetite valeur de \\lambda (grande variance \\sigma_\\theta^2): a priori peu informatif, on fait confiance aux données\n\nL’a priori gaussien sur les paramètres est parfois appelé dégradation des poids (weight decay) dans le contexte des réseaux de neurones, car il “tire” les paramètres vers zéro pendant l’entraînement.","type":"content","url":"/learning-problem#r-gression-ridge-map-avec-prior-gaussien","position":119},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Unification: deux langages pour un même problème"},"type":"lvl2","url":"/learning-problem#unification-deux-langages-pour-un-m-me-probl-me","position":120},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Unification: deux langages pour un même problème"},"content":"Les sections précédentes ont présenté deux approches pour l’apprentissage supervisé. La première, fondée sur la théorie de la décision, définit une fonction de perte et minimise le risque empirique. La seconde, probabiliste, modélise la distribution des données et estime les paramètres par maximum de vraisemblance ou maximum a posteriori.\n\nCes deux approches semblent différentes, mais elles aboutissent aux mêmes algorithmes. En choisissant la perte logarithmique \\ell(y, \\hat{y}) = -\\log p(y | \\hat{y}), le risque empirique devient exactement la log-vraisemblance négative (à un facteur 1/N près). Minimiser l’un revient à minimiser l’autre. Sous bruit gaussien, cette perte se réduit à la perte quadratique; sous modèle de Bernoulli, à l’entropie croisée.\n\nDe même, ajouter une régularisation \\ell_2 au risque empirique revient à supposer un a priori gaussien sur les paramètres. La régression ridge n’est rien d’autre que l’estimation MAP avec cet a priori. Le coefficient \\lambda encode la force de notre croyance a priori: plus \\lambda est grand, plus nous “tirons” les paramètres vers zéro.\n\nPourquoi alors utiliser deux langages? Parce qu’ils éclairent des aspects différents du problème. Le langage décisionnel (risque, perte, minimisation) est opérationnel: il dit comment construire un algorithme. Le langage probabiliste (vraisemblance, a priori, a posteriori) est interprétatif: il dit ce que nous supposons sur les données et pourquoi nos choix sont raisonnables. Ensemble, ils permettent de concevoir des algorithmes et de comprendre leur comportement.","type":"content","url":"/learning-problem#unification-deux-langages-pour-un-m-me-probl-me","position":121},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Interprétation informationnelle","lvl2":"Unification: deux langages pour un même problème"},"type":"lvl3","url":"/learning-problem#interpr-tation-informationnelle","position":122},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Interprétation informationnelle","lvl2":"Unification: deux langages pour un même problème"},"content":"La théorie de l’information offre une troisième perspective. L’EMV peut se comprendre comme la recherche du modèle paramétrique le plus proche de la distribution empirique des données.\n\nLa distribution empirique place une masse 1/N sur chaque observation:p_{\\mathcal{D}}(y) = \\frac{1}{N} \\sum_{i=1}^N \\delta(y - y_i)\n\nLa divergence de Kullback-Leibler mesure la dissimilarité entre deux distributions:D_{\\text{KL}}(p \\| q) = \\sum_y p(y) \\log \\frac{p(y)}{q(y)}\n\nCette quantité est toujours positive ou nulle, et vaut zéro si et seulement si les deux distributions sont identiques. En posant p = p_{\\mathcal{D}} (ce que nous avons observé) et q = p(\\cdot | \\boldsymbol{\\theta}) (notre modèle), on peut montrer que:\\arg\\min_{\\boldsymbol{\\theta}} D_{\\text{KL}}(p_{\\mathcal{D}} \\| p(\\cdot|\\boldsymbol{\\theta})) = \\arg\\min_{\\boldsymbol{\\theta}} \\text{NLV}(\\boldsymbol{\\theta})\n\nL’EMV trouve les paramètres qui rendent notre modèle aussi proche que possible de ce que nous avons observé, au sens de la divergence KL. Cette interprétation géométrique complète les perspectives décisionnelle et probabiliste.","type":"content","url":"/learning-problem#interpr-tation-informationnelle","position":123},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Résumé"},"type":"lvl2","url":"/learning-problem#r-sum","position":124},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Résumé"},"content":"Ce chapitre a développé le cadre formel de l’apprentissage supervisé à travers deux perspectives complémentaires.\n\nLa perspective décisionnelle définit le risque comme l’erreur moyenne attendue sur de nouvelles données, puis le remplace par le risque empirique (calculable sur les données d’entraînement). La minimisation du risque empirique cherche le modèle qui minimise cette erreur d’entraînement, en espérant qu’il généralisera. La régularisation pénalise la complexité pour éviter le surapprentissage.\n\nLa perspective probabiliste modélise explicitement comment les données ont été générées. Le maximum de vraisemblance (EMV) trouve les paramètres qui rendent les observations les plus probables; le maximum a posteriori (MAP) incorpore des croyances a priori sur les paramètres.\n\nCes deux perspectives aboutissent aux mêmes algorithmes. Sous bruit gaussien, l’EMV coïncide avec les moindres carrés. La régularisation \\ell_2 correspond au MAP avec a priori gaussien sur les paramètres. Le langage décisionnel est opérationnel (comment construire l’algorithme); le langage probabiliste est interprétatif (pourquoi ces choix sont raisonnables).\n\nLe chapitre suivant développe les outils théoriques pour quantifier quand et comment le risque empirique prédit le vrai risque.","type":"content","url":"/learning-problem#r-sum","position":125},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Exercices"},"type":"lvl2","url":"/learning-problem#exercices","position":126},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Exercices"},"content":"Exercice 1: Usure d’outil\n\nUn machiniste mesure l’usure d’un outil de coupe (en mm) à différents temps de coupe (en minutes):import numpy as np\n\n# Données d'usure d'outil (simulées selon une loi de puissance avec bruit)\ntime = np.array([2.0, 5.1, 8.2, 11.3, 14.4, 17.6, 20.7, 23.8, 26.9, 30.0])\nwear = np.array([0.08, 0.14, 0.17, 0.21, 0.22, 0.25, 0.27, 0.28, 0.31, 0.32])\n\nL’outil doit être remplacé lorsque l’usure atteint 0.4 mm.\n\nVisualisation. Tracez les données. Quelle forme de relation observez-vous?\n\nAjustement. Ajustez un modèle linéaire w(t) = at + b et un modèle en loi de puissance w(t) = at^b aux données. Pour le second modèle, utilisez une transformation logarithmique: \\log w = \\log a + b \\log t.\n\nComparaison. Calculez le MSE de chaque modèle sur les données. Lequel ajuste mieux?\n\nPrédiction. Selon chaque modèle, à quel moment l’usure atteindra-t-elle 0.4 mm? Les deux modèles donnent-ils la même réponse?\n\nExtrapolation. Si vous n’aviez mesuré que jusqu’à t = 15 min, vos prédictions changeraient-elles? Discutez du risque d’extrapolation.\n\nSolution Exercice 1\n\nVisualisation. Les données montrent une relation non linéaire, concave: l’usure augmente rapidement au début puis ralentit. Cela suggère une loi de puissance avec exposant b < 1.\n\nAjustement.\n\nModèle linéaire: coeffs = np.polyfit(time, wear, 1) donne a \\approx 0.006, b \\approx 0.05.\n\nLoi de puissance: en posant \\log w = \\log a + b \\log t, on ajuste une droite dans l’espace log-log: coeffs = np.polyfit(np.log(time), np.log(wear), 1). On obtient b \\approx 0.5 et a = \\exp(\\text{intercept}) \\approx 0.05.\n\nComparaison. Le MSE du modèle linéaire est typiquement plus élevé car il ne capture pas la courbure. Le modèle en loi de puissance ajuste mieux les données.\n\nPrédiction. Pour trouver t tel que w(t) = 0.4:\n\nLinéaire: t = (0.4 - b) / a\n\nPuissance: t = (0.4 / a)^{1/b}\n\nLes réponses diffèrent significativement car les modèles extrapolent différemment.\n\nExtrapolation. Avec moins de données, les estimations des paramètres changent, et les prédictions au-delà des données observées deviennent plus incertaines. L’extrapolation est risquée car le comportement futur peut ne pas suivre le modèle ajusté sur les données passées.\n\nExercice 2: Risque et risque empirique\n\nSoit un problème de classification binaire avec la perte 0-1. Un classificateur f fait 3 erreurs sur 20 exemples d’entraînement.\n\nQuel est le risque empirique de f sur l’ensemble d’entraînement?\n\nPeut-on en déduire le vrai risque \\mathcal{R}(f)? Pourquoi ou pourquoi pas?\n\nSi nous avions 1000 exemples de test et que f fait 45 erreurs, quelle serait notre meilleure estimation du vrai risque?\n\nSolution Exercice 2\n\nRisque empirique: \\hat{\\mathcal{R}}(f) = \\frac{3}{20} = 0.15 (soit 15% d’erreur).\n\nNon, on ne peut pas en déduire le vrai risque. Le risque empirique sur l’entraînement est une estimation biaisée du vrai risque car:\n\nLe modèle f a été choisi/optimisé pour bien performer sur ces mêmes données\n\nIl y a surapprentissage potentiel: f peut avoir mémorisé des particularités de l’entraînement qui ne généralisent pas\n\nLe risque empirique sur l’entraînement sous-estime généralement le vrai risque\n\nEstimation sur le test: \\hat{\\mathcal{R}}(f) = \\frac{45}{1000} = 0.045 (soit 4.5% d’erreur). Cette estimation est plus fiable car:\n\nLes données de test n’ont pas été utilisées pour construire f\n\nAvec 1000 exemples, l’estimation est plus précise (écart-type \\approx \\sqrt{0.045 \\times 0.955 / 1000} \\approx 0.007)\n\nExercice 3: Maximum de vraisemblance\n\nSoit \\{y_1, \\ldots, y_N\\} un échantillon i.i.d. d’une distribution exponentielle de paramètre \\lambda > 0:p(y | \\lambda) = \\lambda e^{-\\lambda y}, \\quad y \\geq 0\n\nÉcrivez la vraisemblance \\mathcal{L}(\\lambda) et la log-vraisemblance \\log \\mathcal{L}(\\lambda).\n\nDérivez l’estimateur du maximum de vraisemblance (EMV) \\hat{\\lambda}_{\\text{EMV}}.\n\nSi les observations sont y = \\{0.5, 1.2, 0.8, 2.1, 0.3\\}, calculez \\hat{\\lambda}_{\\text{EMV}}.\n\nSolution Exercice 3\n\nVraisemblance et log-vraisemblance:\\mathcal{L}(\\lambda) = \\prod_{i=1}^N \\lambda e^{-\\lambda y_i} = \\lambda^N \\exp\\left(-\\lambda \\sum_{i=1}^N y_i\\right)\\log \\mathcal{L}(\\lambda) = N \\log \\lambda - \\lambda \\sum_{i=1}^N y_i\n\nDérivation de l’EMV:\n\nOn dérive par rapport à \\lambda et on égale à zéro:\\frac{d}{d\\lambda} \\log \\mathcal{L}(\\lambda) = \\frac{N}{\\lambda} - \\sum_{i=1}^N y_i = 0\n\nD’où:\\hat{\\lambda}_{\\text{EMV}} = \\frac{N}{\\sum_{i=1}^N y_i} = \\frac{1}{\\bar{y}}\n\nL’EMV est l’inverse de la moyenne empirique.\n\nApplication numérique:\n\n\\bar{y} = \\frac{0.5 + 1.2 + 0.8 + 2.1 + 0.3}{5} = \\frac{4.9}{5} = 0.98\n\n\\hat{\\lambda}_{\\text{EMV}} = \\frac{1}{0.98} \\approx 1.02\n\nExercice 4: Fonctions de perte\n\nSoit y = 1 (classe positive) et un score s = f(x) = 2.\n\nCalculez la perte 0-1, la perte logistique, et la perte à charnière.\n\nRépétez pour s = -0.5 (prédiction incorrecte).\n\nTracez les trois fonctions de perte en fonction de y \\cdot s pour y \\cdot s \\in [-3, 3]. Vérifiez que les pertes de substitution majorent la perte 0-1.\n\nSolution Exercice 4\n\nPour y = 1 et s = 2 (prédiction correcte, marge y \\cdot s = 2):\n\nPerte 0-1: \\mathbb{1}[\\text{sign}(s) \\neq y] = \\mathbb{1}[1 \\neq 1] = 0\n\nPerte logistique: \\log(1 + e^{-y \\cdot s}) = \\log(1 + e^{-2}) \\approx \\log(1.135) \\approx 0.127\n\nPerte à charnière: \\max(0, 1 - y \\cdot s) = \\max(0, 1 - 2) = \\max(0, -1) = 0\n\nPour y = 1 et s = -0.5 (prédiction incorrecte, marge y \\cdot s = -0.5):\n\nPerte 0-1: \\mathbb{1}[\\text{sign}(-0.5) \\neq 1] = \\mathbb{1}[-1 \\neq 1] = 1\n\nPerte logistique: \\log(1 + e^{-(-0.5)}) = \\log(1 + e^{0.5}) \\approx \\log(2.649) \\approx 0.974\n\nPerte à charnière: \\max(0, 1 - (-0.5)) = \\max(0, 1.5) = 1.5\n\nVérification graphique:import numpy as np\nimport matplotlib.pyplot as plt\n\nmargin = np.linspace(-3, 3, 100)\nloss_01 = (margin < 0).astype(float)\nloss_log = np.log(1 + np.exp(-margin))\nloss_hinge = np.maximum(0, 1 - margin)\n\nplt.plot(margin, loss_01, label='0-1')\nplt.plot(margin, loss_log, label='Logistique')\nplt.plot(margin, loss_hinge, label='Charnière')\nplt.legend()\n\nOn vérifie que pour tout m: \\ell_{\\text{log}}(m) \\geq \\ell_{0-1}(m) et \\ell_{\\text{hinge}}(m) \\geq \\ell_{0-1}(m).","type":"content","url":"/learning-problem#exercices","position":127}]}