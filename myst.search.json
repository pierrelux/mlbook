{"version":"1","records":[{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire"},"type":"lvl1","url":"/appendix-linear-algebra","position":0},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire"},"content":"Cette annexe révise les concepts d’algèbre linéaire utilisés tout au long du livre.","type":"content","url":"/appendix-linear-algebra","position":1},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl2":"Vecteurs et matrices"},"type":"lvl2","url":"/appendix-linear-algebra#vecteurs-et-matrices","position":2},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl2":"Vecteurs et matrices"},"content":"","type":"content","url":"/appendix-linear-algebra#vecteurs-et-matrices","position":3},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Notation","lvl2":"Vecteurs et matrices"},"type":"lvl3","url":"/appendix-linear-algebra#notation","position":4},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Notation","lvl2":"Vecteurs et matrices"},"content":"Un vecteur \\boldsymbol{x} \\in \\mathbb{R}^D est un tableau de D nombres réels:\\boldsymbol{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_D \\end{pmatrix}\n\nPar convention, les vecteurs sont des vecteurs colonnes. Le transposé \\boldsymbol{x}^\\top est un vecteur ligne.\n\nUne matrice \\mathbf{A} \\in \\mathbb{R}^{M \\times N} a M lignes et N colonnes:\\mathbf{A} = \\begin{pmatrix}\na_{11} & a_{12} & \\cdots & a_{1N} \\\\\na_{21} & a_{22} & \\cdots & a_{2N} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{M1} & a_{M2} & \\cdots & a_{MN}\n\\end{pmatrix}","type":"content","url":"/appendix-linear-algebra#notation","position":5},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Opérations de base","lvl2":"Vecteurs et matrices"},"type":"lvl3","url":"/appendix-linear-algebra#op-rations-de-base","position":6},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Opérations de base","lvl2":"Vecteurs et matrices"},"content":"Produit scalaire (dot product):\\boldsymbol{x}^\\top \\boldsymbol{y} = \\sum_{i=1}^D x_i y_i\n\nProduit matrice-vecteur: \\mathbf{A}\\boldsymbol{x} où \\mathbf{A} \\in \\mathbb{R}^{M \\times N} et \\boldsymbol{x} \\in \\mathbb{R}^N donne un vecteur \\in \\mathbb{R}^M.\n\nProduit matriciel: \\mathbf{AB} où \\mathbf{A} \\in \\mathbb{R}^{M \\times K} et \\mathbf{B} \\in \\mathbb{R}^{K \\times N} donne \\mathbf{C} \\in \\mathbb{R}^{M \\times N}.C_{ij} = \\sum_{k=1}^K A_{ik} B_{kj}","type":"content","url":"/appendix-linear-algebra#op-rations-de-base","position":7},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Matrices spéciales","lvl2":"Vecteurs et matrices"},"type":"lvl3","url":"/appendix-linear-algebra#matrices-sp-ciales","position":8},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Matrices spéciales","lvl2":"Vecteurs et matrices"},"content":"Type\n\nDéfinition\n\nIdentité \\mathbf{I}\n\nI_{ij} = 1 si i=j, 0 sinon\n\nDiagonale\n\nA_{ij} = 0 si i \\neq j\n\nSymétrique\n\n\\mathbf{A} = \\mathbf{A}^\\top\n\nOrthogonale\n\n\\mathbf{A}^\\top \\mathbf{A} = \\mathbf{I}\n\nDéfinie positive\n\n\\boldsymbol{x}^\\top \\mathbf{A} \\boldsymbol{x} > 0 pour tout \\boldsymbol{x} \\neq \\mathbf{0}\n\nSemi-définie positive\n\n\\boldsymbol{x}^\\top \\mathbf{A} \\boldsymbol{x} \\geq 0 pour tout \\boldsymbol{x}","type":"content","url":"/appendix-linear-algebra#matrices-sp-ciales","position":9},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl2":"Normes vectorielles et matricielles"},"type":"lvl2","url":"/appendix-linear-algebra#normes-vectorielles-et-matricielles","position":10},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl2":"Normes vectorielles et matricielles"},"content":"","type":"content","url":"/appendix-linear-algebra#normes-vectorielles-et-matricielles","position":11},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Normes vectorielles","lvl2":"Normes vectorielles et matricielles"},"type":"lvl3","url":"/appendix-linear-algebra#normes-vectorielles","position":12},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Normes vectorielles","lvl2":"Normes vectorielles et matricielles"},"content":"La norme \\ell_p d’un vecteur:\\|\\boldsymbol{x}\\|_p = \\left(\\sum_{i=1}^D |x_i|^p\\right)^{1/p}\n\nCas particuliers:\n\nNorme \\ell_1 (Manhattan): \\|\\boldsymbol{x}\\|_1 = \\sum_i |x_i|\n\nNorme \\ell_2 (euclidienne): \\|\\boldsymbol{x}\\|_2 = \\sqrt{\\sum_i x_i^2}\n\nNorme \\ell_\\infty: \\|\\boldsymbol{x}\\|_\\infty = \\max_i |x_i|","type":"content","url":"/appendix-linear-algebra#normes-vectorielles","position":13},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Normes matricielles","lvl2":"Normes vectorielles et matricielles"},"type":"lvl3","url":"/appendix-linear-algebra#normes-matricielles","position":14},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Normes matricielles","lvl2":"Normes vectorielles et matricielles"},"content":"Norme de Frobenius:\\|\\mathbf{A}\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2} = \\sqrt{\\text{tr}(\\mathbf{A}^\\top \\mathbf{A})}\n\nNorme spectrale (norme induite par \\ell_2):\\|\\mathbf{A}\\|_2 = \\sigma_{\\max}(\\mathbf{A})\n\noù \\sigma_{\\max} est la plus grande valeur singulière.","type":"content","url":"/appendix-linear-algebra#normes-matricielles","position":15},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl2":"Valeurs propres et vecteurs propres"},"type":"lvl2","url":"/appendix-linear-algebra#valeurs-propres-et-vecteurs-propres","position":16},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl2":"Valeurs propres et vecteurs propres"},"content":"","type":"content","url":"/appendix-linear-algebra#valeurs-propres-et-vecteurs-propres","position":17},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Définition","lvl2":"Valeurs propres et vecteurs propres"},"type":"lvl3","url":"/appendix-linear-algebra#d-finition","position":18},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Définition","lvl2":"Valeurs propres et vecteurs propres"},"content":"Un vecteur \\boldsymbol{v} \\neq \\mathbf{0} est un vecteur propre de \\mathbf{A} avec valeur propre \\lambda si:\\mathbf{A}\\boldsymbol{v} = \\lambda \\boldsymbol{v}\n\nPour une matrice n \\times n, il existe au plus n valeurs propres distinctes.","type":"content","url":"/appendix-linear-algebra#d-finition","position":19},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Propriétés","lvl2":"Valeurs propres et vecteurs propres"},"type":"lvl3","url":"/appendix-linear-algebra#propri-t-s","position":20},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Propriétés","lvl2":"Valeurs propres et vecteurs propres"},"content":"Pour une matrice symétrique \\mathbf{A} = \\mathbf{A}^\\top:\n\nToutes les valeurs propres sont réelles\n\nLes vecteurs propres associés à des valeurs propres distinctes sont orthogonaux\n\n\\mathbf{A} est diagonalisable: \\mathbf{A} = \\mathbf{V}\\boldsymbol{\\Lambda}\\mathbf{V}^\\top","type":"content","url":"/appendix-linear-algebra#propri-t-s","position":21},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Diagonalisation","lvl2":"Valeurs propres et vecteurs propres"},"type":"lvl3","url":"/appendix-linear-algebra#diagonalisation","position":22},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Diagonalisation","lvl2":"Valeurs propres et vecteurs propres"},"content":"Si \\mathbf{A} a n vecteurs propres linéairement indépendants:\\mathbf{A} = \\mathbf{V}\\boldsymbol{\\Lambda}\\mathbf{V}^{-1}\n\noù:\n\n\\mathbf{V} a les vecteurs propres en colonnes\n\n\\boldsymbol{\\Lambda} = \\text{diag}(\\lambda_1, \\ldots, \\lambda_n)\n\nPour une matrice symétrique: \\mathbf{V}^{-1} = \\mathbf{V}^\\top (matrice orthogonale).","type":"content","url":"/appendix-linear-algebra#diagonalisation","position":23},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl2":"Décompositions matricielles"},"type":"lvl2","url":"/appendix-linear-algebra#d-compositions-matricielles","position":24},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl2":"Décompositions matricielles"},"content":"","type":"content","url":"/appendix-linear-algebra#d-compositions-matricielles","position":25},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Décomposition en valeurs singulières (SVD)","lvl2":"Décompositions matricielles"},"type":"lvl3","url":"/appendix-linear-algebra#d-composition-en-valeurs-singuli-res-svd","position":26},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Décomposition en valeurs singulières (SVD)","lvl2":"Décompositions matricielles"},"content":"Toute matrice \\mathbf{A} \\in \\mathbb{R}^{M \\times N} peut s’écrire:\\mathbf{A} = \\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^\\top\n\noù:\n\n\\mathbf{U} \\in \\mathbb{R}^{M \\times M} est orthogonale (vecteurs singuliers gauches)\n\n\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{M \\times N} est diagonale (valeurs singulières \\sigma_i \\geq 0)\n\n\\mathbf{V} \\in \\mathbb{R}^{N \\times N} est orthogonale (vecteurs singuliers droits)\n\nSVD tronquée: approximation de rang k:\\mathbf{A}_k = \\sum_{i=1}^k \\sigma_i \\boldsymbol{u}_i \\boldsymbol{v}_i^\\top\n\nC’est la meilleure approximation de rang k au sens de la norme de Frobenius.","type":"content","url":"/appendix-linear-algebra#d-composition-en-valeurs-singuli-res-svd","position":27},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Décomposition de Cholesky","lvl2":"Décompositions matricielles"},"type":"lvl3","url":"/appendix-linear-algebra#d-composition-de-cholesky","position":28},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Décomposition de Cholesky","lvl2":"Décompositions matricielles"},"content":"Si \\mathbf{A} est symétrique définie positive:\\mathbf{A} = \\mathbf{L}\\mathbf{L}^\\top\n\noù \\mathbf{L} est triangulaire inférieure avec des éléments diagonaux positifs.\n\nUtile pour:\n\nRésoudre efficacement \\mathbf{A}\\boldsymbol{x} = \\boldsymbol{b}\n\nCalculer \\mathbf{A}^{-1}\n\nÉchantillonner d’une gaussienne multivariée","type":"content","url":"/appendix-linear-algebra#d-composition-de-cholesky","position":29},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Décomposition QR","lvl2":"Décompositions matricielles"},"type":"lvl3","url":"/appendix-linear-algebra#d-composition-qr","position":30},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Décomposition QR","lvl2":"Décompositions matricielles"},"content":"\\mathbf{A} = \\mathbf{Q}\\mathbf{R}\n\noù \\mathbf{Q} est orthogonale et \\mathbf{R} est triangulaire supérieure.\n\nUtile pour résoudre les problèmes de moindres carrés.","type":"content","url":"/appendix-linear-algebra#d-composition-qr","position":31},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl2":"Inverses et pseudo-inverses"},"type":"lvl2","url":"/appendix-linear-algebra#inverses-et-pseudo-inverses","position":32},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl2":"Inverses et pseudo-inverses"},"content":"","type":"content","url":"/appendix-linear-algebra#inverses-et-pseudo-inverses","position":33},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Matrice inverse","lvl2":"Inverses et pseudo-inverses"},"type":"lvl3","url":"/appendix-linear-algebra#matrice-inverse","position":34},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Matrice inverse","lvl2":"Inverses et pseudo-inverses"},"content":"Si \\mathbf{A} est carrée et inversible:\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\n\nPropriétés:\n\n(\\mathbf{AB})^{-1} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}\n\n(\\mathbf{A}^\\top)^{-1} = (\\mathbf{A}^{-1})^\\top","type":"content","url":"/appendix-linear-algebra#matrice-inverse","position":35},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Pseudo-inverse de Moore-Penrose","lvl2":"Inverses et pseudo-inverses"},"type":"lvl3","url":"/appendix-linear-algebra#pseudo-inverse-de-moore-penrose","position":36},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Pseudo-inverse de Moore-Penrose","lvl2":"Inverses et pseudo-inverses"},"content":"Pour toute matrice \\mathbf{A}, la pseudo-inverse \\mathbf{A}^+ satisfait:\\mathbf{A}\\mathbf{A}^+\\mathbf{A} = \\mathbf{A}\n\nSi \\mathbf{A} = \\mathbf{U}\\boldsymbol{\\Sigma}\\mathbf{V}^\\top (SVD):\\mathbf{A}^+ = \\mathbf{V}\\boldsymbol{\\Sigma}^+\\mathbf{U}^\\top\n\noù \\Sigma^+_{ii} = 1/\\sigma_i si \\sigma_i > 0.\n\nApplication: solution des moindres carrés:\\boldsymbol{x}^* = \\mathbf{A}^+ \\boldsymbol{b}","type":"content","url":"/appendix-linear-algebra#pseudo-inverse-de-moore-penrose","position":37},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl2":"Trace et déterminant"},"type":"lvl2","url":"/appendix-linear-algebra#trace-et-d-terminant","position":38},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl2":"Trace et déterminant"},"content":"","type":"content","url":"/appendix-linear-algebra#trace-et-d-terminant","position":39},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Trace","lvl2":"Trace et déterminant"},"type":"lvl3","url":"/appendix-linear-algebra#trace","position":40},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Trace","lvl2":"Trace et déterminant"},"content":"La trace d’une matrice carrée est la somme des éléments diagonaux:\\text{tr}(\\mathbf{A}) = \\sum_i A_{ii}\n\nPropriétés:\n\n\\text{tr}(\\mathbf{A} + \\mathbf{B}) = \\text{tr}(\\mathbf{A}) + \\text{tr}(\\mathbf{B})\n\n\\text{tr}(\\mathbf{AB}) = \\text{tr}(\\mathbf{BA}) (propriété cyclique)\n\n\\text{tr}(\\mathbf{A}) = \\sum_i \\lambda_i (somme des valeurs propres)","type":"content","url":"/appendix-linear-algebra#trace","position":41},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Déterminant","lvl2":"Trace et déterminant"},"type":"lvl3","url":"/appendix-linear-algebra#d-terminant","position":42},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Déterminant","lvl2":"Trace et déterminant"},"content":"Le déterminant mesure le changement de volume d’une transformation linéaire.\n\nPropriétés:\n\n\\det(\\mathbf{AB}) = \\det(\\mathbf{A})\\det(\\mathbf{B})\n\n\\det(\\mathbf{A}^{-1}) = 1/\\det(\\mathbf{A})\n\n\\det(\\mathbf{A}) = \\prod_i \\lambda_i (produit des valeurs propres)\n\n\\mathbf{A} est inversible \\Leftrightarrow \\det(\\mathbf{A}) \\neq 0","type":"content","url":"/appendix-linear-algebra#d-terminant","position":43},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl2":"Dérivées matricielles"},"type":"lvl2","url":"/appendix-linear-algebra#d-riv-es-matricielles","position":44},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl2":"Dérivées matricielles"},"content":"","type":"content","url":"/appendix-linear-algebra#d-riv-es-matricielles","position":45},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Gradient","lvl2":"Dérivées matricielles"},"type":"lvl3","url":"/appendix-linear-algebra#gradient","position":46},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Gradient","lvl2":"Dérivées matricielles"},"content":"Le gradient de f: \\mathbb{R}^D \\to \\mathbb{R} par rapport à \\boldsymbol{x}:\\nabla_{\\boldsymbol{x}} f = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_D} \\end{pmatrix}","type":"content","url":"/appendix-linear-algebra#gradient","position":47},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Jacobienne","lvl2":"Dérivées matricielles"},"type":"lvl3","url":"/appendix-linear-algebra#jacobienne","position":48},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Jacobienne","lvl2":"Dérivées matricielles"},"content":"Pour \\boldsymbol{f}: \\mathbb{R}^N \\to \\mathbb{R}^M, la jacobienne:\\mathbf{J} = \\frac{\\partial \\boldsymbol{f}}{\\partial \\boldsymbol{x}} \\in \\mathbb{R}^{M \\times N}, \\quad J_{ij} = \\frac{\\partial f_i}{\\partial x_j}","type":"content","url":"/appendix-linear-algebra#jacobienne","position":49},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Hessienne","lvl2":"Dérivées matricielles"},"type":"lvl3","url":"/appendix-linear-algebra#hessienne","position":50},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Hessienne","lvl2":"Dérivées matricielles"},"content":"Pour f: \\mathbb{R}^D \\to \\mathbb{R}, la hessienne:\\mathbf{H} = \\nabla^2 f, \\quad H_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j}","type":"content","url":"/appendix-linear-algebra#hessienne","position":51},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Identités utiles","lvl2":"Dérivées matricielles"},"type":"lvl3","url":"/appendix-linear-algebra#identit-s-utiles","position":52},{"hierarchy":{"lvl1":"Annexe: Révision d’algèbre linéaire","lvl3":"Identités utiles","lvl2":"Dérivées matricielles"},"content":"Expression\n\nGradient\n\n\\boldsymbol{a}^\\top \\boldsymbol{x}\n\n\\boldsymbol{a}\n\n\\boldsymbol{x}^\\top \\mathbf{A} \\boldsymbol{x}\n\n(\\mathbf{A} + \\mathbf{A}^\\top)\\boldsymbol{x}\n\n|\\boldsymbol{x}|_2^2\n\n2\\boldsymbol{x}\n\n|\\mathbf{A}\\boldsymbol{x} - \\boldsymbol{b}|_2^2\n\n2\\mathbf{A}^\\top(\\mathbf{A}\\boldsymbol{x} - \\boldsymbol{b})\n\nPour la matrice \\mathbf{X}:\n\nExpression\n\nGradient par rapport à \\mathbf{X}\n\n\\text{tr}(\\mathbf{AX})\n\n\\mathbf{A}^\\top\n\n\\text{tr}(\\mathbf{X}^\\top\\mathbf{A})\n\n\\mathbf{A}\n\n\\log\\det(\\mathbf{X})\n\n\\mathbf{X}^{-\\top}","type":"content","url":"/appendix-linear-algebra#identit-s-utiles","position":53},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation"},"type":"lvl1","url":"/appendix-optimization","position":0},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation"},"content":"Cette annexe révise les concepts d’optimisation utilisés tout au long du livre.","type":"content","url":"/appendix-optimization","position":1},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl2":"Formulation générale"},"type":"lvl2","url":"/appendix-optimization#formulation-g-n-rale","position":2},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl2":"Formulation générale"},"content":"Un problème d’optimisation s’écrit:\\min_{\\boldsymbol{x} \\in \\mathcal{X}} f(\\boldsymbol{x})\n\noù:\n\nf: \\mathbb{R}^D \\to \\mathbb{R} est la fonction objectif\n\n\\mathcal{X} \\subseteq \\mathbb{R}^D est l’ensemble réalisable\n\n\\boldsymbol{x}^* est un minimiseur (ou optimum)\n\nNote: Maximiser f équivaut à minimiser -f.","type":"content","url":"/appendix-optimization#formulation-g-n-rale","position":3},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl2":"Convexité"},"type":"lvl2","url":"/appendix-optimization#convexit","position":4},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl2":"Convexité"},"content":"","type":"content","url":"/appendix-optimization#convexit","position":5},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Ensembles convexes","lvl2":"Convexité"},"type":"lvl3","url":"/appendix-optimization#ensembles-convexes","position":6},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Ensembles convexes","lvl2":"Convexité"},"content":"Un ensemble \\mathcal{C} est convexe si pour tout \\boldsymbol{x}, \\boldsymbol{y} \\in \\mathcal{C} et \\alpha \\in [0, 1]:\\alpha \\boldsymbol{x} + (1-\\alpha)\\boldsymbol{y} \\in \\mathcal{C}\n\nIntuition: le segment entre deux points de l’ensemble reste dans l’ensemble.","type":"content","url":"/appendix-optimization#ensembles-convexes","position":7},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Fonctions convexes","lvl2":"Convexité"},"type":"lvl3","url":"/appendix-optimization#fonctions-convexes","position":8},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Fonctions convexes","lvl2":"Convexité"},"content":"Une fonction f est convexe si son domaine est convexe et:f(\\alpha \\boldsymbol{x} + (1-\\alpha)\\boldsymbol{y}) \\leq \\alpha f(\\boldsymbol{x}) + (1-\\alpha) f(\\boldsymbol{y})\n\npour tout \\alpha \\in [0, 1].\n\nInterprétation géométrique: la fonction est en-dessous de toute corde.","type":"content","url":"/appendix-optimization#fonctions-convexes","position":9},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Caractérisations","lvl2":"Convexité"},"type":"lvl3","url":"/appendix-optimization#caract-risations","position":10},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Caractérisations","lvl2":"Convexité"},"content":"Pour une fonction f deux fois différentiable:\n\nCondition\n\nSignification\n\nf convexe\n\n\\nabla^2 f(\\boldsymbol{x}) \\succeq 0 (Hessienne semi-définie positive)\n\nf strictement convexe\n\n\\nabla^2 f(\\boldsymbol{x}) \\succ 0 (Hessienne définie positive)\n\nf concave\n\n-f convexe","type":"content","url":"/appendix-optimization#caract-risations","position":11},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Exemples de fonctions convexes","lvl2":"Convexité"},"type":"lvl3","url":"/appendix-optimization#exemples-de-fonctions-convexes","position":12},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Exemples de fonctions convexes","lvl2":"Convexité"},"content":"Fonctions affines: f(\\boldsymbol{x}) = \\boldsymbol{a}^\\top \\boldsymbol{x} + b\n\nNormes: f(\\boldsymbol{x}) = \\|\\boldsymbol{x}\\|_p pour p \\geq 1\n\nFormes quadratiques: f(\\boldsymbol{x}) = \\boldsymbol{x}^\\top \\mathbf{A} \\boldsymbol{x} si \\mathbf{A} \\succeq 0\n\nExponentielle: f(x) = e^x\n\nEntropie négative: f(x) = x \\log x pour x > 0","type":"content","url":"/appendix-optimization#exemples-de-fonctions-convexes","position":13},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Propriété fondamentale","lvl2":"Convexité"},"type":"lvl3","url":"/appendix-optimization#propri-t-fondamentale","position":14},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Propriété fondamentale","lvl2":"Convexité"},"content":"Pour une fonction convexe, tout minimum local est un minimum global.\n\nC’est pourquoi les problèmes convexes sont plus faciles à résoudre.","type":"content","url":"/appendix-optimization#propri-t-fondamentale","position":15},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl2":"Conditions d’optimalité"},"type":"lvl2","url":"/appendix-optimization#conditions-doptimalit","position":16},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl2":"Conditions d’optimalité"},"content":"","type":"content","url":"/appendix-optimization#conditions-doptimalit","position":17},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Condition nécessaire du premier ordre","lvl2":"Conditions d’optimalité"},"type":"lvl3","url":"/appendix-optimization#condition-n-cessaire-du-premier-ordre","position":18},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Condition nécessaire du premier ordre","lvl2":"Conditions d’optimalité"},"content":"Si \\boldsymbol{x}^* est un minimum local et f est différentiable:\\nabla f(\\boldsymbol{x}^*) = \\mathbf{0}\n\nUn point où \\nabla f = \\mathbf{0} est appelé point stationnaire.","type":"content","url":"/appendix-optimization#condition-n-cessaire-du-premier-ordre","position":19},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Condition nécessaire du second ordre","lvl2":"Conditions d’optimalité"},"type":"lvl3","url":"/appendix-optimization#condition-n-cessaire-du-second-ordre","position":20},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Condition nécessaire du second ordre","lvl2":"Conditions d’optimalité"},"content":"Si \\boldsymbol{x}^* est un minimum local et f est deux fois différentiable:\\nabla f(\\boldsymbol{x}^*) = \\mathbf{0} \\quad \\text{et} \\quad \\nabla^2 f(\\boldsymbol{x}^*) \\succeq 0","type":"content","url":"/appendix-optimization#condition-n-cessaire-du-second-ordre","position":21},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Condition suffisante du second ordre","lvl2":"Conditions d’optimalité"},"type":"lvl3","url":"/appendix-optimization#condition-suffisante-du-second-ordre","position":22},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Condition suffisante du second ordre","lvl2":"Conditions d’optimalité"},"content":"Si \\nabla f(\\boldsymbol{x}^*) = \\mathbf{0} et \\nabla^2 f(\\boldsymbol{x}^*) \\succ 0, alors \\boldsymbol{x}^* est un minimum local strict.","type":"content","url":"/appendix-optimization#condition-suffisante-du-second-ordre","position":23},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Points-selle","lvl2":"Conditions d’optimalité"},"type":"lvl3","url":"/appendix-optimization#points-selle","position":24},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Points-selle","lvl2":"Conditions d’optimalité"},"content":"Un point-selle satisfait \\nabla f = \\mathbf{0} mais n’est ni un minimum ni un maximum:\n\nMinimum dans certaines directions\n\nMaximum dans d’autres directions\n\nLa Hessienne a des valeurs propres de signes différents.","type":"content","url":"/appendix-optimization#points-selle","position":25},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl2":"Descente de gradient"},"type":"lvl2","url":"/appendix-optimization#descente-de-gradient","position":26},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl2":"Descente de gradient"},"content":"","type":"content","url":"/appendix-optimization#descente-de-gradient","position":27},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Algorithme de base","lvl2":"Descente de gradient"},"type":"lvl3","url":"/appendix-optimization#algorithme-de-base","position":28},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Algorithme de base","lvl2":"Descente de gradient"},"content":"Mise à jour itérative:\\boldsymbol{x}^{(t+1)} = \\boldsymbol{x}^{(t)} - \\eta \\nabla f(\\boldsymbol{x}^{(t)})\n\noù \\eta > 0 est le taux d’apprentissage (learning rate).\n\nIntuition: on se déplace dans la direction de plus forte descente.","type":"content","url":"/appendix-optimization#algorithme-de-base","position":29},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Choix du taux d’apprentissage","lvl2":"Descente de gradient"},"type":"lvl3","url":"/appendix-optimization#choix-du-taux-dapprentissage","position":30},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Choix du taux d’apprentissage","lvl2":"Descente de gradient"},"content":"\\eta trop petit: convergence lente\n\n\\eta trop grand: oscillations, divergence possible\n\nRègle pratique: commencer avec \\eta modéré, observer la perte.","type":"content","url":"/appendix-optimization#choix-du-taux-dapprentissage","position":31},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Convergence","lvl2":"Descente de gradient"},"type":"lvl3","url":"/appendix-optimization#convergence","position":32},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Convergence","lvl2":"Descente de gradient"},"content":"Pour une fonction L-lisse (\\|\\nabla f(\\boldsymbol{x}) - \\nabla f(\\boldsymbol{y})\\| \\leq L\\|\\boldsymbol{x} - \\boldsymbol{y}\\|) et convexe:\n\nAvec \\eta \\leq 1/L: convergence en O(1/T) itérations\n\nSi fortement convexe: convergence linéaire en O(\\kappa \\log(1/\\epsilon))\n\noù \\kappa est le conditionnement (rapport des valeurs propres extrêmes de la Hessienne).","type":"content","url":"/appendix-optimization#convergence","position":33},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl2":"Descente de gradient stochastique (SGD)"},"type":"lvl2","url":"/appendix-optimization#descente-de-gradient-stochastique-sgd","position":34},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl2":"Descente de gradient stochastique (SGD)"},"content":"","type":"content","url":"/appendix-optimization#descente-de-gradient-stochastique-sgd","position":35},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Motivation","lvl2":"Descente de gradient stochastique (SGD)"},"type":"lvl3","url":"/appendix-optimization#motivation","position":36},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Motivation","lvl2":"Descente de gradient stochastique (SGD)"},"content":"Pour une fonction de la forme:f(\\boldsymbol{\\theta}) = \\frac{1}{N} \\sum_{i=1}^N f_i(\\boldsymbol{\\theta})\n\nLe gradient exact coûte O(N) par itération.","type":"content","url":"/appendix-optimization#motivation","position":37},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Algorithme SGD","lvl2":"Descente de gradient stochastique (SGD)"},"type":"lvl3","url":"/appendix-optimization#algorithme-sgd","position":38},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Algorithme SGD","lvl2":"Descente de gradient stochastique (SGD)"},"content":"À chaque itération, on utilise un estimateur non biaisé du gradient:\\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} - \\eta_t \\nabla f_i(\\boldsymbol{\\theta}^{(t)})\n\noù i est tiré uniformément au hasard.\n\nMini-batch SGD: moyenne sur B exemples:\\boldsymbol{\\theta}^{(t+1)} = \\boldsymbol{\\theta}^{(t)} - \\frac{\\eta_t}{B} \\sum_{i \\in \\mathcal{B}} \\nabla f_i(\\boldsymbol{\\theta}^{(t)})","type":"content","url":"/appendix-optimization#algorithme-sgd","position":39},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Variance et convergence","lvl2":"Descente de gradient stochastique (SGD)"},"type":"lvl3","url":"/appendix-optimization#variance-et-convergence","position":40},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Variance et convergence","lvl2":"Descente de gradient stochastique (SGD)"},"content":"SGD introduit du bruit (variance). Pour converger:\n\nLe taux d’apprentissage doit décroître: \\sum_t \\eta_t = \\infty et \\sum_t \\eta_t^2 < \\infty\n\nExemple: \\eta_t = \\eta_0 / \\sqrt{t}","type":"content","url":"/appendix-optimization#variance-et-convergence","position":41},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl2":"Méthodes avec momentum"},"type":"lvl2","url":"/appendix-optimization#m-thodes-avec-momentum","position":42},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl2":"Méthodes avec momentum"},"content":"","type":"content","url":"/appendix-optimization#m-thodes-avec-momentum","position":43},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Momentum classique","lvl2":"Méthodes avec momentum"},"type":"lvl3","url":"/appendix-optimization#momentum-classique","position":44},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Momentum classique","lvl2":"Méthodes avec momentum"},"content":"\\boldsymbol{v}^{(t+1)} = \\beta \\boldsymbol{v}^{(t)} + \\nabla f(\\boldsymbol{x}^{(t)})\\boldsymbol{x}^{(t+1)} = \\boldsymbol{x}^{(t)} - \\eta \\boldsymbol{v}^{(t+1)}\n\noù \\beta \\in [0, 1) est le coefficient de momentum (typiquement 0.9).\n\nAvantages:\n\nAccélère la convergence dans les directions cohérentes\n\nAmortit les oscillations","type":"content","url":"/appendix-optimization#momentum-classique","position":45},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Momentum de Nesterov","lvl2":"Méthodes avec momentum"},"type":"lvl3","url":"/appendix-optimization#momentum-de-nesterov","position":46},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Momentum de Nesterov","lvl2":"Méthodes avec momentum"},"content":"\\boldsymbol{v}^{(t+1)} = \\beta \\boldsymbol{v}^{(t)} + \\nabla f(\\boldsymbol{x}^{(t)} - \\eta \\beta \\boldsymbol{v}^{(t)})\\boldsymbol{x}^{(t+1)} = \\boldsymbol{x}^{(t)} - \\eta \\boldsymbol{v}^{(t+1)}\n\nIdée: on regarde le gradient “en avance”.","type":"content","url":"/appendix-optimization#momentum-de-nesterov","position":47},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl2":"Méthodes adaptatives"},"type":"lvl2","url":"/appendix-optimization#m-thodes-adaptatives","position":48},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl2":"Méthodes adaptatives"},"content":"","type":"content","url":"/appendix-optimization#m-thodes-adaptatives","position":49},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"AdaGrad","lvl2":"Méthodes adaptatives"},"type":"lvl3","url":"/appendix-optimization#adagrad","position":50},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"AdaGrad","lvl2":"Méthodes adaptatives"},"content":"Adapte le taux d’apprentissage par coordonnée:g_i^{(t)} = \\nabla_i f(\\boldsymbol{x}^{(t)})\n\nG_{ii}^{(t)} = G_{ii}^{(t-1)} + (g_i^{(t)})^2\n\nx_i^{(t+1)} = x_i^{(t)} - \\frac{\\eta}{\\sqrt{G_{ii}^{(t)} + \\epsilon}} g_i^{(t)}\n\nProblème: le taux effectif décroît trop vite.","type":"content","url":"/appendix-optimization#adagrad","position":51},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"RMSProp","lvl2":"Méthodes adaptatives"},"type":"lvl3","url":"/appendix-optimization#rmsprop","position":52},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"RMSProp","lvl2":"Méthodes adaptatives"},"content":"Utilise une moyenne mobile exponentielle:v_i^{(t)} = \\gamma v_i^{(t-1)} + (1-\\gamma)(g_i^{(t)})^2\n\nx_i^{(t+1)} = x_i^{(t)} - \\frac{\\eta}{\\sqrt{v_i^{(t)} + \\epsilon}} g_i^{(t)}","type":"content","url":"/appendix-optimization#rmsprop","position":53},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Adam","lvl2":"Méthodes adaptatives"},"type":"lvl3","url":"/appendix-optimization#adam","position":54},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Adam","lvl2":"Méthodes adaptatives"},"content":"Combine momentum et adaptation:m^{(t)} = \\beta_1 m^{(t-1)} + (1-\\beta_1) g^{(t)} \\quad \\text{(premier moment)}\n\nv^{(t)} = \\beta_2 v^{(t-1)} + (1-\\beta_2) (g^{(t)})^2 \\quad \\text{(second moment)}\n\n\\hat{m}^{(t)} = \\frac{m^{(t)}}{1-\\beta_1^t}, \\quad \\hat{v}^{(t)} = \\frac{v^{(t)}}{1-\\beta_2^t} \\quad \\text{(correction de biais)}\n\nx^{(t+1)} = x^{(t)} - \\frac{\\eta}{\\sqrt{\\hat{v}^{(t)}} + \\epsilon} \\hat{m}^{(t)}\n\nHyperparamètres par défaut: \\beta_1 = 0.9, \\beta_2 = 0.999, \\epsilon = 10^{-8}.","type":"content","url":"/appendix-optimization#adam","position":55},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl2":"Optimisation sous contraintes"},"type":"lvl2","url":"/appendix-optimization#optimisation-sous-contraintes","position":56},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl2":"Optimisation sous contraintes"},"content":"","type":"content","url":"/appendix-optimization#optimisation-sous-contraintes","position":57},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Formulation","lvl2":"Optimisation sous contraintes"},"type":"lvl3","url":"/appendix-optimization#formulation","position":58},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Formulation","lvl2":"Optimisation sous contraintes"},"content":"\\min_{\\boldsymbol{x}} f(\\boldsymbol{x}) \\quad \\text{s.t.} \\quad g_i(\\boldsymbol{x}) \\leq 0, \\quad h_j(\\boldsymbol{x}) = 0\n\noù:\n\ng_i: contraintes d’inégalité\n\nh_j: contraintes d’égalité","type":"content","url":"/appendix-optimization#formulation","position":59},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Multiplicateurs de Lagrange (contraintes d’égalité)","lvl2":"Optimisation sous contraintes"},"type":"lvl3","url":"/appendix-optimization#multiplicateurs-de-lagrange-contraintes-d-galit","position":60},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Multiplicateurs de Lagrange (contraintes d’égalité)","lvl2":"Optimisation sous contraintes"},"content":"Pour le problème \\min f(\\boldsymbol{x}) s.t. h(\\boldsymbol{x}) = 0:\n\nLe Lagrangien:\\mathcal{L}(\\boldsymbol{x}, \\boldsymbol{\\lambda}) = f(\\boldsymbol{x}) + \\boldsymbol{\\lambda}^\\top h(\\boldsymbol{x})\n\nConditions d’optimalité:\\nabla_{\\boldsymbol{x}} \\mathcal{L} = \\nabla f(\\boldsymbol{x}) + \\sum_j \\lambda_j \\nabla h_j(\\boldsymbol{x}) = \\mathbf{0}\n\n\\nabla_{\\boldsymbol{\\lambda}} \\mathcal{L} = h(\\boldsymbol{x}) = \\mathbf{0}\n\nInterprétation: au point optimal, \\nabla f est une combinaison linéaire des \\nabla h_j.","type":"content","url":"/appendix-optimization#multiplicateurs-de-lagrange-contraintes-d-galit","position":61},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Conditions KKT (Karush-Kuhn-Tucker)","lvl2":"Optimisation sous contraintes"},"type":"lvl3","url":"/appendix-optimization#conditions-kkt-karush-kuhn-tucker","position":62},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Conditions KKT (Karush-Kuhn-Tucker)","lvl2":"Optimisation sous contraintes"},"content":"Pour le problème général avec inégalités:\n\nLe Lagrangien:\\mathcal{L}(\\boldsymbol{x}, \\boldsymbol{\\mu}, \\boldsymbol{\\lambda}) = f(\\boldsymbol{x}) + \\sum_i \\mu_i g_i(\\boldsymbol{x}) + \\sum_j \\lambda_j h_j(\\boldsymbol{x})\n\nConditions KKT (nécessaires pour l’optimalité):\n\nStationnarité: \\nabla_{\\boldsymbol{x}} \\mathcal{L} = \\mathbf{0}\n\nRéalisabilité primale: g_i(\\boldsymbol{x}^*) \\leq 0, h_j(\\boldsymbol{x}^*) = 0\n\nRéalisabilité duale: \\mu_i \\geq 0\n\nComplémentarité: \\mu_i g_i(\\boldsymbol{x}^*) = 0\n\nLa condition de complémentarité signifie:\n\nSi g_i(\\boldsymbol{x}^*) < 0 (contrainte inactive): \\mu_i = 0\n\nSi \\mu_i > 0: g_i(\\boldsymbol{x}^*) = 0 (contrainte active)","type":"content","url":"/appendix-optimization#conditions-kkt-karush-kuhn-tucker","position":63},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Dualité","lvl2":"Optimisation sous contraintes"},"type":"lvl3","url":"/appendix-optimization#dualit","position":64},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Dualité","lvl2":"Optimisation sous contraintes"},"content":"Le problème primal:p^* = \\min_{\\boldsymbol{x}} \\max_{\\boldsymbol{\\mu} \\geq 0, \\boldsymbol{\\lambda}} \\mathcal{L}(\\boldsymbol{x}, \\boldsymbol{\\mu}, \\boldsymbol{\\lambda})\n\nLe problème dual:d^* = \\max_{\\boldsymbol{\\mu} \\geq 0, \\boldsymbol{\\lambda}} \\min_{\\boldsymbol{x}} \\mathcal{L}(\\boldsymbol{x}, \\boldsymbol{\\mu}, \\boldsymbol{\\lambda})\n\nDualité faible: d^* \\leq p^* (toujours vraie)\n\nDualité forte: d^* = p^* (vraie pour les problèmes convexes satisfaisant une condition de régularité)","type":"content","url":"/appendix-optimization#dualit","position":65},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Application aux SVM","lvl2":"Optimisation sous contraintes"},"type":"lvl3","url":"/appendix-optimization#application-aux-svm","position":66},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Application aux SVM","lvl2":"Optimisation sous contraintes"},"content":"Le problème SVM à marge souple:\\min_{\\boldsymbol{w}, b, \\boldsymbol{\\xi}} \\frac{1}{2}\\|\\boldsymbol{w}\\|^2 + C\\sum_i \\xi_i\n\n\\text{s.t.} \\quad y_i(\\boldsymbol{w}^\\top \\boldsymbol{x}_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0\n\nPar la dualité de Lagrange, on obtient le problème dual (plus facile à résoudre avec l’astuce du noyau).","type":"content","url":"/appendix-optimization#application-aux-svm","position":67},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl2":"Méthodes de Newton"},"type":"lvl2","url":"/appendix-optimization#m-thodes-de-newton","position":68},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl2":"Méthodes de Newton"},"content":"","type":"content","url":"/appendix-optimization#m-thodes-de-newton","position":69},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Newton classique","lvl2":"Méthodes de Newton"},"type":"lvl3","url":"/appendix-optimization#newton-classique","position":70},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Newton classique","lvl2":"Méthodes de Newton"},"content":"Utilise l’information de second ordre:\\boldsymbol{x}^{(t+1)} = \\boldsymbol{x}^{(t)} - [\\nabla^2 f(\\boldsymbol{x}^{(t)})]^{-1} \\nabla f(\\boldsymbol{x}^{(t)})\n\nAvantages:\n\nConvergence quadratique près de l’optimum\n\nInvariant aux transformations affines\n\nInconvénients:\n\nCoût O(D^3) pour l’inversion\n\nPeut diverger si la Hessienne n’est pas définie positive","type":"content","url":"/appendix-optimization#newton-classique","position":71},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Quasi-Newton (BFGS, L-BFGS)","lvl2":"Méthodes de Newton"},"type":"lvl3","url":"/appendix-optimization#quasi-newton-bfgs-l-bfgs","position":72},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl3":"Quasi-Newton (BFGS, L-BFGS)","lvl2":"Méthodes de Newton"},"content":"Approxime la Hessienne sans la calculer explicitement.\n\nL-BFGS (Limited-memory BFGS):\n\nStocke seulement les m dernières différences de gradient\n\nCoût O(mD) par itération\n\nTrès utilisé pour l’optimisation de grande dimension","type":"content","url":"/appendix-optimization#quasi-newton-bfgs-l-bfgs","position":73},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl2":"Résumé des méthodes"},"type":"lvl2","url":"/appendix-optimization#r-sum-des-m-thodes","position":74},{"hierarchy":{"lvl1":"Annexe: Révision d’optimisation","lvl2":"Résumé des méthodes"},"content":"Méthode\n\nCoût/itération\n\nConvergence\n\nUsage\n\nGradient\n\nO(D)\n\nSous-linéaire\n\nGrande dimension\n\nSGD\n\nO(D)\n\nBruitée\n\nTrès grands datasets\n\nAdam\n\nO(D)\n\nAdaptative\n\nDeep learning\n\nNewton\n\nO(D^3)\n\nQuadratique\n\nPetite dimension\n\nL-BFGS\n\nO(mD)\n\nSuper-linéaire\n\nDimension moyenne","type":"content","url":"/appendix-optimization#r-sum-des-m-thodes","position":75},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités"},"type":"lvl1","url":"/appendix-probability","position":0},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités"},"content":"Cette annexe révise les concepts de probabilité utilisés tout au long du livre.","type":"content","url":"/appendix-probability","position":1},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl2":"Variables aléatoires"},"type":"lvl2","url":"/appendix-probability#variables-al-atoires","position":2},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl2":"Variables aléatoires"},"content":"","type":"content","url":"/appendix-probability#variables-al-atoires","position":3},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Définitions","lvl2":"Variables aléatoires"},"type":"lvl3","url":"/appendix-probability#d-finitions","position":4},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Définitions","lvl2":"Variables aléatoires"},"content":"Une variable aléatoire est une fonction qui associe un résultat d’une expérience aléatoire à un nombre réel.\n\nVariable discrète: prend un nombre fini ou dénombrable de valeurs\n\nVariable continue: prend des valeurs dans un intervalle de \\mathbb{R}","type":"content","url":"/appendix-probability#d-finitions","position":5},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Fonction de masse de probabilité (PMF)","lvl2":"Variables aléatoires"},"type":"lvl3","url":"/appendix-probability#fonction-de-masse-de-probabilit-pmf","position":6},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Fonction de masse de probabilité (PMF)","lvl2":"Variables aléatoires"},"content":"Pour une variable discrète X, la PMF est:p(x) = P(X = x)\n\nPropriétés:\n\np(x) \\geq 0 pour tout x\n\n\\sum_x p(x) = 1","type":"content","url":"/appendix-probability#fonction-de-masse-de-probabilit-pmf","position":7},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Fonction de densité de probabilité (PDF)","lvl2":"Variables aléatoires"},"type":"lvl3","url":"/appendix-probability#fonction-de-densit-de-probabilit-pdf","position":8},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Fonction de densité de probabilité (PDF)","lvl2":"Variables aléatoires"},"content":"Pour une variable continue X, la PDF f(x) satisfait:P(a \\leq X \\leq b) = \\int_a^b f(x) \\, dx\n\nPropriétés:\n\nf(x) \\geq 0 pour tout x\n\n\\int_{-\\infty}^{\\infty} f(x) \\, dx = 1\n\nNote: f(x) peut être > 1; c’est une densité, pas une probabilité.","type":"content","url":"/appendix-probability#fonction-de-densit-de-probabilit-pdf","position":9},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl2":"Distributions courantes"},"type":"lvl2","url":"/appendix-probability#distributions-courantes","position":10},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl2":"Distributions courantes"},"content":"","type":"content","url":"/appendix-probability#distributions-courantes","position":11},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Distribution de Bernoulli","lvl2":"Distributions courantes"},"type":"lvl3","url":"/appendix-probability#distribution-de-bernoulli","position":12},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Distribution de Bernoulli","lvl2":"Distributions courantes"},"content":"Variable binaire X \\in \\{0, 1\\}:p(x \\mid \\theta) = \\theta^x (1-\\theta)^{1-x}\n\nParamètre: \\theta \\in [0, 1] (probabilité de succès)\n\nEspérance: \\mathbb{E}[X] = \\theta\n\nVariance: \\text{Var}(X) = \\theta(1-\\theta)","type":"content","url":"/appendix-probability#distribution-de-bernoulli","position":13},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Distribution catégorique (multinoulli)","lvl2":"Distributions courantes"},"type":"lvl3","url":"/appendix-probability#distribution-cat-gorique-multinoulli","position":14},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Distribution catégorique (multinoulli)","lvl2":"Distributions courantes"},"content":"Variable à K catégories X \\in \\{1, \\ldots, K\\}:p(x = k \\mid \\boldsymbol{\\theta}) = \\theta_k\n\noù \\boldsymbol{\\theta} = (\\theta_1, \\ldots, \\theta_K) avec \\sum_k \\theta_k = 1.\n\nAvec le codage one-hot \\boldsymbol{x} \\in \\{0,1\\}^K:p(\\boldsymbol{x} \\mid \\boldsymbol{\\theta}) = \\prod_{k=1}^K \\theta_k^{x_k}","type":"content","url":"/appendix-probability#distribution-cat-gorique-multinoulli","position":15},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Distribution binomiale","lvl2":"Distributions courantes"},"type":"lvl3","url":"/appendix-probability#distribution-binomiale","position":16},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Distribution binomiale","lvl2":"Distributions courantes"},"content":"Nombre de succès en n essais de Bernoulli:p(k \\mid n, \\theta) = \\binom{n}{k} \\theta^k (1-\\theta)^{n-k}\n\nEspérance: \\mathbb{E}[X] = n\\theta\n\nVariance: \\text{Var}(X) = n\\theta(1-\\theta)","type":"content","url":"/appendix-probability#distribution-binomiale","position":17},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Distribution de Poisson","lvl2":"Distributions courantes"},"type":"lvl3","url":"/appendix-probability#distribution-de-poisson","position":18},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Distribution de Poisson","lvl2":"Distributions courantes"},"content":"Nombre d’événements dans un intervalle:p(k \\mid \\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}\n\nEspérance: \\mathbb{E}[X] = \\lambda\n\nVariance: \\text{Var}(X) = \\lambda","type":"content","url":"/appendix-probability#distribution-de-poisson","position":19},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Distribution gaussienne (normale)","lvl2":"Distributions courantes"},"type":"lvl3","url":"/appendix-probability#distribution-gaussienne-normale","position":20},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Distribution gaussienne (normale)","lvl2":"Distributions courantes"},"content":"Univariée:\\mathcal{N}(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n\nEspérance: \\mathbb{E}[X] = \\mu\n\nVariance: \\text{Var}(X) = \\sigma^2\n\nMultivariée:\\mathcal{N}(\\boldsymbol{x} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{D/2}|\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\boldsymbol{x}-\\boldsymbol{\\mu})^\\top \\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{x}-\\boldsymbol{\\mu})\\right)\n\noù:\n\n\\boldsymbol{\\mu} \\in \\mathbb{R}^D est le vecteur moyenne\n\n\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{D \\times D} est la matrice de covariance (symétrique, définie positive)","type":"content","url":"/appendix-probability#distribution-gaussienne-normale","position":21},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Distribution uniforme","lvl2":"Distributions courantes"},"type":"lvl3","url":"/appendix-probability#distribution-uniforme","position":22},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Distribution uniforme","lvl2":"Distributions courantes"},"content":"Continue sur [a, b]:p(x) = \\frac{1}{b-a} \\mathbb{I}(a \\leq x \\leq b)","type":"content","url":"/appendix-probability#distribution-uniforme","position":23},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Distribution exponentielle","lvl2":"Distributions courantes"},"type":"lvl3","url":"/appendix-probability#distribution-exponentielle","position":24},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Distribution exponentielle","lvl2":"Distributions courantes"},"content":"p(x \\mid \\lambda) = \\lambda e^{-\\lambda x} \\mathbb{I}(x \\geq 0)","type":"content","url":"/appendix-probability#distribution-exponentielle","position":25},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Distribution Beta","lvl2":"Distributions courantes"},"type":"lvl3","url":"/appendix-probability#distribution-beta","position":26},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Distribution Beta","lvl2":"Distributions courantes"},"content":"Pour x \\in [0, 1]:\\text{Beta}(x \\mid \\alpha, \\beta) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha, \\beta)}\n\noù B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}.\n\nUtile comme a priori pour les paramètres de Bernoulli (conjugué).","type":"content","url":"/appendix-probability#distribution-beta","position":27},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Distribution de Dirichlet","lvl2":"Distributions courantes"},"type":"lvl3","url":"/appendix-probability#distribution-de-dirichlet","position":28},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Distribution de Dirichlet","lvl2":"Distributions courantes"},"content":"Généralisation de Beta à K dimensions:\\text{Dir}(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\alpha}) = \\frac{1}{B(\\boldsymbol{\\alpha})} \\prod_{k=1}^K \\theta_k^{\\alpha_k - 1}\n\nUtile comme a priori pour les paramètres catégoriques (conjugué).","type":"content","url":"/appendix-probability#distribution-de-dirichlet","position":29},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl2":"Espérance et variance"},"type":"lvl2","url":"/appendix-probability#esp-rance-et-variance","position":30},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl2":"Espérance et variance"},"content":"","type":"content","url":"/appendix-probability#esp-rance-et-variance","position":31},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Espérance","lvl2":"Espérance et variance"},"type":"lvl3","url":"/appendix-probability#esp-rance","position":32},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Espérance","lvl2":"Espérance et variance"},"content":"L’espérance (ou moyenne) d’une variable aléatoire:\n\nDiscrète: \\mathbb{E}[X] = \\sum_x x \\, p(x)\n\nContinue: \\mathbb{E}[X] = \\int x \\, f(x) \\, dx\n\nPropriétés:\n\nLinéarité: \\mathbb{E}[aX + bY] = a\\mathbb{E}[X] + b\\mathbb{E}[Y]\n\n\\mathbb{E}[g(X)] = \\sum_x g(x) p(x) (ou intégrale pour le continu)","type":"content","url":"/appendix-probability#esp-rance","position":33},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Variance","lvl2":"Espérance et variance"},"type":"lvl3","url":"/appendix-probability#variance","position":34},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Variance","lvl2":"Espérance et variance"},"content":"La variance mesure la dispersion autour de la moyenne:\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2] = \\mathbb{E}[X^2] - \\mathbb{E}[X]^2\n\nL’écart-type: \\sigma = \\sqrt{\\text{Var}(X)}\n\nPropriétés:\n\n\\text{Var}(aX + b) = a^2 \\text{Var}(X)\n\nSi X \\perp Y: \\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)","type":"content","url":"/appendix-probability#variance","position":35},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Covariance","lvl2":"Espérance et variance"},"type":"lvl3","url":"/appendix-probability#covariance","position":36},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Covariance","lvl2":"Espérance et variance"},"content":"\\text{Cov}(X, Y) = \\mathbb{E}[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])] = \\mathbb{E}[XY] - \\mathbb{E}[X]\\mathbb{E}[Y]\n\nCorrélation (covariance normalisée):\\rho(X, Y) = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y} \\in [-1, 1]\n\nMatrice de covariance pour un vecteur aléatoire \\boldsymbol{X}:\\boldsymbol{\\Sigma} = \\mathbb{E}[(\\boldsymbol{X} - \\boldsymbol{\\mu})(\\boldsymbol{X} - \\boldsymbol{\\mu})^\\top]","type":"content","url":"/appendix-probability#covariance","position":37},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl2":"Probabilités jointes et marginales"},"type":"lvl2","url":"/appendix-probability#probabilit-s-jointes-et-marginales","position":38},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl2":"Probabilités jointes et marginales"},"content":"","type":"content","url":"/appendix-probability#probabilit-s-jointes-et-marginales","position":39},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Règle de la somme (marginalisation)","lvl2":"Probabilités jointes et marginales"},"type":"lvl3","url":"/appendix-probability#r-gle-de-la-somme-marginalisation","position":40},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Règle de la somme (marginalisation)","lvl2":"Probabilités jointes et marginales"},"content":"p(X) = \\sum_Y p(X, Y) \\quad \\text{ou} \\quad p(x) = \\int p(x, y) \\, dy","type":"content","url":"/appendix-probability#r-gle-de-la-somme-marginalisation","position":41},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Règle du produit","lvl2":"Probabilités jointes et marginales"},"type":"lvl3","url":"/appendix-probability#r-gle-du-produit","position":42},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Règle du produit","lvl2":"Probabilités jointes et marginales"},"content":"p(X, Y) = p(X \\mid Y) p(Y) = p(Y \\mid X) p(X)","type":"content","url":"/appendix-probability#r-gle-du-produit","position":43},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Règle de la chaîne","lvl2":"Probabilités jointes et marginales"},"type":"lvl3","url":"/appendix-probability#r-gle-de-la-cha-ne","position":44},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Règle de la chaîne","lvl2":"Probabilités jointes et marginales"},"content":"p(X_1, \\ldots, X_n) = \\prod_{i=1}^n p(X_i \\mid X_1, \\ldots, X_{i-1})","type":"content","url":"/appendix-probability#r-gle-de-la-cha-ne","position":45},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl2":"Théorème de Bayes"},"type":"lvl2","url":"/appendix-probability#th-or-me-de-bayes","position":46},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl2":"Théorème de Bayes"},"content":"","type":"content","url":"/appendix-probability#th-or-me-de-bayes","position":47},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Formulation","lvl2":"Théorème de Bayes"},"type":"lvl3","url":"/appendix-probability#formulation","position":48},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Formulation","lvl2":"Théorème de Bayes"},"content":"p(\\theta \\mid \\mathcal{D}) = \\frac{p(\\mathcal{D} \\mid \\theta) p(\\theta)}{p(\\mathcal{D})}\n\noù:\n\np(\\theta): a priori (croyance avant les données)\n\np(\\mathcal{D} \\mid \\theta): vraisemblance (probabilité des données étant donné \\theta)\n\np(\\theta \\mid \\mathcal{D}): a posteriori (croyance après avoir vu les données)\n\np(\\mathcal{D}): évidence ou constante de normalisation","type":"content","url":"/appendix-probability#formulation","position":49},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Constante de normalisation","lvl2":"Théorème de Bayes"},"type":"lvl3","url":"/appendix-probability#constante-de-normalisation","position":50},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Constante de normalisation","lvl2":"Théorème de Bayes"},"content":"p(\\mathcal{D}) = \\int p(\\mathcal{D} \\mid \\theta) p(\\theta) \\, d\\theta\n\nCette intégrale est souvent difficile à calculer (inférence approximative).","type":"content","url":"/appendix-probability#constante-de-normalisation","position":51},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Forme proportionnelle","lvl2":"Théorème de Bayes"},"type":"lvl3","url":"/appendix-probability#forme-proportionnelle","position":52},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Forme proportionnelle","lvl2":"Théorème de Bayes"},"content":"Souvent, on écrit:p(\\theta \\mid \\mathcal{D}) \\propto p(\\mathcal{D} \\mid \\theta) p(\\theta)\n\n(a posteriori proportionnel à vraisemblance × a priori)","type":"content","url":"/appendix-probability#forme-proportionnelle","position":53},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl2":"Indépendance"},"type":"lvl2","url":"/appendix-probability#ind-pendance","position":54},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl2":"Indépendance"},"content":"","type":"content","url":"/appendix-probability#ind-pendance","position":55},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Indépendance marginale","lvl2":"Indépendance"},"type":"lvl3","url":"/appendix-probability#ind-pendance-marginale","position":56},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Indépendance marginale","lvl2":"Indépendance"},"content":"X \\perp Y si:p(X, Y) = p(X) p(Y)\n\nÉquivalent à: p(X \\mid Y) = p(X)","type":"content","url":"/appendix-probability#ind-pendance-marginale","position":57},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Indépendance conditionnelle","lvl2":"Indépendance"},"type":"lvl3","url":"/appendix-probability#ind-pendance-conditionnelle","position":58},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Indépendance conditionnelle","lvl2":"Indépendance"},"content":"X \\perp Y \\mid Z si:p(X, Y \\mid Z) = p(X \\mid Z) p(Y \\mid Z)\n\nAttention: X \\perp Y n’implique pas X \\perp Y \\mid Z (et vice versa).","type":"content","url":"/appendix-probability#ind-pendance-conditionnelle","position":59},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl2":"Divergence de Kullback-Leibler"},"type":"lvl2","url":"/appendix-probability#divergence-de-kullback-leibler","position":60},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl2":"Divergence de Kullback-Leibler"},"content":"","type":"content","url":"/appendix-probability#divergence-de-kullback-leibler","position":61},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Définition","lvl2":"Divergence de Kullback-Leibler"},"type":"lvl3","url":"/appendix-probability#d-finition","position":62},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Définition","lvl2":"Divergence de Kullback-Leibler"},"content":"La divergence KL mesure la “distance” entre deux distributions:D_{\\text{KL}}(p \\| q) = \\mathbb{E}_p\\left[\\log \\frac{p(x)}{q(x)}\\right] = \\sum_x p(x) \\log \\frac{p(x)}{q(x)}","type":"content","url":"/appendix-probability#d-finition","position":63},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Propriétés","lvl2":"Divergence de Kullback-Leibler"},"type":"lvl3","url":"/appendix-probability#propri-t-s","position":64},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Propriétés","lvl2":"Divergence de Kullback-Leibler"},"content":"D_{\\text{KL}}(p \\| q) \\geq 0 (inégalité de Gibbs)\n\nD_{\\text{KL}}(p \\| q) = 0 ssi p = q\n\nNon symétrique: D_{\\text{KL}}(p \\| q) \\neq D_{\\text{KL}}(q \\| p) en général\n\nCe n’est pas une distance (ne satisfait pas l’inégalité triangulaire)","type":"content","url":"/appendix-probability#propri-t-s","position":65},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Pour les gaussiennes","lvl2":"Divergence de Kullback-Leibler"},"type":"lvl3","url":"/appendix-probability#pour-les-gaussiennes","position":66},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Pour les gaussiennes","lvl2":"Divergence de Kullback-Leibler"},"content":"Pour p = \\mathcal{N}(\\boldsymbol{\\mu}_1, \\boldsymbol{\\Sigma}_1) et q = \\mathcal{N}(\\boldsymbol{\\mu}_2, \\boldsymbol{\\Sigma}_2):D_{\\text{KL}}(p \\| q) = \\frac{1}{2}\\left[\\log\\frac{|\\boldsymbol{\\Sigma}_2|}{|\\boldsymbol{\\Sigma}_1|} - D + \\text{tr}(\\boldsymbol{\\Sigma}_2^{-1}\\boldsymbol{\\Sigma}_1) + (\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_1)^\\top \\boldsymbol{\\Sigma}_2^{-1}(\\boldsymbol{\\mu}_2 - \\boldsymbol{\\mu}_1)\\right]","type":"content","url":"/appendix-probability#pour-les-gaussiennes","position":67},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl2":"Entropie"},"type":"lvl2","url":"/appendix-probability#entropie","position":68},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl2":"Entropie"},"content":"","type":"content","url":"/appendix-probability#entropie","position":69},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Entropie de Shannon","lvl2":"Entropie"},"type":"lvl3","url":"/appendix-probability#entropie-de-shannon","position":70},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Entropie de Shannon","lvl2":"Entropie"},"content":"L’entropie mesure l’incertitude d’une distribution:\\mathbb{H}(X) = -\\mathbb{E}[\\log p(X)] = -\\sum_x p(x) \\log p(x)\n\nMaximale pour une distribution uniforme\n\nMinimale (= 0) pour une distribution déterministe","type":"content","url":"/appendix-probability#entropie-de-shannon","position":71},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Entropie croisée","lvl2":"Entropie"},"type":"lvl3","url":"/appendix-probability#entropie-crois-e","position":72},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Entropie croisée","lvl2":"Entropie"},"content":"\\mathbb{H}(p, q) = -\\mathbb{E}_p[\\log q(X)] = -\\sum_x p(x) \\log q(x)\n\nRelation avec KL:\\mathbb{H}(p, q) = \\mathbb{H}(p) + D_{\\text{KL}}(p \\| q)","type":"content","url":"/appendix-probability#entropie-crois-e","position":73},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl2":"Inégalités utiles"},"type":"lvl2","url":"/appendix-probability#in-galit-s-utiles","position":74},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl2":"Inégalités utiles"},"content":"","type":"content","url":"/appendix-probability#in-galit-s-utiles","position":75},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Inégalité de Jensen","lvl2":"Inégalités utiles"},"type":"lvl3","url":"/appendix-probability#in-galit-de-jensen","position":76},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Inégalité de Jensen","lvl2":"Inégalités utiles"},"content":"Pour une fonction convexe \\phi:\\phi(\\mathbb{E}[X]) \\leq \\mathbb{E}[\\phi(X)]\n\nPour une fonction concave: inégalité inversée.","type":"content","url":"/appendix-probability#in-galit-de-jensen","position":77},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Inégalité de Markov","lvl2":"Inégalités utiles"},"type":"lvl3","url":"/appendix-probability#in-galit-de-markov","position":78},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Inégalité de Markov","lvl2":"Inégalités utiles"},"content":"Pour X \\geq 0 et a > 0:P(X \\geq a) \\leq \\frac{\\mathbb{E}[X]}{a}","type":"content","url":"/appendix-probability#in-galit-de-markov","position":79},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Inégalité de Chebyshev","lvl2":"Inégalités utiles"},"type":"lvl3","url":"/appendix-probability#in-galit-de-chebyshev","position":80},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Inégalité de Chebyshev","lvl2":"Inégalités utiles"},"content":"P(|X - \\mu| \\geq k\\sigma) \\leq \\frac{1}{k^2}","type":"content","url":"/appendix-probability#in-galit-de-chebyshev","position":81},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Inégalité de Hoeffding","lvl2":"Inégalités utiles"},"type":"lvl3","url":"/appendix-probability#in-galit-de-hoeffding","position":82},{"hierarchy":{"lvl1":"Annexe: Révision de probabilités","lvl3":"Inégalité de Hoeffding","lvl2":"Inégalités utiles"},"content":"Pour des variables X_1, \\ldots, X_n indépendantes bornées dans [a_i, b_i]:P\\left(\\left|\\frac{1}{n}\\sum_{i=1}^n X_i - \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^n X_i\\right]\\right| \\geq t\\right) \\leq 2\\exp\\left(-\\frac{2n^2t^2}{\\sum_{i=1}^n(b_i - a_i)^2}\\right)","type":"content","url":"/appendix-probability#in-galit-de-hoeffding","position":83},{"hierarchy":{"lvl1":"Boosting"},"type":"lvl1","url":"/boosting","position":0},{"hierarchy":{"lvl1":"Boosting"},"content":"Objectifs d’apprentissage\n\nÀ la fin de ce chapitre, vous serez en mesure de:\n\nExpliquer le principe du boosting et de la modélisation additive\n\nDériver l’algorithme AdaBoost à partir de la perte exponentielle\n\nComprendre le gradient boosting comme descente de gradient fonctionnelle\n\nImplémenter le gradient tree boosting\n\nAnalyser les fonctions de perte appropriées pour le boosting\n\nComparer le boosting au bagging en termes de biais et variance","type":"content","url":"/boosting","position":1},{"hierarchy":{"lvl1":"Boosting","lvl2":"Modèles additifs"},"type":"lvl2","url":"/boosting#mod-les-additifs","position":2},{"hierarchy":{"lvl1":"Boosting","lvl2":"Modèles additifs"},"content":"","type":"content","url":"/boosting#mod-les-additifs","position":3},{"hierarchy":{"lvl1":"Boosting","lvl3":"Structure générale","lvl2":"Modèles additifs"},"type":"lvl3","url":"/boosting#structure-g-n-rale","position":4},{"hierarchy":{"lvl1":"Boosting","lvl3":"Structure générale","lvl2":"Modèles additifs"},"content":"Les ensembles d’arbres (et plus généralement, les méthodes d’ensemble) correspondent à un modèle de la forme:f(\\boldsymbol{x}; \\boldsymbol{\\theta}) = \\sum_{m=1}^M \\beta_m F_m(\\boldsymbol{x}; \\boldsymbol{\\theta}_m)\n\noù F_m est le m-ième modèle de base et \\beta_m est son poids. Ce type de modèle est appelé un modèle additif.\n\nDans le bagging, nous avons \\beta_m = 1/M et les modèles sont entraînés indépendamment. Le boosting adopte une approche différente: les modèles sont entraînés séquentiellement, chaque nouveau modèle corrigeant les erreurs des précédents.","type":"content","url":"/boosting#structure-g-n-rale","position":5},{"hierarchy":{"lvl1":"Boosting","lvl3":"Objectif d’optimisation","lvl2":"Modèles additifs"},"type":"lvl3","url":"/boosting#objectif-doptimisation","position":6},{"hierarchy":{"lvl1":"Boosting","lvl3":"Objectif d’optimisation","lvl2":"Modèles additifs"},"content":"Nous cherchons à minimiser:\\mathcal{L}(f) = \\sum_{i=1}^N \\ell(y_i, f(\\boldsymbol{x}_i))\n\noù \\ell est une fonction de perte. Le défi est que l’espace des fonctions est de dimension infinie.","type":"content","url":"/boosting#objectif-doptimisation","position":7},{"hierarchy":{"lvl1":"Boosting","lvl2":"Modélisation additive itérative"},"type":"lvl2","url":"/boosting#mod-lisation-additive-it-rative","position":8},{"hierarchy":{"lvl1":"Boosting","lvl2":"Modélisation additive itérative"},"content":"","type":"content","url":"/boosting#mod-lisation-additive-it-rative","position":9},{"hierarchy":{"lvl1":"Boosting","lvl3":"Principe","lvl2":"Modélisation additive itérative"},"type":"lvl3","url":"/boosting#principe","position":10},{"hierarchy":{"lvl1":"Boosting","lvl3":"Principe","lvl2":"Modélisation additive itérative"},"content":"En modélisation additive itérative en avant (forward stagewise additive modeling), nous construisons le modèle séquentiellement. À l’étape m, nous résolvons:(\\beta_m, \\boldsymbol{\\theta}_m) = \\arg\\min_{\\beta, \\boldsymbol{\\theta}} \\sum_{i=1}^N \\ell\\left(y_i, f_{m-1}(\\boldsymbol{x}_i) + \\beta F(\\boldsymbol{x}_i; \\boldsymbol{\\theta})\\right)\n\nPuis nous mettons à jour le modèle:f_m(\\boldsymbol{x}) = f_{m-1}(\\boldsymbol{x}) + \\beta_m F_m(\\boldsymbol{x})\n\nImportant: nous ne réajustons pas les paramètres des modèles précédents. Chaque modèle est ajouté de manière incrémentale.","type":"content","url":"/boosting#principe","position":11},{"hierarchy":{"lvl1":"Boosting","lvl3":"Interprétation","lvl2":"Modélisation additive itérative"},"type":"lvl3","url":"/boosting#interpr-tation","position":12},{"hierarchy":{"lvl1":"Boosting","lvl3":"Interprétation","lvl2":"Modélisation additive itérative"},"content":"Intuitivement:\n\nLe modèle f_m tente de corriger les résidus de f_{m-1}\n\nChaque nouveau modèle se concentre sur les exemples difficiles\n\nL’ensemble devient progressivement plus puissant","type":"content","url":"/boosting#interpr-tation","position":13},{"hierarchy":{"lvl1":"Boosting","lvl2":"Boosting des moindres carrés"},"type":"lvl2","url":"/boosting#boosting-des-moindres-carr-s","position":14},{"hierarchy":{"lvl1":"Boosting","lvl2":"Boosting des moindres carrés"},"content":"","type":"content","url":"/boosting#boosting-des-moindres-carr-s","position":15},{"hierarchy":{"lvl1":"Boosting","lvl3":"Cas de la perte quadratique","lvl2":"Boosting des moindres carrés"},"type":"lvl3","url":"/boosting#cas-de-la-perte-quadratique","position":16},{"hierarchy":{"lvl1":"Boosting","lvl3":"Cas de la perte quadratique","lvl2":"Boosting des moindres carrés"},"content":"Avec la perte d’erreur quadratique \\ell(y, \\hat{y}) = (y - \\hat{y})^2, l’objectif à l’étape m devient:\\ell(y_i, f_{m-1}(\\boldsymbol{x}_i) + \\beta F(\\boldsymbol{x}_i; \\boldsymbol{\\theta})) = (y_i - f_{m-1}(\\boldsymbol{x}_i) - \\beta F(\\boldsymbol{x}_i; \\boldsymbol{\\theta}))^2 = (r_{im} - \\beta F(\\boldsymbol{x}_i; \\boldsymbol{\\theta}))^2\n\noù r_{im} = y_i - f_{m-1}(\\boldsymbol{x}_i) est le résidu du modèle actuel sur l’exemple i.","type":"content","url":"/boosting#cas-de-la-perte-quadratique","position":17},{"hierarchy":{"lvl1":"Boosting","lvl3":"Interprétation des résidus","lvl2":"Boosting des moindres carrés"},"type":"lvl3","url":"/boosting#interpr-tation-des-r-sidus","position":18},{"hierarchy":{"lvl1":"Boosting","lvl3":"Interprétation des résidus","lvl2":"Boosting des moindres carrés"},"content":"Le modèle faible F_m apprend à prédire les résidus. Ainsi:\n\nSi f_{m-1} sous-estime y_i, le résidu r_{im} est positif\n\nF_m apprend à produire des corrections positives pour ces exemples\n\nLa somme f_{m-1} + F_m s’approche de y\n\nCette approche s’appelle le boosting des moindres carrés (least squares boosting).","type":"content","url":"/boosting#interpr-tation-des-r-sidus","position":19},{"hierarchy":{"lvl1":"Boosting","lvl2":"AdaBoost"},"type":"lvl2","url":"/boosting#adaboost","position":20},{"hierarchy":{"lvl1":"Boosting","lvl2":"AdaBoost"},"content":"","type":"content","url":"/boosting#adaboost","position":21},{"hierarchy":{"lvl1":"Boosting","lvl3":"Contexte: classification binaire","lvl2":"AdaBoost"},"type":"lvl3","url":"/boosting#contexte-classification-binaire","position":22},{"hierarchy":{"lvl1":"Boosting","lvl3":"Contexte: classification binaire","lvl2":"AdaBoost"},"content":"Considérons la classification binaire avec \\tilde{y}_i \\in \\{-1, +1\\}. Nous modélisons la probabilité comme:p(y = 1 \\mid \\boldsymbol{x}) = \\frac{e^{F(\\boldsymbol{x})}}{e^{-F(\\boldsymbol{x})} + e^{F(\\boldsymbol{x})}} = \\frac{1}{1 + e^{-2F(\\boldsymbol{x})}}\n\noù F(\\boldsymbol{x}) représente la moitié du logarithme des cotes (log-odds).","type":"content","url":"/boosting#contexte-classification-binaire","position":23},{"hierarchy":{"lvl1":"Boosting","lvl3":"Fonction de perte exponentielle","lvl2":"AdaBoost"},"type":"lvl3","url":"/boosting#fonction-de-perte-exponentielle","position":24},{"hierarchy":{"lvl1":"Boosting","lvl3":"Fonction de perte exponentielle","lvl2":"AdaBoost"},"content":"La perte exponentielle est définie par:\\ell(\\tilde{y}, F(\\boldsymbol{x})) = \\exp(-\\tilde{y} F(\\boldsymbol{x}))\n\nPropriétés importantes:\n\nC’est une borne supérieure lisse sur la perte 0-1\n\nElle a la même solution optimale que la perte logarithmique\n\nElle est plus facile à optimiser dans le cadre du boosting\n\nLa perte exponentielle est sensible aux valeurs aberrantes car elle pénalise exponentiellement les erreurs.","type":"content","url":"/boosting#fonction-de-perte-exponentielle","position":25},{"hierarchy":{"lvl1":"Boosting","lvl3":"Dérivation d’AdaBoost","lvl2":"AdaBoost"},"type":"lvl3","url":"/boosting#d-rivation-dadaboost","position":26},{"hierarchy":{"lvl1":"Boosting","lvl3":"Dérivation d’AdaBoost","lvl2":"AdaBoost"},"content":"À l’étape m, nous minimisons:L_m(F) = \\sum_{i=1}^N \\exp\\left[-\\tilde{y}_i\\left(f_{m-1}(\\boldsymbol{x}_i) + \\beta F(\\boldsymbol{x}_i)\\right)\\right] = \\sum_{i=1}^N \\omega_{im} \\exp\\left(-\\beta \\tilde{y}_i F(\\boldsymbol{x}_i)\\right)\n\noù \\omega_{im} \\triangleq \\exp(-\\tilde{y}_i f_{m-1}(\\boldsymbol{x}_i)) sont les poids des exemples.\n\nPour un classificateur binaire F \\in \\{-1, +1\\}, nous pouvons réécrire:L_m = (e^\\beta - e^{-\\beta}) \\sum_{i=1}^N \\omega_{im} \\mathbb{I}(\\tilde{y}_i \\neq F(\\boldsymbol{x}_i)) + e^{-\\beta} \\sum_{i=1}^N \\omega_{im}","type":"content","url":"/boosting#d-rivation-dadaboost","position":27},{"hierarchy":{"lvl1":"Boosting","lvl3":"Choix du modèle de base","lvl2":"AdaBoost"},"type":"lvl3","url":"/boosting#choix-du-mod-le-de-base","position":28},{"hierarchy":{"lvl1":"Boosting","lvl3":"Choix du modèle de base","lvl2":"AdaBoost"},"content":"Le modèle de base optimal minimise l’erreur pondérée:F_m = \\arg\\min_F \\sum_{i=1}^N \\omega_{im} \\mathbb{I}(\\tilde{y}_i \\neq F(\\boldsymbol{x}_i))\n\nNous entraînons donc un classificateur sur une version pondérée des données.","type":"content","url":"/boosting#choix-du-mod-le-de-base","position":29},{"hierarchy":{"lvl1":"Boosting","lvl3":"Choix du coefficient","lvl2":"AdaBoost"},"type":"lvl3","url":"/boosting#choix-du-coefficient","position":30},{"hierarchy":{"lvl1":"Boosting","lvl3":"Choix du coefficient","lvl2":"AdaBoost"},"content":"Le coefficient optimal est:\\beta_m = \\frac{1}{2} \\log \\frac{1 - \\text{err}_m}{\\text{err}_m}\n\noù l’erreur pondérée est:\\text{err}_m = \\frac{\\sum_{i=1}^N \\omega_{im} \\mathbb{I}(\\tilde{y}_i \\neq F_m(\\boldsymbol{x}_i))}{\\sum_{i=1}^N \\omega_{im}}\n\nRemarques:\n\nSi \\text{err}_m = 0, alors \\beta_m \\to +\\infty (confiance totale)\n\nSi \\text{err}_m = 0.5, alors \\beta_m = 0 (pas mieux que le hasard)\n\nSi \\text{err}_m > 0.5, alors \\beta_m < 0 (inversé!)","type":"content","url":"/boosting#choix-du-coefficient","position":31},{"hierarchy":{"lvl1":"Boosting","lvl3":"Mise à jour des poids","lvl2":"AdaBoost"},"type":"lvl3","url":"/boosting#mise-jour-des-poids","position":32},{"hierarchy":{"lvl1":"Boosting","lvl3":"Mise à jour des poids","lvl2":"AdaBoost"},"content":"Les poids pour l’itération suivante sont:\\omega_{i,m+1} = \\omega_{im} \\exp(-\\tilde{y}_i \\beta_m F_m(\\boldsymbol{x}_i))\n\nEn pratique, cela signifie:\n\nLes exemples mal classifiés (\\tilde{y}_i \\neq F_m(\\boldsymbol{x}_i)) voient leur poids augmenter exponentiellement\n\nLes exemples bien classifiés voient leur poids diminuer","type":"content","url":"/boosting#mise-jour-des-poids","position":33},{"hierarchy":{"lvl1":"Boosting","lvl3":"Algorithme AdaBoost.M1","lvl2":"AdaBoost"},"type":"lvl3","url":"/boosting#algorithme-adaboost-m1","position":34},{"hierarchy":{"lvl1":"Boosting","lvl3":"Algorithme AdaBoost.M1","lvl2":"AdaBoost"},"content":"Entrée: Données {(x_i, y_i)}, nombre d'itérations M\n1. Initialiser ω_i = 1/N pour tout i\n2. Pour m = 1 à M:\n   a. Apprendre F_m sur les données pondérées par ω\n   b. Calculer err_m = Σ ω_i I(ỹ_i ≠ F_m(x_i)) / Σ ω_i\n   c. Calculer α_m = log[(1 - err_m) / err_m]\n   d. Mettre à jour: ω_i ← ω_i exp[α_m I(ỹ_i ≠ F_m(x_i))]\n3. Retourner f(x) = signe[Σ α_m F_m(x)]","type":"content","url":"/boosting#algorithme-adaboost-m1","position":35},{"hierarchy":{"lvl1":"Boosting","lvl2":"LogitBoost"},"type":"lvl2","url":"/boosting#logitboost","position":36},{"hierarchy":{"lvl1":"Boosting","lvl2":"LogitBoost"},"content":"","type":"content","url":"/boosting#logitboost","position":37},{"hierarchy":{"lvl1":"Boosting","lvl3":"Motivation","lvl2":"LogitBoost"},"type":"lvl3","url":"/boosting#motivation","position":38},{"hierarchy":{"lvl1":"Boosting","lvl3":"Motivation","lvl2":"LogitBoost"},"content":"AdaBoost augmente exponentiellement les poids des exemples mal classifiés, ce qui le rend très sensible aux valeurs aberrantes et au bruit.\n\nLogitBoost utilise plutôt la perte logarithmique (entropie croisée):L_m(F) = \\sum_{i=1}^N \\log\\left[1 + \\exp\\left(-2\\tilde{y}_i(f_{m-1}(\\boldsymbol{x}) + F(\\boldsymbol{x}_i))\\right)\\right]\n\nCette perte croît linéairement (au lieu d’exponentiellement) avec la marge négative, ce qui donne une meilleure robustesse aux outliers.","type":"content","url":"/boosting#motivation","position":39},{"hierarchy":{"lvl1":"Boosting","lvl3":"Optimisation","lvl2":"LogitBoost"},"type":"lvl3","url":"/boosting#optimisation","position":40},{"hierarchy":{"lvl1":"Boosting","lvl3":"Optimisation","lvl2":"LogitBoost"},"content":"Contrairement à AdaBoost, la mise à jour de LogitBoost n’a pas de forme analytique. On utilise une approximation par la méthode de Newton (Newton boosting).","type":"content","url":"/boosting#optimisation","position":41},{"hierarchy":{"lvl1":"Boosting","lvl2":"Gradient Boosting"},"type":"lvl2","url":"/boosting#gradient-boosting","position":42},{"hierarchy":{"lvl1":"Boosting","lvl2":"Gradient Boosting"},"content":"","type":"content","url":"/boosting#gradient-boosting","position":43},{"hierarchy":{"lvl1":"Boosting","lvl3":"Idée générale","lvl2":"Gradient Boosting"},"type":"lvl3","url":"/boosting#id-e-g-n-rale","position":44},{"hierarchy":{"lvl1":"Boosting","lvl3":"Idée générale","lvl2":"Gradient Boosting"},"content":"Le gradient boosting est une approche générique qui résout:\\hat{f} = \\arg\\min_f \\mathcal{L}(f)\n\nen effectuant une descente de gradient dans l’espace des fonctions.","type":"content","url":"/boosting#id-e-g-n-rale","position":45},{"hierarchy":{"lvl1":"Boosting","lvl3":"Représentation finie","lvl2":"Gradient Boosting"},"type":"lvl3","url":"/boosting#repr-sentation-finie","position":46},{"hierarchy":{"lvl1":"Boosting","lvl3":"Représentation finie","lvl2":"Gradient Boosting"},"content":"Puisque les fonctions sont des objets de dimension infinie, nous les représentons par leurs valeurs sur l’ensemble d’entraînement:\\boldsymbol{f} = (f(\\boldsymbol{x}_1), \\ldots, f(\\boldsymbol{x}_N))\n\nLe gradient de \\mathcal{L} par rapport à \\boldsymbol{f} au point f_{m-1} est:g_{im} = \\left[\\frac{\\partial \\ell(y_i, f(\\boldsymbol{x}_i))}{\\partial f(\\boldsymbol{x}_i)}\\right]_{f = f_{m-1}}","type":"content","url":"/boosting#repr-sentation-finie","position":47},{"hierarchy":{"lvl1":"Boosting","lvl3":"Descente de gradient fonctionnelle","lvl2":"Gradient Boosting"},"type":"lvl3","url":"/boosting#descente-de-gradient-fonctionnelle","position":48},{"hierarchy":{"lvl1":"Boosting","lvl3":"Descente de gradient fonctionnelle","lvl2":"Gradient Boosting"},"content":"La mise à jour serait:\\boldsymbol{f}_m = \\boldsymbol{f}_{m-1} - \\beta_m \\boldsymbol{g}_m\n\navec:\\beta_m = \\arg\\min_\\beta \\mathcal{L}(\\boldsymbol{f}_{m-1} - \\beta \\boldsymbol{g}_m)\n\nProblème: cette mise à jour ne définit f_m que sur les points d’entraînement. Elle ne généralise pas aux nouveaux exemples.","type":"content","url":"/boosting#descente-de-gradient-fonctionnelle","position":49},{"hierarchy":{"lvl1":"Boosting","lvl3":"Solution: approximer le gradient","lvl2":"Gradient Boosting"},"type":"lvl3","url":"/boosting#solution-approximer-le-gradient","position":50},{"hierarchy":{"lvl1":"Boosting","lvl3":"Solution: approximer le gradient","lvl2":"Gradient Boosting"},"content":"Le gradient boosting résout ce problème en apprenant un modèle faible F_m qui approxime le gradient négatif:F_m = \\arg\\min_F \\sum_{i=1}^N (-g_{im} - F(\\boldsymbol{x}_i))^2\n\nCe modèle peut ensuite être évalué sur n’importe quel point \\boldsymbol{x}.","type":"content","url":"/boosting#solution-approximer-le-gradient","position":51},{"hierarchy":{"lvl1":"Boosting","lvl3":"Algorithme général","lvl2":"Gradient Boosting"},"type":"lvl3","url":"/boosting#algorithme-g-n-ral","position":52},{"hierarchy":{"lvl1":"Boosting","lvl3":"Algorithme général","lvl2":"Gradient Boosting"},"content":"1. Initialiser f_0(x) = argmin_F Σ ℓ(y_i, F(x_i))\n2. Pour m = 1 à M:\n   a. Calculer les résidus: r_im = -∂ℓ(y_i, f(x_i))/∂f(x_i)|_{f=f_{m-1}}\n   b. Apprendre F_m sur les couples (x_i, r_im) par régression\n   c. Mettre à jour: f_m(x) = f_{m-1}(x) + ν F_m(x)\n3. Retourner f_M(x)\n\nLe paramètre 0 < \\nu \\leq 1 est le taux d’apprentissage (ou facteur de rétrécissement, shrinkage). Des valeurs petites (\\nu \\approx 0.1) améliorent la généralisation au prix de plus d’itérations.","type":"content","url":"/boosting#algorithme-g-n-ral","position":53},{"hierarchy":{"lvl1":"Boosting","lvl2":"Gradient Tree Boosting"},"type":"lvl2","url":"/boosting#gradient-tree-boosting","position":54},{"hierarchy":{"lvl1":"Boosting","lvl2":"Gradient Tree Boosting"},"content":"","type":"content","url":"/boosting#gradient-tree-boosting","position":55},{"hierarchy":{"lvl1":"Boosting","lvl3":"Arbres comme modèles de base","lvl2":"Gradient Tree Boosting"},"type":"lvl3","url":"/boosting#arbres-comme-mod-les-de-base","position":56},{"hierarchy":{"lvl1":"Boosting","lvl3":"Arbres comme modèles de base","lvl2":"Gradient Tree Boosting"},"content":"Les arbres de décision sont des modèles de base naturels pour le gradient boosting:F_m(\\boldsymbol{x}) = \\sum_{j=1}^{J_m} w_{jm} \\mathbb{I}(\\boldsymbol{x} \\in R_{jm})\n\noù w_{jm} est la prédiction de la région R_{jm}.","type":"content","url":"/boosting#arbres-comme-mod-les-de-base","position":57},{"hierarchy":{"lvl1":"Boosting","lvl3":"Procédure en deux étapes","lvl2":"Gradient Tree Boosting"},"type":"lvl3","url":"/boosting#proc-dure-en-deux-tapes","position":58},{"hierarchy":{"lvl1":"Boosting","lvl3":"Procédure en deux étapes","lvl2":"Gradient Tree Boosting"},"content":"Trouver les régions: Construire un arbre de régression sur les résidus \\{-g_{im}\\} pour déterminer les régions R_{jm}\n\nOptimiser les poids: Pour chaque feuille j, trouver le poids optimal:\\hat{w}_{jm} = \\arg\\min_w \\sum_{\\boldsymbol{x}_i \\in R_{jm}} \\ell(y_i, f_{m-1}(\\boldsymbol{x}_i) + w)\n\nPour la perte quadratique, le poids optimal est simplement la moyenne des résidus dans la feuille.","type":"content","url":"/boosting#proc-dure-en-deux-tapes","position":59},{"hierarchy":{"lvl1":"Boosting","lvl3":"XGBoost","lvl2":"Gradient Tree Boosting"},"type":"lvl3","url":"/boosting#xgboost","position":60},{"hierarchy":{"lvl1":"Boosting","lvl3":"XGBoost","lvl2":"Gradient Tree Boosting"},"content":"XGBoost (eXtreme Gradient Boosting) est une implémentation très populaire du gradient tree boosting. Ses innovations principales:\n\nRégularisation explicite:\n\\mathcal{L}_m(F_m) = \\sum_{i=1}^N \\ell(y_i, f_{m-1}(\\boldsymbol{x}_i) + F_m(\\boldsymbol{x}_i)) + \\Omega(F_m)\n\noù \\Omega(F_m) = \\gamma J + \\frac{\\lambda}{2} \\sum_{j=1}^J w_j^2 pénalise le nombre de feuilles et les poids.\n\nApproximation de second ordre: Taylor de second ordre de la perte:\n\\mathcal{L}_m \\approx \\sum_{i=1}^N \\left[g_{im} F_m(\\boldsymbol{x}_i) + \\frac{1}{2} h_{im} F_m^2(\\boldsymbol{x}_i)\\right] + \\Omega(F_m)\n\noù h_{im} est la dérivée seconde (Hessienne).\n\nCritère de gain optimisé:\n\\text{gain} = \\frac{1}{2}\\left[\\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L + G_R)^2}{H_L + H_R + \\lambda}\\right] - \\gamma","type":"content","url":"/boosting#xgboost","position":61},{"hierarchy":{"lvl1":"Boosting","lvl2":"Importance des caractéristiques"},"type":"lvl2","url":"/boosting#importance-des-caract-ristiques","position":62},{"hierarchy":{"lvl1":"Boosting","lvl2":"Importance des caractéristiques"},"content":"","type":"content","url":"/boosting#importance-des-caract-ristiques","position":63},{"hierarchy":{"lvl1":"Boosting","lvl3":"Définition pour un arbre","lvl2":"Importance des caractéristiques"},"type":"lvl3","url":"/boosting#d-finition-pour-un-arbre","position":64},{"hierarchy":{"lvl1":"Boosting","lvl3":"Définition pour un arbre","lvl2":"Importance des caractéristiques"},"content":"Pour un arbre T, l’importance de la caractéristique k est:R_k(T) = \\sum_{j=1}^{J-1} G_j \\mathbb{I}(v_j = k)\n\noù:\n\nLa somme est sur tous les nœuds internes\n\nG_j est le gain (réduction de coût) au nœud j\n\nv_j = k si le nœud j utilise la caractéristique k","type":"content","url":"/boosting#d-finition-pour-un-arbre","position":65},{"hierarchy":{"lvl1":"Boosting","lvl3":"Pour un ensemble","lvl2":"Importance des caractéristiques"},"type":"lvl3","url":"/boosting#pour-un-ensemble","position":66},{"hierarchy":{"lvl1":"Boosting","lvl3":"Pour un ensemble","lvl2":"Importance des caractéristiques"},"content":"Pour un ensemble de M arbres, nous moyennons:R_k = \\frac{1}{M} \\sum_{m=1}^M R_k(T_m)\n\nCes scores peuvent être normalisés dans l’intervalle [0, 1].\n\nCette mesure d’importance permet d’interpréter les ensembles d’arbres, même si les arbres individuels ne sont plus interprétables.","type":"content","url":"/boosting#pour-un-ensemble","position":67},{"hierarchy":{"lvl1":"Boosting","lvl2":"Comparaison: Bagging vs Boosting"},"type":"lvl2","url":"/boosting#comparaison-bagging-vs-boosting","position":68},{"hierarchy":{"lvl1":"Boosting","lvl2":"Comparaison: Bagging vs Boosting"},"content":"Aspect\n\nBagging\n\nBoosting\n\nRéduction\n\nVariance\n\nBiais\n\nEntraînement\n\nParallèle\n\nSéquentiel\n\nPondération\n\nUniforme\n\nAdaptative\n\nModèles de base\n\nHaute variance\n\nFaible complexité\n\nSurapprentissage\n\nRésistant\n\nPossible\n\nInterprétation\n\nImportance des variables\n\nImportance des variables","type":"content","url":"/boosting#comparaison-bagging-vs-boosting","position":69},{"hierarchy":{"lvl1":"Boosting","lvl3":"Quand utiliser quoi?","lvl2":"Comparaison: Bagging vs Boosting"},"type":"lvl3","url":"/boosting#quand-utiliser-quoi","position":70},{"hierarchy":{"lvl1":"Boosting","lvl3":"Quand utiliser quoi?","lvl2":"Comparaison: Bagging vs Boosting"},"content":"Bagging/Forêts aléatoires: données bruitées, prévention du surapprentissage\n\nBoosting: maximiser la performance prédictive, arbres peu profonds suffisants","type":"content","url":"/boosting#quand-utiliser-quoi","position":71},{"hierarchy":{"lvl1":"Boosting","lvl2":"Résumé"},"type":"lvl2","url":"/boosting#r-sum","position":72},{"hierarchy":{"lvl1":"Boosting","lvl2":"Résumé"},"content":"Le boosting est une méthode puissante qui construit un ensemble séquentiellement:\n\nChaque modèle corrige les erreurs des précédents\n\nAdaBoost utilise la perte exponentielle et des poids adaptatifs\n\nGradient boosting généralise à n’importe quelle perte différentiable\n\nLes arbres de décision sont des modèles de base efficaces\n\nXGBoost ajoute régularisation et optimisations\n\nLe boosting réduit principalement le biais (contrairement au bagging qui réduit la variance), ce qui en fait un complément naturel aux forêts aléatoires.","type":"content","url":"/boosting#r-sum","position":73},{"hierarchy":{"lvl1":"Boosting","lvl2":"Exercices"},"type":"lvl2","url":"/boosting#exercices","position":74},{"hierarchy":{"lvl1":"Boosting","lvl2":"Exercices"},"content":"Exercice 1: Poids AdaBoost\n\nConsidérez AdaBoost avec 3 exemples initialement pondérés uniformément (\\omega_i = 1/3). Le premier classificateur faible a une erreur pondérée de \\text{err}_1 = 0.25.\n\nCalculez \\alpha_1 = \\log[(1 - \\text{err}_1) / \\text{err}_1].\n\nSi l’exemple 1 est mal classifié et les exemples 2 et 3 sont bien classifiés, calculez les nouveaux poids \\omega_i.\n\nNormalisez les poids pour qu’ils somment à 1.\n\nExercice 2: Résidus\n\nPour le boosting des moindres carrés avec les données \\{(1, 2), (2, 4), (3, 3)\\}:\n\nSi f_0(x) = 3 (constante), calculez les résidus r_{i1}.\n\nSi F_1(x) = 0.5x - 1, calculez f_1(x) = f_0(x) + F_1(x).\n\nCalculez les nouveaux résidus r_{i2}.\n\nExercice 3: Gradient de la perte\n\nPour la perte logistique \\ell(y, f) = \\log(1 + e^{-yf}) où y \\in \\{-1, +1\\}:\n\nCalculez le gradient \\frac{\\partial \\ell}{\\partial f}.\n\nInterprétez le signe du gradient en fonction de y et f.\n\nQue représentent les résidus -g_{im} dans le gradient boosting avec cette perte?\n\nExercice 4: Régularisation\n\nDans XGBoost avec la régularisation \\Omega(F) = \\gamma J + \\frac{\\lambda}{2} \\sum_j w_j^2:\n\nExpliquez l’effet de \\gamma sur la structure de l’arbre.\n\nExpliquez l’effet de \\lambda sur les prédictions.\n\nSi \\gamma = 0 et \\lambda = 0, que devient le critère de gain?","type":"content","url":"/boosting#exercices","position":75},{"hierarchy":{"lvl1":"Partitionnement"},"type":"lvl1","url":"/clustering","position":0},{"hierarchy":{"lvl1":"Partitionnement"},"content":"Objectifs d’apprentissage\n\nÀ la fin de ce chapitre, vous serez en mesure de:\n\nDéfinir le problème de partitionnement (clustering)\n\nImplémenter l’algorithme k-moyennes\n\nDériver l’algorithme EM pour les mélanges gaussiens (GMM)\n\nExpliquer le lien entre k-moyennes et GMM\n\nComprendre les différents types de covariance dans les GMM\n\nAppliquer ces méthodes à des problèmes réels","type":"content","url":"/clustering","position":1},{"hierarchy":{"lvl1":"Partitionnement","lvl2":"Motivation"},"type":"lvl2","url":"/clustering#motivation","position":2},{"hierarchy":{"lvl1":"Partitionnement","lvl2":"Motivation"},"content":"Le partitionnement (ou clustering) est une tâche d’apprentissage non supervisé qui consiste à regrouper des exemples similaires ensemble, sans disposer d’étiquettes. C’est l’une des techniques les plus utilisées en analyse de données.","type":"content","url":"/clustering#motivation","position":3},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Applications","lvl2":"Motivation"},"type":"lvl3","url":"/clustering#applications","position":4},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Applications","lvl2":"Motivation"},"content":"Segmentation de clientèle: regrouper des clients ayant des comportements similaires\n\nCompression d’images: réduire le nombre de couleurs en regroupant les pixels similaires\n\nDétection d’anomalies: identifier les points qui n’appartiennent à aucun groupe\n\nPrétraitement: découvrir une structure dans les données avant une analyse supervisée\n\nBioinformatique: regrouper des gènes ou des protéines ayant des fonctions similaires","type":"content","url":"/clustering#applications","position":5},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Types de partitionnement","lvl2":"Motivation"},"type":"lvl3","url":"/clustering#types-de-partitionnement","position":6},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Types de partitionnement","lvl2":"Motivation"},"content":"Dur (hard clustering): chaque point appartient à exactement un groupe\n\nSouple (soft clustering): chaque point a une probabilité d’appartenir à chaque groupe","type":"content","url":"/clustering#types-de-partitionnement","position":7},{"hierarchy":{"lvl1":"Partitionnement","lvl2":"K-moyennes"},"type":"lvl2","url":"/clustering#k-moyennes","position":8},{"hierarchy":{"lvl1":"Partitionnement","lvl2":"K-moyennes"},"content":"","type":"content","url":"/clustering#k-moyennes","position":9},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Formulation","lvl2":"K-moyennes"},"type":"lvl3","url":"/clustering#formulation","position":10},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Formulation","lvl2":"K-moyennes"},"content":"L’algorithme k-moyennes (k-means) partitionne N points en K groupes en minimisant la distorsion intra-groupe:\\mathcal{L} = \\sum_{n=1}^N \\sum_{k=1}^K r_{nk} \\|\\boldsymbol{x}_n - \\boldsymbol{\\mu}_k\\|^2\n\noù:\n\nr_{nk} \\in \\{0, 1\\} est l’assignation du point n au groupe k (avec \\sum_k r_{nk} = 1)\n\n\\boldsymbol{\\mu}_k est le centroïde du groupe k","type":"content","url":"/clustering#formulation","position":11},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Algorithme","lvl2":"K-moyennes"},"type":"lvl3","url":"/clustering#algorithme","position":12},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Algorithme","lvl2":"K-moyennes"},"content":"K-moyennes alterne entre deux étapes:\n\nÉtape E (assignation): Fixer les centroïdes, optimiser les assignationsr_{nk} = \\begin{cases} 1 & \\text{si } k = \\arg\\min_{k'} \\|\\boldsymbol{x}_n - \\boldsymbol{\\mu}_{k'}\\|^2 \\\\ 0 & \\text{sinon} \\end{cases}\n\nEn d’autres termes, chaque point est assigné au centroïde le plus proche.\n\nÉtape M (mise à jour): Fixer les assignations, optimiser les centroïdes\\boldsymbol{\\mu}_k = \\frac{\\sum_{n=1}^N r_{nk} \\boldsymbol{x}_n}{\\sum_{n=1}^N r_{nk}}\n\nChaque centroïde est la moyenne des points qui lui sont assignés.","type":"content","url":"/clustering#algorithme","position":13},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Pseudocode","lvl2":"K-moyennes"},"type":"lvl3","url":"/clustering#pseudocode","position":14},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Pseudocode","lvl2":"K-moyennes"},"content":"Entrée: Données X, nombre de groupes K\n1. Initialiser les centroïdes μ_1, ..., μ_K (aléatoirement)\n2. Répéter jusqu'à convergence:\n   a. Pour chaque point n:\n      - Assigner x_n au centroïde le plus proche\n   b. Pour chaque groupe k:\n      - μ_k ← moyenne des points assignés à k\nSortie: Centroïdes et assignations","type":"content","url":"/clustering#pseudocode","position":15},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Propriétés","lvl2":"K-moyennes"},"type":"lvl3","url":"/clustering#propri-t-s","position":16},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Propriétés","lvl2":"K-moyennes"},"content":"Convergence: K-moyennes converge toujours vers un minimum local car:\n\nL’étape E réduit ou maintient la distorsion (assignation optimale)\n\nL’étape M réduit ou maintient la distorsion (centroïde optimal)\n\nLa distorsion est bornée inférieurement par 0\n\nSensibilité à l’initialisation: Le minimum trouvé dépend de l’initialisation. Solutions:\n\nExécuter plusieurs fois avec des initialisations différentes\n\nUtiliser k-means++ pour une meilleure initialisation","type":"content","url":"/clustering#propri-t-s","position":17},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"K-means++","lvl2":"K-moyennes"},"type":"lvl3","url":"/clustering#k-means","position":18},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"K-means++","lvl2":"K-moyennes"},"content":"Initialisation intelligente qui choisit les centroïdes initiaux de manière à les espacer:\n\nChoisir le premier centroïde uniformément au hasard parmi les données\n\nPour chaque centroïde suivant, choisir un point avec probabilité proportionnelle au carré de la distance au centroïde le plus proche\n\nRépéter jusqu’à avoir K centroïdes\n\nCette stratégie garantit une solution O(\\log K)-compétitive avec l’optimum.","type":"content","url":"/clustering#k-means","position":19},{"hierarchy":{"lvl1":"Partitionnement","lvl2":"Modèles de mélange gaussien (GMM)"},"type":"lvl2","url":"/clustering#mod-les-de-m-lange-gaussien-gmm","position":20},{"hierarchy":{"lvl1":"Partitionnement","lvl2":"Modèles de mélange gaussien (GMM)"},"content":"","type":"content","url":"/clustering#mod-les-de-m-lange-gaussien-gmm","position":21},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Formulation probabiliste","lvl2":"Modèles de mélange gaussien (GMM)"},"type":"lvl3","url":"/clustering#formulation-probabiliste","position":22},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Formulation probabiliste","lvl2":"Modèles de mélange gaussien (GMM)"},"content":"Un modèle de mélange gaussien (GMM, Gaussian Mixture Model) suppose que les données sont générées par un mélange de K distributions gaussiennes:p(\\boldsymbol{x} \\mid \\boldsymbol{\\theta}) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(\\boldsymbol{x} \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)\n\noù:\n\n\\pi_k est le poids du composant k (avec \\sum_k \\pi_k = 1 et \\pi_k \\geq 0)\n\n\\boldsymbol{\\mu}_k est la moyenne du composant k\n\n\\boldsymbol{\\Sigma}_k est la matrice de covariance du composant k","type":"content","url":"/clustering#formulation-probabiliste","position":23},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Variables latentes","lvl2":"Modèles de mélange gaussien (GMM)"},"type":"lvl3","url":"/clustering#variables-latentes","position":24},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Variables latentes","lvl2":"Modèles de mélange gaussien (GMM)"},"content":"Pour chaque observation \\boldsymbol{x}_n, nous introduisons une variable latente z_n \\in \\{1, \\ldots, K\\} indiquant le composant dont elle provient:p(z_n = k) = \\pi_kp(\\boldsymbol{x}_n \\mid z_n = k) = \\mathcal{N}(\\boldsymbol{x}_n \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)","type":"content","url":"/clustering#variables-latentes","position":25},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Responsabilités","lvl2":"Modèles de mélange gaussien (GMM)"},"type":"lvl3","url":"/clustering#responsabilit-s","position":26},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Responsabilités","lvl2":"Modèles de mélange gaussien (GMM)"},"content":"La responsabilité du composant k pour le point n est la probabilité a posteriori:r_{nk} \\triangleq p(z_n = k \\mid \\boldsymbol{x}_n, \\boldsymbol{\\theta}) = \\frac{\\pi_k \\mathcal{N}(\\boldsymbol{x}_n \\mid \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{\\sum_{k'=1}^K \\pi_{k'} \\mathcal{N}(\\boldsymbol{x}_n \\mid \\boldsymbol{\\mu}_{k'}, \\boldsymbol{\\Sigma}_{k'})}\n\nContrairement à k-moyennes, les responsabilités sont des valeurs continues dans [0, 1].","type":"content","url":"/clustering#responsabilit-s","position":27},{"hierarchy":{"lvl1":"Partitionnement","lvl2":"Algorithme EM pour GMM"},"type":"lvl2","url":"/clustering#algorithme-em-pour-gmm","position":28},{"hierarchy":{"lvl1":"Partitionnement","lvl2":"Algorithme EM pour GMM"},"content":"","type":"content","url":"/clustering#algorithme-em-pour-gmm","position":29},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"L’algorithme Espérance-Maximisation","lvl2":"Algorithme EM pour GMM"},"type":"lvl3","url":"/clustering#lalgorithme-esp-rance-maximisation","position":30},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"L’algorithme Espérance-Maximisation","lvl2":"Algorithme EM pour GMM"},"content":"L’algorithme EM (Expectation-Maximization) est une méthode générale pour l’estimation de paramètres dans des modèles avec variables latentes. Il maximise itérativement une borne inférieure de la log-vraisemblance.","type":"content","url":"/clustering#lalgorithme-esp-rance-maximisation","position":31},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Étape E","lvl2":"Algorithme EM pour GMM"},"type":"lvl3","url":"/clustering#id-tape-e","position":32},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Étape E","lvl2":"Algorithme EM pour GMM"},"content":"Calculer les responsabilités avec les paramètres actuels \\boldsymbol{\\theta}^{(t)}:r_{nk}^{(t)} = \\frac{\\pi_k^{(t)} \\mathcal{N}(\\boldsymbol{x}_n \\mid \\boldsymbol{\\mu}_k^{(t)}, \\boldsymbol{\\Sigma}_k^{(t)})}{\\sum_{k'=1}^K \\pi_{k'}^{(t)} \\mathcal{N}(\\boldsymbol{x}_n \\mid \\boldsymbol{\\mu}_{k'}^{(t)}, \\boldsymbol{\\Sigma}_{k'}^{(t)})}","type":"content","url":"/clustering#id-tape-e","position":33},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Étape M","lvl2":"Algorithme EM pour GMM"},"type":"lvl3","url":"/clustering#id-tape-m","position":34},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Étape M","lvl2":"Algorithme EM pour GMM"},"content":"Maximiser la log-vraisemblance des données complètes attendue. Les mises à jour sont:\n\nPoids du mélange:\\pi_k^{(t+1)} = \\frac{1}{N} \\sum_{n=1}^N r_{nk}^{(t)} = \\frac{r_k^{(t)}}{N}\n\noù r_k^{(t)} = \\sum_n r_{nk}^{(t)} est le nombre effectif de points dans le groupe k.\n\nMoyennes:\\boldsymbol{\\mu}_k^{(t+1)} = \\frac{\\sum_{n=1}^N r_{nk}^{(t)} \\boldsymbol{x}_n}{r_k^{(t)}}\n\nCovariances:\\boldsymbol{\\Sigma}_k^{(t+1)} = \\frac{\\sum_{n=1}^N r_{nk}^{(t)} (\\boldsymbol{x}_n - \\boldsymbol{\\mu}_k^{(t+1)})(\\boldsymbol{x}_n - \\boldsymbol{\\mu}_k^{(t+1)})^\\top}{r_k^{(t)}}","type":"content","url":"/clustering#id-tape-m","position":35},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Pseudocode","lvl2":"Algorithme EM pour GMM"},"type":"lvl3","url":"/clustering#pseudocode-1","position":36},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Pseudocode","lvl2":"Algorithme EM pour GMM"},"content":"Entrée: Données X, nombre de composants K\n1. Initialiser les paramètres θ = (π, μ, Σ)\n2. Répéter jusqu'à convergence:\n   a. Étape E: Calculer les responsabilités r_nk\n   b. Étape M: Mettre à jour π, μ, Σ\nSortie: Paramètres θ et responsabilités r","type":"content","url":"/clustering#pseudocode-1","position":37},{"hierarchy":{"lvl1":"Partitionnement","lvl2":"Types de covariance"},"type":"lvl2","url":"/clustering#types-de-covariance","position":38},{"hierarchy":{"lvl1":"Partitionnement","lvl2":"Types de covariance"},"content":"","type":"content","url":"/clustering#types-de-covariance","position":39},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Covariance complète","lvl2":"Types de covariance"},"type":"lvl3","url":"/clustering#covariance-compl-te","position":40},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Covariance complète","lvl2":"Types de covariance"},"content":"Chaque composant a sa propre matrice de covariance \\boldsymbol{\\Sigma}_k, qui peut avoir n’importe quelle forme (ellipsoïdes orientés arbitrairement).\n\nParamètres: K \\times D(D+1)/2 pour les covariances","type":"content","url":"/clustering#covariance-compl-te","position":41},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Covariance partagée (tied)","lvl2":"Types de covariance"},"type":"lvl3","url":"/clustering#covariance-partag-e-tied","position":42},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Covariance partagée (tied)","lvl2":"Types de covariance"},"content":"Tous les composants partagent la même matrice de covariance \\boldsymbol{\\Sigma}:\\boldsymbol{\\Sigma} = \\frac{1}{N} \\sum_{k=1}^K \\sum_{n=1}^N r_{nk} (\\boldsymbol{x}_n - \\boldsymbol{\\mu}_k)(\\boldsymbol{x}_n - \\boldsymbol{\\mu}_k)^\\top","type":"content","url":"/clustering#covariance-partag-e-tied","position":43},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Covariance diagonale","lvl2":"Types de covariance"},"type":"lvl3","url":"/clustering#covariance-diagonale","position":44},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Covariance diagonale","lvl2":"Types de covariance"},"content":"Chaque \\boldsymbol{\\Sigma}_k est diagonale: les dimensions sont indépendantes au sein de chaque groupe.\n\nParamètres: K \\times D","type":"content","url":"/clustering#covariance-diagonale","position":45},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Covariance sphérique","lvl2":"Types de covariance"},"type":"lvl3","url":"/clustering#covariance-sph-rique","position":46},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Covariance sphérique","lvl2":"Types de covariance"},"content":"Chaque \\boldsymbol{\\Sigma}_k = \\sigma_k^2 \\mathbf{I} est un multiple de l’identité: les groupes sont des sphères.\n\nParamètres: K scalaires","type":"content","url":"/clustering#covariance-sph-rique","position":47},{"hierarchy":{"lvl1":"Partitionnement","lvl2":"Lien entre k-moyennes et GMM"},"type":"lvl2","url":"/clustering#lien-entre-k-moyennes-et-gmm","position":48},{"hierarchy":{"lvl1":"Partitionnement","lvl2":"Lien entre k-moyennes et GMM"},"content":"","type":"content","url":"/clustering#lien-entre-k-moyennes-et-gmm","position":49},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"K-moyennes comme cas particulier","lvl2":"Lien entre k-moyennes et GMM"},"type":"lvl3","url":"/clustering#k-moyennes-comme-cas-particulier","position":50},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"K-moyennes comme cas particulier","lvl2":"Lien entre k-moyennes et GMM"},"content":"K-moyennes peut être vu comme un GMM avec:\n\nCovariances sphériques partagées: \\boldsymbol{\\Sigma}_k = \\sigma^2 \\mathbf{I} pour tout k\n\nLimite \\sigma^2 \\to 0: les responsabilités deviennent dures (0 ou 1)\n\nPoids uniformes: \\pi_k = 1/K\n\nDans cette limite:r_{nk} \\to \\mathbb{I}(k = \\arg\\min_{k'} \\|\\boldsymbol{x}_n - \\boldsymbol{\\mu}_{k'}\\|^2)","type":"content","url":"/clustering#k-moyennes-comme-cas-particulier","position":51},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Regroupement dur vs souple","lvl2":"Lien entre k-moyennes et GMM"},"type":"lvl3","url":"/clustering#regroupement-dur-vs-souple","position":52},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Regroupement dur vs souple","lvl2":"Lien entre k-moyennes et GMM"},"content":"Aspect\n\nK-moyennes\n\nGMM\n\nAssignation\n\nDure (0 ou 1)\n\nSouple (probabilités)\n\nForme des groupes\n\nSphériques\n\nEllipsoïdales\n\nModèle probabiliste\n\nNon\n\nOui\n\nIncertitude\n\nNon\n\nOui\n\nGénératif\n\nNon\n\nOui","type":"content","url":"/clustering#regroupement-dur-vs-souple","position":53},{"hierarchy":{"lvl1":"Partitionnement","lvl2":"Considérations pratiques"},"type":"lvl2","url":"/clustering#consid-rations-pratiques","position":54},{"hierarchy":{"lvl1":"Partitionnement","lvl2":"Considérations pratiques"},"content":"","type":"content","url":"/clustering#consid-rations-pratiques","position":55},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Choix de K","lvl2":"Considérations pratiques"},"type":"lvl3","url":"/clustering#choix-de-k","position":56},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Choix de K","lvl2":"Considérations pratiques"},"content":"Le nombre de groupes est un hyperparamètre difficile à choisir:\n\nMéthode du coude: tracer la distorsion en fonction de K et chercher un coude\n\nCritères d’information:\n\nBIC: -2\\log p(\\mathcal{D}|\\hat{\\boldsymbol{\\theta}}) + p \\log N\n\nAIC: -2\\log p(\\mathcal{D}|\\hat{\\boldsymbol{\\theta}}) + 2p\n\noù p est le nombre de paramètres\n\nSilhouette: mesure de cohésion et séparation des groupes","type":"content","url":"/clustering#choix-de-k","position":57},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Problèmes potentiels","lvl2":"Considérations pratiques"},"type":"lvl3","url":"/clustering#probl-mes-potentiels","position":58},{"hierarchy":{"lvl1":"Partitionnement","lvl3":"Problèmes potentiels","lvl2":"Considérations pratiques"},"content":"Singularités: Si un composant GMM ne contient qu’un point, sa covariance peut devenir singulière. Solutions:\n\nAjouter une régularisation diagonale\n\nUtiliser des covariances partagées ou diagonales\n\nRéinitialiser les composants problématiques\n\nMinima locaux: EM converge vers un minimum local. Solutions:\n\nPlusieurs exécutions avec initialisations différentes\n\nInitialiser avec k-moyennes","type":"content","url":"/clustering#probl-mes-potentiels","position":59},{"hierarchy":{"lvl1":"Partitionnement","lvl2":"Résumé"},"type":"lvl2","url":"/clustering#r-sum","position":60},{"hierarchy":{"lvl1":"Partitionnement","lvl2":"Résumé"},"content":"Le partitionnement regroupe des données non étiquetées:\n\nK-moyennes minimise la distorsion en alternant assignation et mise à jour des centroïdes\n\nLes GMM modélisent les données comme un mélange de gaussiennes\n\nEM estime les paramètres des GMM en alternant étape E (responsabilités) et étape M (paramètres)\n\nK-moyennes est un cas limite de GMM avec covariances sphériques\n\nLe choix de K reste un défi pratique\n\nCes méthodes forment la base de nombreuses techniques d’apprentissage non supervisé plus avancées.","type":"content","url":"/clustering#r-sum","position":61},{"hierarchy":{"lvl1":"Partitionnement","lvl2":"Exercices"},"type":"lvl2","url":"/clustering#exercices","position":62},{"hierarchy":{"lvl1":"Partitionnement","lvl2":"Exercices"},"content":"Exercice 1: K-moyennes manuel\n\nConsidérez les points suivants en 1D: \\{1, 2, 3, 10, 11, 12\\} avec K=2.\n\nSi les centroïdes initiaux sont \\mu_1 = 1 et \\mu_2 = 2, effectuez une itération de k-moyennes.\n\nAprès combien d’itérations l’algorithme converge-t-il?\n\nQuelle est la distorsion finale?\n\nExercice 2: Responsabilités GMM\n\nUn GMM à 2 composants en 1D a les paramètres:\n\n\\pi_1 = 0.3, \\mu_1 = 0, \\sigma_1^2 = 1\n\n\\pi_2 = 0.7, \\mu_2 = 3, \\sigma_2^2 = 1\n\nPour l’observation x = 1:\n\nCalculez p(x | z=1) et p(x | z=2).\n\nCalculez les responsabilités r_1 et r_2.\n\nÀ quel composant ce point “appartient-il” le plus?\n\nExercice 3: Nombre de paramètres\n\nPour un GMM avec K composants en dimension D:\n\nCombien de paramètres y a-t-il avec covariances complètes?\n\nEt avec covariances diagonales?\n\nPour D = 100 et K = 10, calculez ces nombres.\n\nExercice 4: Convergence EM\n\nExpliquez pourquoi l’algorithme EM est garanti de converger (au sens où la log-vraisemblance ne décroît jamais).\n\nPourquoi cela ne garantit-il pas de trouver le maximum global?","type":"content","url":"/clustering#exercices","position":63},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs"},"type":"lvl1","url":"/cnn","position":0},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs"},"content":"Objectifs d’apprentissage\n\nÀ la fin de ce chapitre, vous serez en mesure de:\n\nExpliquer pourquoi les MLP ne sont pas adaptés au traitement d’images\n\nDéfinir l’opération de convolution et ses propriétés d’invariance\n\nDécrire l’architecture d’un réseau convolutif (CNN)\n\nComprendre les couches de convolution, pooling et normalisation\n\nImplémenter la régularisation par dropout\n\nDiscuter des architectures classiques (LeNet, VGG, ResNet)","type":"content","url":"/cnn","position":1},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl2":"Motivation: les limites des MLP pour les images"},"type":"lvl2","url":"/cnn#motivation-les-limites-des-mlp-pour-les-images","position":2},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl2":"Motivation: les limites des MLP pour les images"},"content":"Dans le chapitre précédent, nous avons vu comment les perceptrons multicouches (MLP) peuvent apprendre des fonctions complexes en composant plusieurs transformations non linéaires. Cependant, lorsqu’il s’agit de traiter des images, les MLP présentent une limitation fondamentale: ils ne sont pas invariants à la translation.\n\nConsidérons un exemple simple. Supposons que nous voulons détecter un motif particulier (par exemple, un visage) dans une image. Si nous utilisons un MLP classique avec une couche entièrement connectée, le vecteur de poids appris sera spécifique à une position particulière dans l’image. Si l’objet se trouve à gauche de l’image, le réseau produira une forte réponse; mais si ce même objet est décalé vers la droite, la réponse sera faible, car les poids ne correspondent plus à la nouvelle position.\n\nCe problème est particulièrement grave pour les images, car:\n\nLe même objet peut apparaître n’importe où dans l’image\n\nLe nombre de paramètres d’un MLP croît quadratiquement avec la taille de l’image\n\nChaque position doit être apprise indépendamment, ce qui nécessite énormément de données\n\nLes réseaux de neurones convolutifs (CNN, pour Convolutional Neural Networks) résolvent ces problèmes en remplaçant la multiplication matricielle par une opération de convolution, qui est intrinsèquement invariante à la translation.","type":"content","url":"/cnn#motivation-les-limites-des-mlp-pour-les-images","position":3},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl2":"L’opération de convolution"},"type":"lvl2","url":"/cnn#lop-ration-de-convolution","position":4},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl2":"L’opération de convolution"},"content":"","type":"content","url":"/cnn#lop-ration-de-convolution","position":5},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Définition mathématique","lvl2":"L’opération de convolution"},"type":"lvl3","url":"/cnn#d-finition-math-matique","position":6},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Définition mathématique","lvl2":"L’opération de convolution"},"content":"La convolution entre deux fonctions f, g: \\mathbb{R}^D \\rightarrow \\mathbb{R} est définie par l’intégrale:[f \\circledast g](\\boldsymbol{z}) = \\int_{\\mathbb{R}^D} f(\\boldsymbol{u}) g(\\boldsymbol{z} - \\boldsymbol{u}) \\, d\\boldsymbol{u}\n\nIntuitivement, la convolution “glisse” une fonction (le noyau ou filtre) sur l’autre et calcule la somme pondérée à chaque position. Le résultat est une mesure de la similarité locale entre les deux fonctions.","type":"content","url":"/cnn#d-finition-math-matique","position":7},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Convolution discrète en 2D","lvl2":"L’opération de convolution"},"type":"lvl3","url":"/cnn#convolution-discr-te-en-2d","position":8},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Convolution discrète en 2D","lvl2":"L’opération de convolution"},"content":"Dans le cas des images numériques, nous travaillons avec des tableaux discrets. La convolution 2D discrète entre une image \\mathbf{X} et un filtre \\mathbf{W} de taille H \\times W est:[\\mathbf{W} \\circledast \\mathbf{X}](i, j) = \\sum_{u=0}^{H-1} \\sum_{v=0}^{W-1} w_{u,v} \\, x_{i+u, j+v}\n\nPrenons un exemple concret. Considérons la convolution d’une image 3 \\times 3 avec un noyau 2 \\times 2:\\mathbf{Y} = \\begin{pmatrix} w_1 & w_2 \\\\ w_3 & w_4 \\end{pmatrix} \\circledast \\begin{pmatrix} x_1 & x_2 & x_3 \\\\ x_4 & x_5 & x_6 \\\\ x_7 & x_8 & x_9 \\end{pmatrix}\n\nLe résultat est une image 2 \\times 2:\\mathbf{Y} = \\begin{pmatrix} \nw_1 x_1 + w_2 x_2 + w_3 x_4 + w_4 x_5 & w_1 x_2 + w_2 x_3 + w_3 x_5 + w_4 x_6 \\\\\nw_1 x_4 + w_2 x_5 + w_3 x_7 + w_4 x_8 & w_1 x_5 + w_2 x_6 + w_3 x_8 + w_4 x_9\n\\end{pmatrix}\n\nChaque élément de la sortie est calculé en appliquant le même filtre à une région locale de l’image. C’est cette propriété de partage des poids qui confère aux CNN leur invariance à la translation.","type":"content","url":"/cnn#convolution-discr-te-en-2d","position":9},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Cartes de réponse","lvl2":"L’opération de convolution"},"type":"lvl3","url":"/cnn#cartes-de-r-ponse","position":10},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Cartes de réponse","lvl2":"L’opération de convolution"},"content":"Lorsque nous appliquons un filtre à une image, nous obtenons une carte de réponse (ou feature map). Chaque point de cette carte indique la force de la réponse du filtre à cette position.\n\nPar exemple, un filtre conçu pour détecter des lignes diagonales produira une carte de réponse avec des valeurs élevées aux endroits où l’image contient effectivement des lignes diagonales. Les CNN apprennent automatiquement ces filtres à partir des données, plutôt que de les concevoir manuellement.","type":"content","url":"/cnn#cartes-de-r-ponse","position":11},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Convolution comme opérateur linéaire","lvl2":"L’opération de convolution"},"type":"lvl3","url":"/cnn#convolution-comme-op-rateur-lin-aire","position":12},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Convolution comme opérateur linéaire","lvl2":"L’opération de convolution"},"content":"La convolution est un opérateur linéaire, ce qui signifie qu’elle peut être représentée par une multiplication matricielle. En “aplatissant” l’image 2D \\mathbf{X} en un vecteur 1D \\boldsymbol{x}, la convolution devient:\\boldsymbol{y} = \\mathbf{C} \\boldsymbol{x}\n\noù \\mathbf{C} est une matrice de Toeplitz dérivée du noyau. Pour notre exemple précédent:\\mathbf{C} = \\begin{pmatrix}\nw_1 & w_2 & 0 & w_3 & w_4 & 0 & 0 & 0 & 0 \\\\\n0 & w_1 & w_2 & 0 & w_3 & w_4 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & w_1 & w_2 & 0 & w_3 & w_4 & 0 \\\\\n0 & 0 & 0 & 0 & w_1 & w_2 & 0 & w_3 & w_4\n\\end{pmatrix}\n\nCette représentation révèle que les CNN sont en fait des MLP avec une structure creuse spéciale dans les matrices de poids. Les éléments non nuls sont liés entre eux (ils correspondent aux mêmes poids du filtre), ce qui:\n\nImpose l’invariance à la translation\n\nRéduit massivement le nombre de paramètres","type":"content","url":"/cnn#convolution-comme-op-rateur-lin-aire","position":13},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl2":"Architecture d’un CNN"},"type":"lvl2","url":"/cnn#architecture-dun-cnn","position":14},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl2":"Architecture d’un CNN"},"content":"","type":"content","url":"/cnn#architecture-dun-cnn","position":15},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Structure générale","lvl2":"Architecture d’un CNN"},"type":"lvl3","url":"/cnn#structure-g-n-rale","position":16},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Structure générale","lvl2":"Architecture d’un CNN"},"content":"Un CNN typique est composé de plusieurs types de couches empilées:\n\nCouches de convolution: extraient des caractéristiques locales\n\nCouches d’activation: introduisent la non-linéarité (ReLU, etc.)\n\nCouches de pooling: réduisent la résolution spatiale\n\nCouches de normalisation: stabilisent l’entraînement\n\nCouches entièrement connectées: effectuent la classification finale","type":"content","url":"/cnn#structure-g-n-rale","position":17},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Couches de convolution","lvl2":"Architecture d’un CNN"},"type":"lvl3","url":"/cnn#couches-de-convolution","position":18},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Couches de convolution","lvl2":"Architecture d’un CNN"},"content":"Une couche de convolution applique plusieurs filtres à son entrée pour produire plusieurs cartes de réponse. Si l’entrée a C_{in} canaux (par exemple, 3 pour une image RGB), chaque filtre a également C_{in} canaux, et la sortie a C_{out} canaux (un par filtre).\n\nParamètres importants:\n\nTaille du noyau (kernel size): dimensions spatiales du filtre (ex: 3 \\times 3)\n\nPas (stride): décalage entre positions successives du filtre\n\nRembourrage (padding): ajout de zéros autour de l’image pour contrôler la taille de sortie","type":"content","url":"/cnn#couches-de-convolution","position":19},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Couches de pooling","lvl2":"Architecture d’un CNN"},"type":"lvl3","url":"/cnn#couches-de-pooling","position":20},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Couches de pooling","lvl2":"Architecture d’un CNN"},"content":"Lors de la classification d’images, nous voulons souvent savoir simplement si un objet est présent, sans nous soucier de sa position exacte. Les couches de pooling réduisent la résolution spatiale en agrégeant l’information locale.\n\nDeux variantes principales existent:\n\nPooling maximum (max pooling): prend le maximum sur une fenêtre localey_{i,j} = \\max_{(u,v) \\in \\mathcal{R}_{i,j}} x_{u,v}\n\nPooling moyen (average pooling): calcule la moyenne sur une fenêtre localey_{i,j} = \\frac{1}{|\\mathcal{R}_{i,j}|} \\sum_{(u,v) \\in \\mathcal{R}_{i,j}} x_{u,v}\n\nLe pooling maximum est généralement préféré car il préserve les caractéristiques les plus saillantes et introduit une certaine invariance aux petites translations.","type":"content","url":"/cnn#couches-de-pooling","position":21},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Couches de normalisation","lvl2":"Architecture d’un CNN"},"type":"lvl3","url":"/cnn#couches-de-normalisation","position":22},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Couches de normalisation","lvl2":"Architecture d’un CNN"},"content":"Pour stabiliser l’entraînement des réseaux profonds, on utilise des couches de normalisation. La plus courante est la normalisation par lots (batch normalization).\n\nLe problème fondamental est le décalage covariant interne (internal covariate shift): les distributions des activations des couches internes changent constamment pendant l’entraînement, ce qui rend l’optimisation difficile.\n\nLa normalisation par lots normalise les pré-activations \\boldsymbol{z}_n pour chaque mini-lot \\mathcal{B}:\\hat{\\boldsymbol{z}}_n = \\frac{\\boldsymbol{z}_n - \\boldsymbol{\\mu}_{\\mathcal{B}}}{\\sqrt{\\boldsymbol{\\sigma}_{\\mathcal{B}}^2 + \\epsilon}}\n\noù:\n\n\\boldsymbol{\\mu}_{\\mathcal{B}} = \\frac{1}{|\\mathcal{B}|} \\sum_{\\boldsymbol{z} \\in \\mathcal{B}} \\boldsymbol{z} est la moyenne sur le mini-lot\n\n\\boldsymbol{\\sigma}_{\\mathcal{B}}^2 = \\frac{1}{|\\mathcal{B}|} \\sum_{\\boldsymbol{z} \\in \\mathcal{B}} (\\boldsymbol{z} - \\boldsymbol{\\mu}_{\\mathcal{B}})^2 est la variance\n\n\\epsilon est une petite constante pour la stabilité numérique\n\nOn ajoute ensuite des paramètres apprenables \\boldsymbol{\\gamma} et \\boldsymbol{\\beta} pour redonner de l’expressivité au réseau:\\tilde{\\boldsymbol{z}}_n = \\boldsymbol{\\gamma} \\odot \\hat{\\boldsymbol{z}}_n + \\boldsymbol{\\beta}\n\nPendant l’entraînement, les statistiques sont calculées sur chaque mini-lot. Pour l’inférence, on utilise des moyennes mobiles calculées sur l’ensemble d’entraînement.","type":"content","url":"/cnn#couches-de-normalisation","position":23},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl2":"Régularisation: Dropout"},"type":"lvl2","url":"/cnn#r-gularisation-dropout","position":24},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl2":"Régularisation: Dropout"},"content":"","type":"content","url":"/cnn#r-gularisation-dropout","position":25},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Le problème du surapprentissage","lvl2":"Régularisation: Dropout"},"type":"lvl3","url":"/cnn#le-probl-me-du-surapprentissage","position":26},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Le problème du surapprentissage","lvl2":"Régularisation: Dropout"},"content":"Les réseaux de neurones profonds ont une grande capacité et peuvent facilement surapprendre les données d’entraînement. Plusieurs techniques de régularisation sont utilisées pour améliorer la généralisation.","type":"content","url":"/cnn#le-probl-me-du-surapprentissage","position":27},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Régularisation \\ell_2","lvl2":"Régularisation: Dropout"},"type":"lvl3","url":"/cnn#r-gularisation-ell-2","position":28},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Régularisation \\ell_2","lvl2":"Régularisation: Dropout"},"content":"Comme en régression ridge, nous pouvons imposer un a priori gaussien sur les poids:p(\\boldsymbol{w}) = \\mathcal{N}(\\boldsymbol{w} \\mid \\mathbf{0}, \\alpha^2 \\mathbf{I})\n\nCela revient à ajouter un terme de pénalisation \\ell_2 à la fonction de perte:\\mathcal{L}_{reg} = \\mathcal{L} + \\lambda \\|\\boldsymbol{w}\\|_2^2\n\nCette technique est aussi appelée dégradation des poids (weight decay).","type":"content","url":"/cnn#r-gularisation-ell-2","position":29},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Dropout","lvl2":"Régularisation: Dropout"},"type":"lvl3","url":"/cnn#dropout","position":30},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Dropout","lvl2":"Régularisation: Dropout"},"content":"Une approche plus radicale est le dropout, qui désactive aléatoirement certaines unités pendant l’entraînement. À chaque passage, chaque neurone est désactivé avec une probabilité p (typiquement p = 0.5).\n\nMathématiquement, si \\epsilon_{li} \\sim \\text{Ber}(1-p), alors les poids effectifs deviennent:\\theta_{lij} = w_{lij} \\, \\epsilon_{li}\n\nSi \\epsilon_{li} = 0, tous les poids sortants de l’unité i dans la couche l-1 vers n’importe quelle unité j dans la couche l seront nuls.\n\nLe dropout peut être interprété comme:\n\nUne régularisation qui empêche la co-adaptation complexe des unités cachées\n\nUn entraînement implicite d’un ensemble de sous-réseaux","type":"content","url":"/cnn#dropout","position":31},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Considérations pratiques","lvl2":"Régularisation: Dropout"},"type":"lvl3","url":"/cnn#consid-rations-pratiques","position":32},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Considérations pratiques","lvl2":"Régularisation: Dropout"},"content":"Plusieurs points techniques sont importants:\n\nMise à l’échelle à l’inférence: Pour que les activations aient la même espérance pendant le test, on multiplie les poids par (1-p) avant de faire des prédictions (ou de manière équivalente, on divise par (1-p) pendant l’entraînement).\n\nDésactivation en test: Normalement, le dropout est désactivé pendant l’inférence pour obtenir des prédictions déterministes.\n\nMonte Carlo Dropout: En gardant le dropout actif pendant l’inférence et en moyennant plusieurs prédictions, on obtient une approximation de l’incertitude prédictive:p(\\boldsymbol{y} \\mid \\boldsymbol{x}, \\mathcal{D}) \\approx \\frac{1}{S} \\sum_{s=1}^S p(\\boldsymbol{y} \\mid \\boldsymbol{x}, \\hat{\\mathbf{W}} \\epsilon^s + \\hat{\\boldsymbol{b}})\n\nCette technique fournit une approximation de la distribution prédictive a posteriori bayésienne.","type":"content","url":"/cnn#consid-rations-pratiques","position":33},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl2":"Réseaux de neurones bayésiens"},"type":"lvl2","url":"/cnn#r-seaux-de-neurones-bay-siens","position":34},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl2":"Réseaux de neurones bayésiens"},"content":"Les réseaux de neurones bayésiens (BNN) capturent l’incertitude de manière plus rigoureuse en marginalisant sur les paramètres:p(\\boldsymbol{y} \\mid \\boldsymbol{x}, \\mathcal{D}) = \\int p(\\boldsymbol{y} \\mid \\boldsymbol{x}, \\boldsymbol{\\theta}) \\, p(\\boldsymbol{\\theta} \\mid \\mathcal{D}) \\, d\\boldsymbol{\\theta}\n\nCela correspond à un ensemble infini de réseaux, pondérés par leur probabilité a posteriori. Cette marginalisation:\n\nÉvite le surapprentissage naturellement\n\nFournit des estimations d’incertitude calibrées\n\nEst cependant difficile à réaliser en pratique pour les grands réseaux\n\nLe Monte Carlo Dropout offre une approximation pratique de cette intégrale.","type":"content","url":"/cnn#r-seaux-de-neurones-bay-siens","position":35},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl2":"Architectures classiques"},"type":"lvl2","url":"/cnn#architectures-classiques","position":36},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl2":"Architectures classiques"},"content":"","type":"content","url":"/cnn#architectures-classiques","position":37},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Le Neocognitron (1980)","lvl2":"Architectures classiques"},"type":"lvl3","url":"/cnn#le-neocognitron-1980","position":38},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Le Neocognitron (1980)","lvl2":"Architectures classiques"},"content":"L’idée d’alterner convolutions et pooling remonte au neocognitron de Kunihiko Fukushima (1980), inspiré par le modèle des cellules simples et complexes dans le cortex visuel découvert par Hubel et Wiesel.","type":"content","url":"/cnn#le-neocognitron-1980","position":39},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"LeNet (1998)","lvl2":"Architectures classiques"},"type":"lvl3","url":"/cnn#lenet-1998","position":40},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"LeNet (1998)","lvl2":"Architectures classiques"},"content":"Yann LeCun a popularisé cette architecture avec LeNet-5, conçu pour la reconnaissance de chiffres manuscrits. L’architecture typique alterne:\n\nCouche de convolution\n\nCouche de pooling\n\nRépétition des étapes 1-2\n\nCouches entièrement connectées pour la classification","type":"content","url":"/cnn#lenet-1998","position":41},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Architectures modernes","lvl2":"Architectures classiques"},"type":"lvl3","url":"/cnn#architectures-modernes","position":42},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl3":"Architectures modernes","lvl2":"Architectures classiques"},"content":"Depuis LeNet, de nombreuses architectures plus profondes et plus sophistiquées ont été développées:\n\nAlexNet (2012): premier CNN à gagner ImageNet avec une grande marge\n\nVGGNet (2014): architecture très profonde avec des convolutions 3 \\times 3 uniformes\n\nGoogLeNet/Inception (2014): modules “inception” avec convolutions parallèles de différentes tailles\n\nResNet (2015): connexions résiduelles permettant l’entraînement de réseaux très profonds (100+ couches)\n\nDenseNet (2017): connexions denses entre toutes les couches\n\nCes architectures ont démontré que la profondeur est cruciale pour les performances, à condition d’utiliser les bonnes techniques de régularisation et de normalisation.","type":"content","url":"/cnn#architectures-modernes","position":43},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl2":"Résumé"},"type":"lvl2","url":"/cnn#r-sum","position":44},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl2":"Résumé"},"content":"Les réseaux de neurones convolutifs constituent l’architecture de choix pour le traitement d’images. Leurs caractéristiques principales sont:\n\nInvariance à la translation grâce à l’opération de convolution\n\nPartage des poids qui réduit drastiquement le nombre de paramètres\n\nExtraction hiérarchique de caractéristiques des motifs simples aux concepts abstraits\n\nRobustesse grâce au pooling et aux techniques de régularisation\n\nLes couches de normalisation (batch norm) et de régularisation (dropout) sont essentielles pour entraîner des réseaux profonds efficacement. Le dropout offre également une voie vers l’estimation de l’incertitude via le Monte Carlo Dropout.\n\nDans le prochain chapitre, nous étudierons les réseaux récurrents, qui étendent ces idées au traitement de données séquentielles.","type":"content","url":"/cnn#r-sum","position":45},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl2":"Exercices"},"type":"lvl2","url":"/cnn#exercices","position":46},{"hierarchy":{"lvl1":"Réseaux de neurones convolutifs","lvl2":"Exercices"},"content":"Exercice 1: Dimensions de sortie\n\nSoit une image d’entrée de taille 32 \\times 32 avec 3 canaux (RGB). On applique une couche de convolution avec 16 filtres de taille 5 \\times 5, un pas de 1, et aucun rembourrage.\n\nQuelle est la taille de la sortie?\n\nCombien de paramètres cette couche a-t-elle (incluant les biais)?\n\nSi on ajoute un rembourrage de 2 pixels, quelle devient la taille de sortie?\n\nExercice 2: Matrice de Toeplitz\n\nConsidérez une image 1D \\boldsymbol{x} = [x_1, x_2, x_3, x_4] et un filtre \\boldsymbol{w} = [w_1, w_2].\n\nÉcrivez la matrice de Toeplitz \\mathbf{C} correspondant à cette convolution (sans rembourrage).\n\nCalculez explicitement le résultat \\boldsymbol{y} = \\mathbf{C}\\boldsymbol{x}.\n\nVérifiez que cela correspond bien à la définition de la convolution discrète.\n\nExercice 3: Batch normalization\n\nSoit un mini-lot de 4 exemples avec des activations z_1 = 2, z_2 = 4, z_3 = 6, z_4 = 8 pour un neurone particulier.\n\nCalculez \\mu_{\\mathcal{B}} et \\sigma_{\\mathcal{B}}^2.\n\nCalculez les activations normalisées \\hat{z}_1, \\hat{z}_2, \\hat{z}_3, \\hat{z}_4.\n\nSi \\gamma = 2 et \\beta = 1, calculez les activations finales \\tilde{z}_i.\n\nExercice 4: Dropout et variance\n\nConsidérez un neurone avec activation h suivi d’un dropout avec probabilité p = 0.5.\n\nQuelle est l’espérance de l’activation effective pendant l’entraînement?\n\nPourquoi faut-il multiplier par (1-p) à l’inférence (ou diviser par (1-p) à l’entraînement)?\n\nCalculez la variance de l’activation effective pendant l’entraînement.","type":"content","url":"/cnn#exercices","position":47},{"hierarchy":{"lvl1":"Méthodes d’ensemble"},"type":"lvl1","url":"/ensembles","position":0},{"hierarchy":{"lvl1":"Méthodes d’ensemble"},"content":"Objectifs d’apprentissage\n\nÀ la fin de ce chapitre, vous serez en mesure de:\n\nExpliquer le principe de l’apprentissage en ensemble\n\nAnalyser le compromis biais-variance\n\nDécrire le bagging et l’échantillonnage bootstrap\n\nImplémenter les forêts aléatoires\n\nComprendre les méthodes de comité et le stacking\n\nJustifier pourquoi les arbres de décision sont des modèles de base idéaux","type":"content","url":"/ensembles","position":1},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl2":"Le principe des ensembles"},"type":"lvl2","url":"/ensembles#le-principe-des-ensembles","position":2},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl2":"Le principe des ensembles"},"content":"Une façon simple de réduire la variance d’un modèle est de faire la moyenne de plusieurs modèles:f(y \\mid \\boldsymbol{x}) = \\frac{1}{|\\mathcal{M}|} \\sum_{m \\in \\mathcal{M}} f_m(y \\mid \\boldsymbol{x})\n\noù f_m est le m-ième modèle de base (ou apprenant faible). L’ensemble aura:\n\nUn biais similaire aux modèles de base\n\nUne variance plus faible grâce à l’effet de moyennage\n\nCette idée simple mais puissante est à la base de certains des algorithmes les plus performants en apprentissage automatique.","type":"content","url":"/ensembles#le-principe-des-ensembles","position":3},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl2":"Le compromis biais-variance"},"type":"lvl2","url":"/ensembles#le-compromis-biais-variance","position":4},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl2":"Le compromis biais-variance"},"content":"","type":"content","url":"/ensembles#le-compromis-biais-variance","position":5},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Décomposition de l’erreur","lvl2":"Le compromis biais-variance"},"type":"lvl3","url":"/ensembles#d-composition-de-lerreur","position":6},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Décomposition de l’erreur","lvl2":"Le compromis biais-variance"},"content":"Soit \\hat{\\theta} = \\hat{\\theta}(\\mathcal{D}) un estimateur et \\bar{\\theta} = \\mathbb{E}[\\hat{\\theta}] son espérance (sous la distribution des données p(\\mathcal{D} \\mid \\theta^*)). L’erreur quadratique moyenne se décompose comme:\\begin{aligned}\n\\mathbb{E}\\left[(\\hat{\\theta} - \\theta^*)^2\\right] &= \\mathbb{E}\\left[\\left[(\\hat{\\theta} - \\bar{\\theta}) + (\\bar{\\theta} - \\theta^*)\\right]^2\\right] \\\\\n&= \\mathbb{E}\\left[(\\hat{\\theta} - \\bar{\\theta})^2\\right] + (\\bar{\\theta} - \\theta^*)^2 \\\\\n&= \\mathbb{V}[\\hat{\\theta}] + \\text{biais}^2(\\hat{\\theta})\n\\end{aligned}\n\nAinsi:\\text{MSE} = \\text{variance} + \\text{biais}^2","type":"content","url":"/ensembles#d-composition-de-lerreur","position":7},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Interprétation","lvl2":"Le compromis biais-variance"},"type":"lvl3","url":"/ensembles#interpr-tation","position":8},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Interprétation","lvl2":"Le compromis biais-variance"},"content":"Biais: erreur systématique due aux hypothèses du modèle\n\nVariance: sensibilité aux fluctuations des données d’entraînement\n\nUn modèle simple (comme la régression linéaire) a typiquement:\n\nBiais élevé: ne peut pas capturer les relations complexes\n\nVariance faible: stable face aux variations des données\n\nUn modèle complexe (comme un arbre profond) a typiquement:\n\nBiais faible: peut approximer des fonctions complexes\n\nVariance élevée: très sensible aux données d’entraînement\n\nLes méthodes d’ensemble visent à réduire la variance tout en conservant un biais faible.","type":"content","url":"/ensembles#interpr-tation","position":9},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl2":"Méthodes de comité"},"type":"lvl2","url":"/ensembles#m-thodes-de-comit","position":10},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl2":"Méthodes de comité"},"content":"","type":"content","url":"/ensembles#m-thodes-de-comit","position":11},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Vote majoritaire","lvl2":"Méthodes de comité"},"type":"lvl3","url":"/ensembles#vote-majoritaire","position":12},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Vote majoritaire","lvl2":"Méthodes de comité"},"content":"Pour la classification, plutôt que de moyenner les probabilités, on peut utiliser un vote majoritaire:\\hat{y} = \\arg\\max_c \\sum_{m=1}^M \\mathbb{I}(f_m(\\boldsymbol{x}) = c)\n\nCette approche est parfois appelée une méthode de comité.","type":"content","url":"/ensembles#vote-majoritaire","position":13},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Analyse théorique","lvl2":"Méthodes de comité"},"type":"lvl3","url":"/ensembles#analyse-th-orique","position":14},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Analyse théorique","lvl2":"Méthodes de comité"},"content":"Supposons que chaque modèle de base est un classificateur binaire avec une précision \\theta > 0.5, et que les erreurs sont indépendantes. Soit:\n\nY_m \\in \\{0, 1\\} la prédiction du m-ième modèle\n\nS = \\sum_{m=1}^M Y_m le nombre de votes pour la classe 1 (supposée correcte)\n\nLe prédicteur final prédit la classe 1 si S > M/2.\n\nLa probabilité que l’ensemble soit correct est:p = \\Pr(S > M/2) = 1 - B(M/2, M, \\theta)\n\noù B(x, M, \\theta) est la fonction de répartition de la loi binomiale.","type":"content","url":"/ensembles#analyse-th-orique","position":15},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Exemple numérique","lvl2":"Méthodes de comité"},"type":"lvl3","url":"/ensembles#exemple-num-rique","position":16},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Exemple numérique","lvl2":"Méthodes de comité"},"content":"Avec \\theta = 0.51 (chaque modèle légèrement meilleur que le hasard):\n\nM = 1000 modèles: p \\approx 0.73\n\nM = 10000 modèles: p \\approx 0.97\n\nLa sagesse de la foule (wisdom of crowds) en action!","type":"content","url":"/ensembles#exemple-num-rique","position":17},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Limitation","lvl2":"Méthodes de comité"},"type":"lvl3","url":"/ensembles#limitation","position":18},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Limitation","lvl2":"Méthodes de comité"},"content":"Cette amélioration repose sur l’hypothèse que les modèles font des erreurs indépendantes, ce qui n’est généralement pas le cas en pratique. Nous devons donc chercher des méthodes pour diversifier les prédictions des modèles de base.","type":"content","url":"/ensembles#limitation","position":19},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl2":"Bagging"},"type":"lvl2","url":"/ensembles#bagging","position":20},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl2":"Bagging"},"content":"","type":"content","url":"/ensembles#bagging","position":21},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Principe","lvl2":"Bagging"},"type":"lvl3","url":"/ensembles#principe","position":22},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Principe","lvl2":"Bagging"},"content":"Le bagging (pour Bootstrap AGGregatING) crée de la diversité en entraînant chaque modèle sur un sous-ensemble différent des données, échantillonné avec remplacement.\n\nCette technique s’appelle l’échantillonnage bootstrap: on tire N exemples uniformément avec remplacement parmi les N exemples originaux.","type":"content","url":"/ensembles#principe","position":23},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Propriétés du bootstrap","lvl2":"Bagging"},"type":"lvl3","url":"/ensembles#propri-t-s-du-bootstrap","position":24},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Propriétés du bootstrap","lvl2":"Bagging"},"content":"En moyenne, chaque modèle de base voit seulement 63% des données. En effet, la probabilité qu’un exemple donné ne soit pas sélectionné dans N tirages est:\\left(1 - \\frac{1}{N}\\right)^N \\xrightarrow{N \\to \\infty} e^{-1} \\approx 0.37\n\nLes 37% d’exemples non utilisés par un modèle sont appelés les instances hors sac (out-of-bag, OOB). Elles permettent une forme de validation croisée gratuite.","type":"content","url":"/ensembles#propri-t-s-du-bootstrap","position":25},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Algorithme","lvl2":"Bagging"},"type":"lvl3","url":"/ensembles#algorithme","position":26},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Algorithme","lvl2":"Bagging"},"content":"Entrée: Données D = {(x_n, y_n)}, nombre de modèles M\nPour m = 1 à M:\n    D_m ← échantillon bootstrap de D (N tirages avec remplacement)\n    f_m ← entraîner un modèle sur D_m\nSortie: f(x) = (1/M) Σ_m f_m(x)  [régression]\n        f(x) = vote_majoritaire({f_m(x)})  [classification]","type":"content","url":"/ensembles#algorithme","position":27},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Avantages du bagging","lvl2":"Bagging"},"type":"lvl3","url":"/ensembles#avantages-du-bagging","position":28},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Avantages du bagging","lvl2":"Bagging"},"content":"Robustesse: empêche l’ensemble de donner trop d’importance à un exemple particulier\n\nGénéralisation: réduit le surapprentissage\n\nParallélisable: chaque modèle peut être entraîné indépendamment\n\nEstimation OOB: pas besoin d’ensemble de validation séparé","type":"content","url":"/ensembles#avantages-du-bagging","position":29},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Quand le bagging fonctionne","lvl2":"Bagging"},"type":"lvl3","url":"/ensembles#quand-le-bagging-fonctionne","position":30},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Quand le bagging fonctionne","lvl2":"Bagging"},"content":"Le bagging fonctionne bien quand les modèles de base sont instables — c’est-à-dire que l’omission de certaines données change significativement le modèle.\n\nBons candidats:\n\nArbres de décision (très instables)\n\nRéseaux de neurones (avec petits ensembles)\n\nMauvais candidats:\n\nk-NN (très stable: changer quelques données ne change pas beaucoup les voisins)\n\nRégression linéaire (relativement stable)","type":"content","url":"/ensembles#quand-le-bagging-fonctionne","position":31},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl2":"Forêts aléatoires"},"type":"lvl2","url":"/ensembles#for-ts-al-atoires","position":32},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl2":"Forêts aléatoires"},"content":"","type":"content","url":"/ensembles#for-ts-al-atoires","position":33},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Au-delà du bagging","lvl2":"Forêts aléatoires"},"type":"lvl3","url":"/ensembles#au-del-du-bagging","position":34},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Au-delà du bagging","lvl2":"Forêts aléatoires"},"content":"Les forêts aléatoires (Random Forests) étendent le bagging en ajoutant une source supplémentaire de randomisation: à chaque division d’un arbre, seul un sous-ensemble aléatoire de caractéristiques est considéré.","type":"content","url":"/ensembles#au-del-du-bagging","position":35},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Algorithme","lvl2":"Forêts aléatoires"},"type":"lvl3","url":"/ensembles#algorithme-1","position":36},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Algorithme","lvl2":"Forêts aléatoires"},"content":"À chaque nœud i, au lieu de chercher la meilleure division parmi toutes les caractéristiques \\{1, \\ldots, D\\}, on tire un sous-ensemble aléatoire S_i \\subset \\{1, \\ldots, D\\} et on optimise:(j_i, t_i) = \\arg\\min_{j \\in S_i} \\min_{t \\in \\mathcal{T}_j} \\left[ \\frac{|\\mathcal{D}_i^L(j, t)|}{|\\mathcal{D}_i|} c(\\mathcal{D}_i^L(j, t)) + \\frac{|\\mathcal{D}_i^R(j, t)|}{|\\mathcal{D}_i|} c(\\mathcal{D}_i^R(j, t)) \\right]\n\nLa taille typique du sous-ensemble est:\n\n|S_i| = \\sqrt{D} pour la classification\n\n|S_i| = D/3 pour la régression","type":"content","url":"/ensembles#algorithme-1","position":37},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Pourquoi ça fonctionne","lvl2":"Forêts aléatoires"},"type":"lvl3","url":"/ensembles#pourquoi-a-fonctionne","position":38},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Pourquoi ça fonctionne","lvl2":"Forêts aléatoires"},"content":"L’échantillonnage de caractéristiques décorrèle les arbres:\n\nDans le bagging simple, les arbres tendent à avoir des structures similaires (mêmes caractéristiques importantes aux premières divisions)\n\nAvec les forêts aléatoires, les arbres sont plus divers\n\nSi les arbres sont moins corrélés, la réduction de variance par moyennage est plus efficace.","type":"content","url":"/ensembles#pourquoi-a-fonctionne","position":39},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Importance des variables","lvl2":"Forêts aléatoires"},"type":"lvl3","url":"/ensembles#importance-des-variables","position":40},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Importance des variables","lvl2":"Forêts aléatoires"},"content":"Un avantage des forêts aléatoires est de fournir une mesure d’importance des variables:\n\nImportance par diminution d’impureté: somme des réductions d’impureté à chaque division utilisant une variable, moyennée sur tous les arbres\n\nImportance par permutation: mesure la dégradation de la performance OOB quand on permute aléatoirement les valeurs d’une variable\n\nCes mesures aident à l’interprétation et à la sélection de caractéristiques.","type":"content","url":"/ensembles#importance-des-variables","position":41},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl2":"Stacking"},"type":"lvl2","url":"/ensembles#stacking","position":42},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl2":"Stacking"},"content":"","type":"content","url":"/ensembles#stacking","position":43},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Combinaison apprise","lvl2":"Stacking"},"type":"lvl3","url":"/ensembles#combinaison-apprise","position":44},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Combinaison apprise","lvl2":"Stacking"},"content":"Au lieu d’utiliser une moyenne non pondérée ou un vote, on peut apprendre comment combiner les modèles:f(y \\mid \\boldsymbol{x}) = \\sum_{m \\in \\mathcal{M}} w_m f_m(y \\mid \\boldsymbol{x})\n\nCette technique s’appelle le stacking (stacked generalization).","type":"content","url":"/ensembles#combinaison-apprise","position":45},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Procédure","lvl2":"Stacking"},"type":"lvl3","url":"/ensembles#proc-dure","position":46},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Procédure","lvl2":"Stacking"},"content":"Niveau 0: entraîner les modèles de base f_1, \\ldots, f_M\n\nCréation des méta-caractéristiques: pour chaque exemple, créer un vecteur (f_1(\\boldsymbol{x}), \\ldots, f_M(\\boldsymbol{x}))\n\nNiveau 1: entraîner un méta-apprenant sur ces nouvelles caractéristiques\n\nImportant: les poids w_m doivent être entraînés sur un ensemble séparé, sinon le stacking mettrait tout le poids sur le modèle de base le plus performant sur l’ensemble d’entraînement.","type":"content","url":"/ensembles#proc-dure","position":47},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Avantages","lvl2":"Stacking"},"type":"lvl3","url":"/ensembles#avantages","position":48},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Avantages","lvl2":"Stacking"},"content":"Permet de combiner des modèles de types différents\n\nPeut apprendre des interactions complexes entre modèles\n\nSouvent utilisé dans les compétitions de ML","type":"content","url":"/ensembles#avantages","position":49},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl2":"Pourquoi les arbres sont idéaux pour les ensembles"},"type":"lvl2","url":"/ensembles#pourquoi-les-arbres-sont-id-aux-pour-les-ensembles","position":50},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl2":"Pourquoi les arbres sont idéaux pour les ensembles"},"content":"","type":"content","url":"/ensembles#pourquoi-les-arbres-sont-id-aux-pour-les-ensembles","position":51},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Haute variance, faible biais","lvl2":"Pourquoi les arbres sont idéaux pour les ensembles"},"type":"lvl3","url":"/ensembles#haute-variance-faible-biais","position":52},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Haute variance, faible biais","lvl2":"Pourquoi les arbres sont idéaux pour les ensembles"},"content":"Les arbres de décision profonds ont:\n\nFaible biais: peuvent approximer n’importe quelle fonction\n\nHaute variance: très sensibles aux données d’entraînement\n\nC’est exactement ce que les méthodes d’ensemble savent exploiter: réduire la variance tout en conservant le faible biais.","type":"content","url":"/ensembles#haute-variance-faible-biais","position":53},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Instabilité","lvl2":"Pourquoi les arbres sont idéaux pour les ensembles"},"type":"lvl3","url":"/ensembles#instabilit","position":54},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Instabilité","lvl2":"Pourquoi les arbres sont idéaux pour les ensembles"},"content":"L’instabilité des arbres signifie que le bagging produit des modèles véritablement différents, ce qui améliore la diversité de l’ensemble.","type":"content","url":"/ensembles#instabilit","position":55},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Efficacité computationnelle","lvl2":"Pourquoi les arbres sont idéaux pour les ensembles"},"type":"lvl3","url":"/ensembles#efficacit-computationnelle","position":56},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Efficacité computationnelle","lvl2":"Pourquoi les arbres sont idéaux pour les ensembles"},"content":"Les arbres s’entraînent rapidement\n\nPas d’optimisation itérative (comme la descente de gradient)\n\nFacilement parallélisables","type":"content","url":"/ensembles#efficacit-computationnelle","position":57},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Pas de pré-traitement","lvl2":"Pourquoi les arbres sont idéaux pour les ensembles"},"type":"lvl3","url":"/ensembles#pas-de-pr-traitement","position":58},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl3":"Pas de pré-traitement","lvl2":"Pourquoi les arbres sont idéaux pour les ensembles"},"content":"Pas besoin de normaliser les données\n\nGèrent les caractéristiques mixtes (continues et catégorielles)\n\nRobustes aux valeurs aberrantes","type":"content","url":"/ensembles#pas-de-pr-traitement","position":59},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl2":"Comparaison: Bagging vs Forêts aléatoires"},"type":"lvl2","url":"/ensembles#comparaison-bagging-vs-for-ts-al-atoires","position":60},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl2":"Comparaison: Bagging vs Forêts aléatoires"},"content":"Aspect\n\nBagging\n\nForêts aléatoires\n\nRandomisation\n\nDonnées (bootstrap)\n\nDonnées + caractéristiques\n\nCorrélation entre arbres\n\nPlus élevée\n\nPlus faible\n\nRéduction de variance\n\nBonne\n\nMeilleure\n\nCoût computationnel\n\nLégèrement plus élevé\n\nLégèrement plus bas par arbre\n\nInterprétabilité\n\nDifficile\n\nImportance des variables\n\nEn pratique, les forêts aléatoires sont presque toujours préférées au bagging simple.","type":"content","url":"/ensembles#comparaison-bagging-vs-for-ts-al-atoires","position":61},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl2":"Résumé"},"type":"lvl2","url":"/ensembles#r-sum","position":62},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl2":"Résumé"},"content":"Les méthodes d’ensemble exploitent le principe que plusieurs modèles sont meilleurs qu’un seul:\n\nLe compromis biais-variance explique pourquoi: le moyennage réduit la variance\n\nLe bagging crée de la diversité par échantillonnage bootstrap\n\nLes forêts aléatoires ajoutent l’échantillonnage de caractéristiques\n\nLe stacking apprend à combiner optimalement les prédictions\n\nLes arbres de décision sont des modèles de base idéaux grâce à leur haute variance\n\nAu chapitre suivant, nous verrons le boosting, une approche complémentaire qui réduit le biais plutôt que la variance.","type":"content","url":"/ensembles#r-sum","position":63},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl2":"Exercices"},"type":"lvl2","url":"/ensembles#exercices","position":64},{"hierarchy":{"lvl1":"Méthodes d’ensemble","lvl2":"Exercices"},"content":"Exercice 1: Probabilité de sélection bootstrap\n\nCalculez exactement (1 - 1/N)^N pour N = 10, 100, 1000.\n\nVérifiez que cela converge vers e^{-1} \\approx 0.368.\n\nSi N = 1000, en moyenne combien d’exemples uniques sont présents dans un échantillon bootstrap?\n\nExercice 2: Vote majoritaire\n\nConsidérez un ensemble de 7 classificateurs binaires, chacun avec une précision de 60%.\n\nCalculez la probabilité que le vote majoritaire soit correct (en supposant l’indépendance des erreurs).\n\nCombien de classificateurs doivent voter correctement pour que l’ensemble soit correct?\n\nQue se passe-t-il si la précision individuelle tombe à 49%?\n\nExercice 3: Variance de l’ensemble\n\nSoit \\hat{f}_m des estimateurs i.i.d. avec variance \\sigma^2. L’estimateur de l’ensemble est \\hat{f} = \\frac{1}{M} \\sum_{m=1}^M \\hat{f}_m.\n\nCalculez \\mathbb{V}[\\hat{f}].\n\nQue se passe-t-il quand M \\to \\infty?\n\nSi les estimateurs sont corrélés avec \\text{Cov}(\\hat{f}_i, \\hat{f}_j) = \\rho \\sigma^2 pour i \\neq j, que devient la variance de l’ensemble?\n\nExercice 4: Forêt aléatoire\n\nUn jeu de données a D = 100 caractéristiques. Pour une forêt aléatoire de classification:\n\nCombien de caractéristiques sont typiquement considérées à chaque division?\n\nSi un arbre a une profondeur de 10, combien de caractéristiques différentes peut-il utiliser au maximum?\n\nExpliquez pourquoi échantillonner les caractéristiques décorrèle les arbres.","type":"content","url":"/ensembles#exercices","position":65},{"hierarchy":{"lvl1":"Généralisation"},"type":"lvl1","url":"/generalization","position":0},{"hierarchy":{"lvl1":"Généralisation"},"content":"Objectifs d’apprentissage\n\nÀ la fin de ce chapitre, vous serez en mesure de:\n\nExpliquer le phénomène de surapprentissage et de sous-apprentissage\n\nRelier la régularisation à l’estimation MAP\n\nDériver la régression ridge comme cas particulier de régularisation L2\n\nImplémenter la validation croisée pour le choix d’hyperparamètres\n\nÉnoncer et prouver une borne de généralisation PAC\n\nAppliquer les inégalités de Hoeffding et de Boole","type":"content","url":"/generalization","position":1},{"hierarchy":{"lvl1":"Généralisation","lvl2":"Le problème de la généralisation"},"type":"lvl2","url":"/generalization#le-probl-me-de-la-g-n-ralisation","position":2},{"hierarchy":{"lvl1":"Généralisation","lvl2":"Le problème de la généralisation"},"content":"L’objectif de l’apprentissage n’est pas de minimiser l’erreur sur les données d’entraînement, mais de faire de bonnes prédictions sur de nouvelles données. Cette capacité à bien performer au-delà des exemples observés est la généralisation.\n\nRappelons que le risque est défini comme l’espérance de la perte sur la distribution des données:\\mathcal{R}(\\theta) = \\mathbb{E}_{(X,Y) \\sim p}\\left[\\ell(Y, f(X; \\theta))\\right]\n\nLe risque empirique, calculé sur un ensemble de données \\mathcal{D}, est:\\hat{\\mathcal{R}}(\\theta; \\mathcal{D}) = \\frac{1}{|\\mathcal{D}|} \\sum_{i=1}^{|\\mathcal{D}|} \\ell(y_i, f(x_i; \\theta))\n\nL’écart de généralisation est la différence entre ces deux quantités:\\mathcal{R}(\\theta) - \\hat{\\mathcal{R}}(\\theta; \\mathcal{D}_{\\text{train}})\n\nUn modèle qui minimise parfaitement le risque empirique peut avoir un écart de généralisation important. L’erreur d’entraînement est alors un estimateur trop optimiste du risque de la population.","type":"content","url":"/generalization#le-probl-me-de-la-g-n-ralisation","position":3},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Surapprentissage et sous-apprentissage","lvl2":"Le problème de la généralisation"},"type":"lvl3","url":"/generalization#surapprentissage-et-sous-apprentissage","position":4},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Surapprentissage et sous-apprentissage","lvl2":"Le problème de la généralisation"},"content":"Le surapprentissage (en anglais overfitting) se produit lorsque le modèle mémorise les particularités des données d’entraînement plutôt que d’apprendre les régularités sous-jacentes. L’erreur d’entraînement est faible, mais l’erreur sur de nouvelles données est élevée.\n\nLe sous-apprentissage (en anglais underfitting) est le problème inverse: le modèle est trop simple pour capturer la structure des données. Les erreurs d’entraînement et de test sont toutes deux élevées.\n\nConsidérons la régression polynomiale comme exemple. Soit f(x; w) = \\sum_{d=0}^D w_d x^d un polynôme de degré D. Si D = 1, le modèle ne peut représenter que des droites, ce qui est insuffisant si la relation entre x et y est non linéaire. À l’inverse, si D = N - 1 où N est le nombre d’exemples, nous avons autant de paramètres que d’observations et pouvons interpoler parfaitement les données. L’erreur d’entraînement atteint zéro, mais le polynôme oscille violemment entre les points et généralise mal.\n\nL’erreur de test suit typiquement une courbe en U en fonction de la complexité du modèle. Elle est élevée pour les modèles trop simples (sous-apprentissage), diminue jusqu’à un minimum optimal, puis augmente à nouveau pour les modèles trop complexes (surapprentissage).","type":"content","url":"/generalization#surapprentissage-et-sous-apprentissage","position":5},{"hierarchy":{"lvl1":"Généralisation","lvl2":"Régularisation"},"type":"lvl2","url":"/generalization#r-gularisation","position":6},{"hierarchy":{"lvl1":"Généralisation","lvl2":"Régularisation"},"content":"La régularisation combat le surapprentissage en pénalisant la complexité du modèle. Plutôt que de minimiser uniquement le risque empirique, nous minimisons une combinaison du risque empirique et d’un terme de pénalité:\\hat{\\mathcal{R}}_{\\lambda}(\\theta) = \\frac{1}{N} \\sum_{n=1}^{N} \\ell(y_n, f(x_n; \\theta)) + \\lambda C(\\theta)\n\nLe terme C(\\theta) mesure la complexité du modèle, et le coefficient de régularisation \\lambda \\geq 0 contrôle l’importance relative de cette pénalité. Cette approche découle du principe de minimisation du risque structurel (en anglais structural risk minimization).","type":"content","url":"/generalization#r-gularisation","position":7},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Régularisation et estimation MAP","lvl2":"Régularisation"},"type":"lvl3","url":"/generalization#r-gularisation-et-estimation-map","position":8},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Régularisation et estimation MAP","lvl2":"Régularisation"},"content":"La régularisation a une interprétation probabiliste naturelle. Si nous choisissons C(\\theta) = -\\log p(\\theta) où p(\\theta) est une distribution a priori, le risque régularisé devient:\\hat{\\mathcal{R}}_{\\lambda}(\\theta) = -\\frac{1}{N} \\sum_{n=1}^{N} \\log p(y_n | x_n, \\theta) - \\lambda \\log p(\\theta)\n\nPour \\lambda = 1 et en ignorant le facteur 1/N, minimiser cette expression équivaut à maximiser le log a posteriori:\\log p(\\theta | \\mathcal{D}) = \\log p(\\mathcal{D} | \\theta) + \\log p(\\theta) - \\text{constante}\n\nLe minimiseur du risque empirique régularisé coïncide donc avec l’estimateur du maximum a posteriori (MAP):\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_{\\theta} \\log p(\\theta | \\mathcal{D}) = \\arg\\max_{\\theta} \\left[\\log p(\\mathcal{D} | \\theta) + \\log p(\\theta)\\right]\n\nL’a priori p(\\theta) encode nos croyances sur les valeurs plausibles des paramètres avant d’observer les données. La régularisation incorpore cette connaissance préalable dans l’apprentissage.","type":"content","url":"/generalization#r-gularisation-et-estimation-map","position":9},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Statistique bayésienne","lvl2":"Régularisation"},"type":"lvl3","url":"/generalization#statistique-bay-sienne","position":10},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Statistique bayésienne","lvl2":"Régularisation"},"content":"L’estimation MAP provient de la statistique bayésienne, qui s’intéresse à caractériser l’incertitude sur les paramètres. Le théorème de Bayes relie l’a posteriori à la vraisemblance et l’a priori:p(\\theta | \\mathcal{D}) = \\frac{p(\\theta) p(\\mathcal{D} | \\theta)}{p(\\mathcal{D})} = \\frac{p(\\theta) p(\\mathcal{D} | \\theta)}{\\int p(\\theta') p(\\mathcal{D} | \\theta') d\\theta'}\n\noù:\n\np(\\theta | \\mathcal{D}) est la distribution a posteriori\n\np(\\theta) est la distribution a priori\n\np(\\mathcal{D} | \\theta) est la vraisemblance\n\np(\\mathcal{D}) est la vraisemblance marginale (ou évidence)\n\nL’approche bayésienne complète utilise la distribution prédictive a posteriori:p(y | x, \\mathcal{D}) = \\int p(y | x, \\theta) p(\\theta | \\mathcal{D}) d\\theta\n\nCette distribution moyenne les prédictions sur tous les modèles possibles, pondérés par leur probabilité a posteriori. L’estimation MAP est une approximation qui utilise uniquement le mode de l’a posteriori.","type":"content","url":"/generalization#statistique-bay-sienne","position":11},{"hierarchy":{"lvl1":"Généralisation","lvl2":"Régularisation L2 et régression ridge"},"type":"lvl2","url":"/generalization#r-gularisation-l2-et-r-gression-ridge","position":12},{"hierarchy":{"lvl1":"Généralisation","lvl2":"Régularisation L2 et régression ridge"},"content":"Un choix courant d’a priori est une gaussienne centrée à zéro:p(w) = \\mathcal{N}(w | 0, \\tau^2 I)\n\nCet a priori favorise les paramètres de petite magnitude. Le log a priori est proportionnel à la norme L2 au carré:\\log p(w) \\propto -\\frac{1}{2\\tau^2} \\|w\\|_2^2\n\nL’estimateur MAP devient:\\hat{w}_{\\text{MAP}} = \\arg\\min_{w} \\text{NLL}(w) + \\lambda \\|w\\|_2^2\n\noù \\|w\\|_2^2 = \\sum_{d=1}^D w_d^2. Cette pénalisation est appelée régularisation L2 ou dégradation des pondérations (en anglais weight decay).","type":"content","url":"/generalization#r-gularisation-l2-et-r-gression-ridge","position":13},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Régression ridge","lvl2":"Régularisation L2 et régression ridge"},"type":"lvl3","url":"/generalization#r-gression-ridge","position":14},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Régression ridge","lvl2":"Régularisation L2 et régression ridge"},"content":"Dans le contexte de la régression linéaire, la régularisation L2 donne la régression ridge (ou régression par crêtes). L’objectif est:\\hat{w}_{\\text{ridge}} = \\arg\\min_{w} \\|Xw - y\\|_2^2 + \\lambda \\|w\\|_2^2\n\nLa solution analytique s’obtient en annulant le gradient:\\nabla_w \\left[\\|Xw - y\\|_2^2 + \\lambda \\|w\\|_2^2\\right] = 2X^\\top(Xw - y) + 2\\lambda w = 0\n\nCe qui donne:\\hat{w}_{\\text{ridge}} = (X^\\top X + \\lambda I)^{-1} X^\\top y\n\nComparée à la solution des moindres carrés ordinaires (X^\\top X)^{-1} X^\\top y, la régression ridge ajoute \\lambda I à la matrice X^\\top X. Cet ajout a plusieurs effets bénéfiques:\n\nLa matrice devient inversible même si X^\\top X est singulière\n\nLes valeurs propres sont augmentées de \\lambda, améliorant la stabilité numérique\n\nLes coefficients sont “rétrécis” vers zéro, réduisant la variance","type":"content","url":"/generalization#r-gression-ridge","position":15},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Exemple: régularisation L2 pour Bernoulli","lvl2":"Régularisation L2 et régression ridge"},"type":"lvl3","url":"/generalization#exemple-r-gularisation-l2-pour-bernoulli","position":16},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Exemple: régularisation L2 pour Bernoulli","lvl2":"Régularisation L2 et régression ridge"},"content":"Considérons l’estimation du paramètre d’une loi de Bernoulli. Avec seulement trois observations “face” (N_1 = 3, N_0 = 0), l’EMV donne:\\hat{\\theta}_{\\text{MLE}} = \\frac{N_1}{N_0 + N_1} = \\frac{3}{3 + 0} = 1\n\nCe modèle prédit que l’événement “pile” est impossible, ce qui est peu plausible intuitivement.\n\nUtilisons plutôt l’estimateur MAP avec un a priori Beta:p(\\theta) = \\text{Beta}(\\theta | a, b)\n\nPour a, b > 1, cet a priori favorise des valeurs de \\theta proches de a/(a+b). Le log a posteriori est:\\ell(\\theta) = \\left[N_1 \\log \\theta + N_0 \\log(1-\\theta)\\right] + \\left[(a-1) \\log \\theta + (b-1) \\log(1-\\theta)\\right]\n\nEn annulant la dérivée, nous obtenons:\\hat{\\theta}_{\\text{MAP}} = \\frac{N_1 + a - 1}{N_1 + N_0 + a + b - 2}\n\nLe choix a = b = 2 favorise les valeurs proches de 0.5 et donne:\\hat{\\theta}_{\\text{MAP}} = \\frac{N_1 + 1}{N_1 + N_0 + 2} = \\frac{3 + 1}{3 + 0 + 2} = 0.8\n\nCette technique est le lissage de Laplace (ou add-one smoothing), particulièrement utile lorsque certains événements n’ont jamais été observés.","type":"content","url":"/generalization#exemple-r-gularisation-l2-pour-bernoulli","position":17},{"hierarchy":{"lvl1":"Généralisation","lvl2":"Choix des hyperparamètres"},"type":"lvl2","url":"/generalization#choix-des-hyperparam-tres","position":18},{"hierarchy":{"lvl1":"Généralisation","lvl2":"Choix des hyperparamètres"},"content":"Le coefficient de régularisation \\lambda est un hyperparamètre: il n’est pas appris par minimisation du risque empirique mais doit être choisi autrement. Une grande valeur de \\lambda met l’emphase sur la proximité à l’a priori, ce qui peut causer du sous-apprentissage. Une petite valeur favorise la minimisation du risque empirique, risquant le surapprentissage.","type":"content","url":"/generalization#choix-des-hyperparam-tres","position":19},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Ensemble de validation","lvl2":"Choix des hyperparamètres"},"type":"lvl3","url":"/generalization#ensemble-de-validation","position":20},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Ensemble de validation","lvl2":"Choix des hyperparamètres"},"content":"La méthode standard consiste à diviser les données en trois parties:\n\nEnsemble d’entraînement \\mathcal{D}_{\\text{train}}: utilisé pour apprendre les paramètres\n\nEnsemble de validation \\mathcal{D}_{\\text{valid}}: utilisé pour choisir les hyperparamètres\n\nEnsemble de test \\mathcal{D}_{\\text{test}}: utilisé une seule fois pour l’évaluation finale\n\nTypiquement, nous gardons 60-80% des données pour l’entraînement, 10-20% pour la validation, et 10-20% pour le test.\n\nLe risque de validation est:\\hat{\\mathcal{R}}^{\\text{val}}_{\\lambda} = \\hat{\\mathcal{R}}_0\\left(\\hat{\\theta}_{\\lambda}(\\mathcal{D}_{\\text{train}}), \\mathcal{D}_{\\text{valid}}\\right)\n\noù \\hat{\\theta}_{\\lambda}(\\mathcal{D}_{\\text{train}}) sont les paramètres appris sur l’ensemble d’entraînement avec régularisation \\lambda, et l’évaluation se fait sans régularisation (\\lambda = 0) sur l’ensemble de validation.","type":"content","url":"/generalization#ensemble-de-validation","position":21},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Recherche par quadrillage","lvl2":"Choix des hyperparamètres"},"type":"lvl3","url":"/generalization#recherche-par-quadrillage","position":22},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Recherche par quadrillage","lvl2":"Choix des hyperparamètres"},"content":"La recherche par quadrillage (en anglais grid search) évalue systématiquement un ensemble discret d’hyperparamètres. Les valeurs sont souvent choisies sur une échelle logarithmique, par exemple \\lambda \\in \\{10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1, 10\\}.\n\nRecherche par quadrillage\n\nEntrée: Ensemble d’hyperparamètres candidats \\mathcal{S} = \\{\\lambda_0, \\ldots, \\lambda_K\\}\n\nSortie: Meilleur hyperparamètre \\lambda^*\n\nPour chaque \\lambda \\in \\mathcal{S}:\n\nEntraîner le modèle: \\hat{\\theta}_{\\lambda} = \\arg\\min_{\\theta} \\hat{\\mathcal{R}}_{\\lambda}(\\theta, \\mathcal{D}_{\\text{train}})\n\nÉvaluer sur la validation: v_{\\lambda} = \\hat{\\mathcal{R}}_0(\\hat{\\theta}_{\\lambda}, \\mathcal{D}_{\\text{valid}})\n\nRetourner \\lambda^* = \\arg\\min_{\\lambda \\in \\mathcal{S}} v_{\\lambda}\n\nUne fois \\lambda^* choisi, nous réentraînons le modèle sur l’union \\mathcal{D}_{\\text{train}} \\cup \\mathcal{D}_{\\text{valid}} pour obtenir les paramètres finaux.","type":"content","url":"/generalization#recherche-par-quadrillage","position":23},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Validation croisée","lvl2":"Choix des hyperparamètres"},"type":"lvl3","url":"/generalization#validation-crois-e","position":24},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Validation croisée","lvl2":"Choix des hyperparamètres"},"content":"Lorsque les données sont limitées, réserver un ensemble de validation réduit significativement les exemples disponibles pour l’entraînement. La validation croisée (en anglais cross-validation) offre une alternative.\n\nDans la validation croisée à K blocs, nous partitionnons les données en K sous-ensembles de taille approximativement égale. Pour chaque bloc k:\n\nNous entraînons le modèle sur tous les blocs sauf le k-ième\n\nNous évaluons sur le bloc k\n\nLe risque de validation croisée est la moyenne des erreurs sur les K blocs:\\hat{\\mathcal{R}}^{\\text{cv}}_{\\lambda} = \\frac{1}{K} \\sum_{k=1}^{K} \\hat{\\mathcal{R}}_0\\left(\\hat{\\theta}_{\\lambda}(\\mathcal{D}_{-k}), \\mathcal{D}_k\\right)\n\noù \\mathcal{D}_{-k} désigne toutes les données sauf le bloc k.\n\nLe cas particulier K = N (un bloc par exemple) est la validation croisée tout-sauf-un (en anglais leave-one-out cross-validation, LOOCV). Cette méthode utilise le maximum de données pour l’entraînement mais est coûteuse en calcul.\n\nValidation croisée à K blocs\n\nEntrée: Données \\mathcal{D}, nombre de blocs K\n\nSortie: Estimation du risque\n\nPartitionner \\mathcal{D} en K blocs: \\mathcal{D}_1, \\ldots, \\mathcal{D}_K\n\nPour chaque k = 1, \\ldots, K:\n\nFormer l’ensemble d’entraînement \\mathcal{D}_{-k} = \\mathcal{D} \\setminus \\mathcal{D}_k\n\nEntraîner le modèle: \\hat{\\theta}_k = \\arg\\min_{\\theta} \\hat{\\mathcal{R}}(\\theta, \\mathcal{D}_{-k})\n\nÉvaluer sur le bloc k: e_k = \\hat{\\mathcal{R}}(\\hat{\\theta}_k, \\mathcal{D}_k)\n\nRetourner \\bar{e} = \\frac{1}{K} \\sum_{k=1}^{K} e_k\n\nLes valeurs K = 5 et K = 10 sont couramment utilisées. Elles offrent un bon compromis entre le biais de l’estimateur (qui diminue avec K) et sa variance (qui augmente avec K).","type":"content","url":"/generalization#validation-crois-e","position":25},{"hierarchy":{"lvl1":"Généralisation","lvl2":"Effet de la taille de l’ensemble d’entraînement"},"type":"lvl2","url":"/generalization#effet-de-la-taille-de-lensemble-dentra-nement","position":26},{"hierarchy":{"lvl1":"Généralisation","lvl2":"Effet de la taille de l’ensemble d’entraînement"},"content":"Pour un modèle de complexité fixe, l’augmentation de la taille de l’ensemble d’entraînement réduit les chances de surapprentissage. Avec plus de données, le risque empirique devient un meilleur estimateur du vrai risque.\n\nIntuitivement, si nous avons peu de données, un modèle flexible peut facilement les mémoriser. Avec beaucoup de données, la seule façon d’obtenir une faible erreur d’entraînement est de capturer les vraies régularités.\n\nLa courbe d’apprentissage, qui trace l’erreur en fonction de la taille de l’ensemble d’entraînement, est un outil diagnostique utile. Si l’erreur d’entraînement et l’erreur de validation sont toutes deux élevées, le modèle est en sous-apprentissage. Si l’écart entre les deux erreurs est grand, le modèle est en surapprentissage et bénéficierait de plus de données ou de plus de régularisation.","type":"content","url":"/generalization#effet-de-la-taille-de-lensemble-dentra-nement","position":27},{"hierarchy":{"lvl1":"Généralisation","lvl2":"Théorie de l’apprentissage statistique"},"type":"lvl2","url":"/generalization#th-orie-de-lapprentissage-statistique","position":28},{"hierarchy":{"lvl1":"Généralisation","lvl2":"Théorie de l’apprentissage statistique"},"content":"La théorie de l’apprentissage statistique fournit des garanties formelles sur la généralisation. Ces résultats quantifient quand et pourquoi la minimisation du risque empirique mène à un faible risque de population.","type":"content","url":"/generalization#th-orie-de-lapprentissage-statistique","position":29},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Apprentissage PAC","lvl2":"Théorie de l’apprentissage statistique"},"type":"lvl3","url":"/generalization#apprentissage-pac","position":30},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Apprentissage PAC","lvl2":"Théorie de l’apprentissage statistique"},"content":"Un problème est dit PAC-apprenable (pour Probably Approximately Correct) s’il existe un algorithme qui, avec haute probabilité, trouve une hypothèse approximativement correcte. Formellement, pour tout \\epsilon > 0 (précision) et \\delta > 0 (confiance), l’algorithme doit produire une hypothèse h telle que:\\mathbb{P}\\left[\\mathcal{R}(h) \\leq \\min_{h' \\in \\mathcal{H}} \\mathcal{R}(h') + \\epsilon\\right] \\geq 1 - \\delta\n\nen utilisant un nombre polynomial d’exemples en 1/\\epsilon et 1/\\delta.","type":"content","url":"/generalization#apprentissage-pac","position":31},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Borne de généralisation","lvl2":"Théorie de l’apprentissage statistique"},"type":"lvl3","url":"/generalization#borne-de-g-n-ralisation","position":32},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Borne de généralisation","lvl2":"Théorie de l’apprentissage statistique"},"content":"Le théorème suivant donne une borne sur la probabilité que l’écart de généralisation dépasse un seuil donné.\n\nBorne de généralisation pour une classe finie\n\nSoit \\mathcal{H} une classe d’hypothèses de dimension finie |\\mathcal{H}|. Pour toute distribution p^* sur les données et tout ensemble \\mathcal{D} de taille N échantillonné i.i.d. de p^*, la probabilité que l’écart de généralisation d’un classifieur binaire dépasse \\epsilon est bornée par:\\mathbb{P}\\left(\\max_{h \\in \\mathcal{H}} |\\mathcal{R}(h) - \\hat{\\mathcal{R}}(h, \\mathcal{D})| > \\epsilon\\right) \\leq 2 |\\mathcal{H}| e^{-2N\\epsilon^2}\n\nCette borne nous dit que:\n\nL’écart de généralisation augmente avec la taille de la classe d’hypothèses |\\mathcal{H}|\n\nL’écart de généralisation diminue avec le nombre d’exemples N","type":"content","url":"/generalization#borne-de-g-n-ralisation","position":33},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Preuve","lvl2":"Théorie de l’apprentissage statistique"},"type":"lvl3","url":"/generalization#preuve","position":34},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Preuve","lvl2":"Théorie de l’apprentissage statistique"},"content":"La preuve utilise deux inégalités fondamentales.\n\nInégalité de Hoeffding. Si E_1, \\ldots, E_N \\sim \\text{Ber}(\\theta) sont des variables aléatoires i.i.d., alors pour tout \\epsilon > 0:\\mathbb{P}(|\\bar{E} - \\theta| > \\epsilon) \\leq 2e^{-2N\\epsilon^2}\n\noù \\bar{E} = \\frac{1}{N} \\sum_{i=1}^{N} E_i est la moyenne empirique.\n\nCette inégalité borne la probabilité que la moyenne d’un échantillon s’éloigne de la vraie moyenne d’une quantité \\epsilon.\n\nInégalité de Boole (union bound). Pour des événements A_1, \\ldots, A_d:\\mathbb{P}\\left(\\bigcup_{i=1}^{d} A_i\\right) \\leq \\sum_{i=1}^{d} \\mathbb{P}(A_i)\n\nEn combinant ces inégalités:\\begin{aligned}\n\\mathbb{P}\\left(\\max_{h \\in \\mathcal{H}} |\\mathcal{R}(h) - \\hat{\\mathcal{R}}(h, \\mathcal{D})| > \\epsilon\\right) \n&= \\mathbb{P}\\left(\\bigcup_{h \\in \\mathcal{H}} \\{|\\mathcal{R}(h) - \\hat{\\mathcal{R}}(h, \\mathcal{D})| > \\epsilon\\}\\right) \\\\\n&\\leq \\sum_{h \\in \\mathcal{H}} \\mathbb{P}(|\\mathcal{R}(h) - \\hat{\\mathcal{R}}(h, \\mathcal{D})| > \\epsilon) \\\\\n&\\leq \\sum_{h \\in \\mathcal{H}} 2e^{-2N\\epsilon^2} \\\\\n&= 2|\\mathcal{H}| e^{-2N\\epsilon^2}\n\\end{aligned}\n\nLa première inégalité utilise l’union bound. La deuxième applique Hoeffding à chaque hypothèse individuellement, en notant que le risque empirique est la moyenne de pertes i.i.d.","type":"content","url":"/generalization#preuve","position":35},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Interprétation","lvl2":"Théorie de l’apprentissage statistique"},"type":"lvl3","url":"/generalization#interpr-tation","position":36},{"hierarchy":{"lvl1":"Généralisation","lvl3":"Interprétation","lvl2":"Théorie de l’apprentissage statistique"},"content":"Cette borne peut être réarrangée pour donner le nombre d’exemples nécessaires. Si nous voulons que l’écart de généralisation soit au plus \\epsilon avec probabilité au moins 1 - \\delta, nous posons:2|\\mathcal{H}| e^{-2N\\epsilon^2} \\leq \\delta\n\nEn résolvant pour N:N \\geq \\frac{1}{2\\epsilon^2} \\ln\\left(\\frac{2|\\mathcal{H}|}{\\delta}\\right)\n\nLa complexité de l’échantillon croît logarithmiquement avec la taille de la classe d’hypothèses, mais polynomialement avec 1/\\epsilon.","type":"content","url":"/generalization#interpr-tation","position":37},{"hierarchy":{"lvl1":"Généralisation","lvl2":"Résumé"},"type":"lvl2","url":"/generalization#r-sum","position":38},{"hierarchy":{"lvl1":"Généralisation","lvl2":"Résumé"},"content":"Ce chapitre a traité de la généralisation, le concept central de l’apprentissage machine:\n\nLe surapprentissage et le sous-apprentissage sont les deux écueils à éviter. Le premier survient avec des modèles trop complexes, le second avec des modèles trop simples.\n\nLa régularisation pénalise la complexité du modèle. Elle a une interprétation probabiliste comme estimation MAP avec un a priori sur les paramètres.\n\nLa régularisation L2 (dégradation des pondérations) correspond à un a priori gaussien. En régression linéaire, elle donne la régression ridge.\n\nLa validation croisée permet de choisir les hyperparamètres en réutilisant les données de manière efficace.\n\nLa théorie de l’apprentissage statistique fournit des bornes sur l’écart de généralisation. Ces bornes dépendent de la taille de la classe d’hypothèses et du nombre d’exemples.\n\nLe prochain chapitre étudie les méthodes non paramétriques, en commençant par les k plus proches voisins.","type":"content","url":"/generalization#r-sum","position":39},{"hierarchy":{"lvl1":"Modèles génératifs"},"type":"lvl1","url":"/generative-models","position":0},{"hierarchy":{"lvl1":"Modèles génératifs"},"content":"Objectifs d’apprentissage\n\nÀ la fin de ce chapitre, vous serez en mesure de:\n\nDistinguer les approches générative et discriminative\n\nDériver l’estimateur du maximum de vraisemblance pour le classifieur naïf bayésien\n\nAppliquer l’analyse discriminante gaussienne (LDA et QDA)\n\nCalculer les estimateurs EMV pour ces modèles\n\nExpliquer les avantages et inconvénients des modèles génératifs","type":"content","url":"/generative-models","position":1},{"hierarchy":{"lvl1":"Modèles génératifs","lvl2":"Approches générative vs discriminative"},"type":"lvl2","url":"/generative-models#approches-g-n-rative-vs-discriminative","position":2},{"hierarchy":{"lvl1":"Modèles génératifs","lvl2":"Approches générative vs discriminative"},"content":"","type":"content","url":"/generative-models#approches-g-n-rative-vs-discriminative","position":3},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Rappel: le théorème de Bayes","lvl2":"Approches générative vs discriminative"},"type":"lvl3","url":"/generative-models#rappel-le-th-or-me-de-bayes","position":4},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Rappel: le théorème de Bayes","lvl2":"Approches générative vs discriminative"},"content":"Pour classifier un exemple \\boldsymbol{x}, nous voulons calculer la probabilité a posteriori:p(y = c \\mid \\boldsymbol{x}; \\boldsymbol{\\theta}) = \\frac{p(\\boldsymbol{x} \\mid y = c; \\boldsymbol{\\theta}) \\, p(y = c; \\boldsymbol{\\theta})}{\\sum_{c'} p(\\boldsymbol{x} \\mid y = c'; \\boldsymbol{\\theta}) \\, p(y = c'; \\boldsymbol{\\theta})}\n\noù:\n\np(\\boldsymbol{x} \\mid y = c; \\boldsymbol{\\theta}) est la vraisemblance (densité conditionnelle de classe)\n\np(y = c; \\boldsymbol{\\theta}) est l’a priori sur les classes","type":"content","url":"/generative-models#rappel-le-th-or-me-de-bayes","position":5},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Modèles génératifs","lvl2":"Approches générative vs discriminative"},"type":"lvl3","url":"/generative-models#mod-les-g-n-ratifs","position":6},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Modèles génératifs","lvl2":"Approches générative vs discriminative"},"content":"Dans l’approche générative, nous modélisons explicitement:\n\nLa distribution des entrées pour chaque classe: p(\\boldsymbol{x} \\mid y = c)\n\nLa distribution a priori sur les classes: p(y = c)\n\nLe nom “génératif” vient du fait que nous pouvons générer de nouvelles données:\n\nÉchantillonner une classe c \\sim p(y)\n\nÉchantillonner une observation \\boldsymbol{x} \\sim p(\\boldsymbol{x} \\mid y = c)","type":"content","url":"/generative-models#mod-les-g-n-ratifs","position":7},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Modèles discriminatifs","lvl2":"Approches générative vs discriminative"},"type":"lvl3","url":"/generative-models#mod-les-discriminatifs","position":8},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Modèles discriminatifs","lvl2":"Approches générative vs discriminative"},"content":"Dans l’approche discriminative, nous modélisons directement:p(y = c \\mid \\boldsymbol{x}; \\boldsymbol{\\theta})\n\nsans passer par p(\\boldsymbol{x} \\mid y). Exemples: régression logistique, SVM, réseaux de neurones.","type":"content","url":"/generative-models#mod-les-discriminatifs","position":9},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Comparaison","lvl2":"Approches générative vs discriminative"},"type":"lvl3","url":"/generative-models#comparaison","position":10},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Comparaison","lvl2":"Approches générative vs discriminative"},"content":"Aspect\n\nGénératif\n\nDiscriminatif\n\nCe qui est modélisé\n\np(\\boldsymbol{x}, y) ou p(\\boldsymbol{x} \\mid y) p(y)\n\np(y \\mid \\boldsymbol{x})\n\nPeut générer des données\n\nOui\n\nNon\n\nDonnées manquantes\n\nPeut les gérer\n\nDifficile\n\nAjout de classes\n\nPossible séparément\n\nNécessite réentraînement\n\nPrécision prédictive\n\nSouvent inférieure\n\nSouvent supérieure\n\nHypothèses\n\nPlus fortes\n\nPlus faibles","type":"content","url":"/generative-models#comparaison","position":11},{"hierarchy":{"lvl1":"Modèles génératifs","lvl2":"Classifieur naïf bayésien"},"type":"lvl2","url":"/generative-models#classifieur-na-f-bay-sien","position":12},{"hierarchy":{"lvl1":"Modèles génératifs","lvl2":"Classifieur naïf bayésien"},"content":"","type":"content","url":"/generative-models#classifieur-na-f-bay-sien","position":13},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Hypothèse d’indépendance conditionnelle","lvl2":"Classifieur naïf bayésien"},"type":"lvl3","url":"/generative-models#hypoth-se-dind-pendance-conditionnelle","position":14},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Hypothèse d’indépendance conditionnelle","lvl2":"Classifieur naïf bayésien"},"content":"Le classifieur naïf bayésien (Naive Bayes) suppose que les caractéristiques sont conditionnellement indépendantes étant donné la classe:p(\\boldsymbol{x} \\mid y = c, \\boldsymbol{\\theta}) = \\prod_{d=1}^D p(x_d \\mid y = c, \\boldsymbol{\\theta}_{dc})\n\nCette hypothèse est “naïve” car elle est rarement vraie en pratique, mais le classifieur fonctionne souvent bien malgré tout.","type":"content","url":"/generative-models#hypoth-se-dind-pendance-conditionnelle","position":15},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"A posteriori","lvl2":"Classifieur naïf bayésien"},"type":"lvl3","url":"/generative-models#a-posteriori","position":16},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"A posteriori","lvl2":"Classifieur naïf bayésien"},"content":"Le a posteriori sur les classes devient:p(y = c \\mid \\boldsymbol{x}, \\boldsymbol{\\theta}) = \\frac{p(y = c \\mid \\boldsymbol{\\pi}) \\prod_{d=1}^D p(x_d \\mid y = c, \\boldsymbol{\\theta}_{dc})}{\\sum_{c'} p(y = c' \\mid \\boldsymbol{\\pi}) \\prod_{d=1}^D p(x_d \\mid y = c', \\boldsymbol{\\theta}_{dc'})}\n\noù \\pi_c = p(y = c) est l’a priori de classe, et les paramètres sont \\boldsymbol{\\theta} = (\\boldsymbol{\\pi}, \\{\\boldsymbol{\\theta}_{dc}\\}).","type":"content","url":"/generative-models#a-posteriori","position":17},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"EMV pour Naive Bayes","lvl2":"Classifieur naïf bayésien"},"type":"lvl3","url":"/generative-models#emv-pour-naive-bayes","position":18},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"EMV pour Naive Bayes","lvl2":"Classifieur naïf bayésien"},"content":"La vraisemblance des données s’écrit:p(\\mathcal{D} \\mid \\boldsymbol{\\theta}) = \\prod_{n=1}^N \\text{Cat}(y_n \\mid \\boldsymbol{\\pi}) \\prod_{d=1}^D p(x_{nd} \\mid y_n, \\boldsymbol{\\theta}_d)\n\nLa log-vraisemblance se factorise:\\log p(\\mathcal{D} \\mid \\boldsymbol{\\theta}) = \\log p(\\mathcal{D}_y \\mid \\boldsymbol{\\pi}) + \\sum_c \\sum_d \\log p(\\mathcal{D}_{dc} \\mid \\boldsymbol{\\theta}_{dc})\n\noù \\mathcal{D}_y = \\{y_n\\} sont les étiquettes et \\mathcal{D}_{dc} = \\{x_{nd} : y_n = c\\} sont les valeurs du trait d pour les exemples de classe c.\n\nCela permet d’optimiser chaque terme séparément.","type":"content","url":"/generative-models#emv-pour-naive-bayes","position":19},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Cas des caractéristiques catégorielles","lvl2":"Classifieur naïf bayésien"},"type":"lvl3","url":"/generative-models#cas-des-caract-ristiques-cat-gorielles","position":20},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Cas des caractéristiques catégorielles","lvl2":"Classifieur naïf bayésien"},"content":"Si les traits prennent des valeurs discrètes parmi K possibilités, l’EMV est:\\hat{\\theta}_{dck} = \\frac{N_{dck}}{N_c}\n\noù N_{dck} = \\sum_{n=1}^N \\mathbb{I}(x_{nd} = k, y_n = c) est le nombre de fois que le trait d vaut k parmi les exemples de classe c.","type":"content","url":"/generative-models#cas-des-caract-ristiques-cat-gorielles","position":21},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Cas des caractéristiques binaires","lvl2":"Classifieur naïf bayésien"},"type":"lvl3","url":"/generative-models#cas-des-caract-ristiques-binaires","position":22},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Cas des caractéristiques binaires","lvl2":"Classifieur naïf bayésien"},"content":"Si les traits sont binaires (présent/absent), on utilise la loi de Bernoulli:\\hat{\\theta}_{dc} = \\frac{N_{dc}}{N_c}\n\noù N_{dc} est le nombre de fois que le trait d est présent dans la classe c.","type":"content","url":"/generative-models#cas-des-caract-ristiques-binaires","position":23},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Lissage de Laplace","lvl2":"Classifieur naïf bayésien"},"type":"lvl3","url":"/generative-models#lissage-de-laplace","position":24},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Lissage de Laplace","lvl2":"Classifieur naïf bayésien"},"content":"Un problème survient si un trait n’est jamais observé avec une certaine classe: N_{dc} = 0 implique \\hat{\\theta}_{dc} = 0, ce qui rend p(\\boldsymbol{x} \\mid y = c) = 0 même si un seul trait est absent.\n\nLe lissage de Laplace (add-one smoothing) résout ce problème:\\hat{\\theta}_{dc} = \\frac{N_{dc} + 1}{N_c + K}\n\noù K est le nombre de valeurs possibles. Cela correspond à un a priori uniforme dans le cadre MAP.","type":"content","url":"/generative-models#lissage-de-laplace","position":25},{"hierarchy":{"lvl1":"Modèles génératifs","lvl2":"Analyse discriminante gaussienne"},"type":"lvl2","url":"/generative-models#analyse-discriminante-gaussienne","position":26},{"hierarchy":{"lvl1":"Modèles génératifs","lvl2":"Analyse discriminante gaussienne"},"content":"","type":"content","url":"/generative-models#analyse-discriminante-gaussienne","position":27},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Modèle","lvl2":"Analyse discriminante gaussienne"},"type":"lvl3","url":"/generative-models#mod-le","position":28},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Modèle","lvl2":"Analyse discriminante gaussienne"},"content":"L’analyse discriminante gaussienne (GDA, Gaussian Discriminant Analysis) suppose que les densités conditionnelles de classe sont gaussiennes:p(\\boldsymbol{x} \\mid y = c, \\boldsymbol{\\theta}) = \\mathcal{N}(\\boldsymbol{x} \\mid \\boldsymbol{\\mu}_c, \\boldsymbol{\\Sigma}_c)\n\nLe a posteriori sur les classes est:p(y = c \\mid \\boldsymbol{x}, \\boldsymbol{\\theta}) \\propto \\pi_c \\mathcal{N}(\\boldsymbol{x} \\mid \\boldsymbol{\\mu}_c, \\boldsymbol{\\Sigma}_c)","type":"content","url":"/generative-models#mod-le","position":29},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Log a posteriori et fonction discriminante","lvl2":"Analyse discriminante gaussienne"},"type":"lvl3","url":"/generative-models#log-a-posteriori-et-fonction-discriminante","position":30},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Log a posteriori et fonction discriminante","lvl2":"Analyse discriminante gaussienne"},"content":"Le logarithme du a posteriori (appelé fonction discriminante) est:\\log p(y = c \\mid \\boldsymbol{x}, \\boldsymbol{\\theta}) = \\log \\pi_c - \\frac{1}{2}\\log|2\\pi\\boldsymbol{\\Sigma}_c| - \\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{\\mu}_c)^\\top \\boldsymbol{\\Sigma}_c^{-1}(\\boldsymbol{x} - \\boldsymbol{\\mu}_c) + \\text{const}\n\nLe terme quadratique (\\boldsymbol{x} - \\boldsymbol{\\mu}_c)^\\top \\boldsymbol{\\Sigma}_c^{-1}(\\boldsymbol{x} - \\boldsymbol{\\mu}_c) est la distance de Mahalanobis entre \\boldsymbol{x} et \\boldsymbol{\\mu}_c.","type":"content","url":"/generative-models#log-a-posteriori-et-fonction-discriminante","position":31},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Analyse discriminante quadratique (QDA)","lvl2":"Analyse discriminante gaussienne"},"type":"lvl3","url":"/generative-models#analyse-discriminante-quadratique-qda","position":32},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Analyse discriminante quadratique (QDA)","lvl2":"Analyse discriminante gaussienne"},"content":"Quand chaque classe a sa propre matrice de covariance \\boldsymbol{\\Sigma}_c, la frontière de décision est quadratique. Cette méthode s’appelle QDA (Quadratic Discriminant Analysis).\n\nLes lignes de niveau sont des ellipsoïdes dont l’orientation et les axes sont déterminés par les vecteurs propres de \\boldsymbol{\\Sigma}_c.","type":"content","url":"/generative-models#analyse-discriminante-quadratique-qda","position":33},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Analyse discriminante linéaire (LDA)","lvl2":"Analyse discriminante gaussienne"},"type":"lvl3","url":"/generative-models#analyse-discriminante-lin-aire-lda","position":34},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Analyse discriminante linéaire (LDA)","lvl2":"Analyse discriminante gaussienne"},"content":"Si toutes les classes partagent la même covariance \\boldsymbol{\\Sigma}, le terme quadratique se simplifie:\\log p(y = c \\mid \\boldsymbol{x}, \\boldsymbol{\\theta}) = \\underbrace{\\log \\pi_c - \\frac{1}{2}\\boldsymbol{\\mu}_c^\\top \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_c}_{\\gamma_c} + \\boldsymbol{x}^\\top \\underbrace{\\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\mu}_c}_{\\boldsymbol{\\beta}_c} + \\kappa\n\noù \\kappa ne dépend pas de c. La fonction discriminante est linéaire en \\boldsymbol{x}:\\log p(y = c \\mid \\boldsymbol{x}, \\boldsymbol{\\theta}) = \\gamma_c + \\boldsymbol{x}^\\top \\boldsymbol{\\beta}_c + \\kappa\n\nCette méthode s’appelle LDA (Linear Discriminant Analysis).","type":"content","url":"/generative-models#analyse-discriminante-lin-aire-lda","position":35},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Estimation des paramètres","lvl2":"Analyse discriminante gaussienne"},"type":"lvl3","url":"/generative-models#estimation-des-param-tres","position":36},{"hierarchy":{"lvl1":"Modèles génératifs","lvl3":"Estimation des paramètres","lvl2":"Analyse discriminante gaussienne"},"content":"Pour la GDA, l’EMV des paramètres est:\n\nA priori de classe:\\hat{\\pi}_c = \\frac{N_c}{N}\n\nMoyenne par classe:\\hat{\\boldsymbol{\\mu}}_c = \\frac{1}{N_c} \\sum_{n: y_n = c} \\boldsymbol{x}_n\n\nCovariance par classe (QDA):\\hat{\\boldsymbol{\\Sigma}}_c = \\frac{1}{N_c} \\sum_{n: y_n = c} (\\boldsymbol{x}_n - \\hat{\\boldsymbol{\\mu}}_c)(\\boldsymbol{x}_n - \\hat{\\boldsymbol{\\mu}}_c)^\\top\n\nCovariance partagée (LDA):\\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{N} \\sum_{c=1}^C \\sum_{n: y_n = c} (\\boldsymbol{x}_n - \\hat{\\boldsymbol{\\mu}}_c)(\\boldsymbol{x}_n - \\hat{\\boldsymbol{\\mu}}_c)^\\top","type":"content","url":"/generative-models#estimation-des-param-tres","position":37},{"hierarchy":{"lvl1":"Modèles génératifs","lvl2":"Avantages des modèles génératifs"},"type":"lvl2","url":"/generative-models#avantages-des-mod-les-g-n-ratifs","position":38},{"hierarchy":{"lvl1":"Modèles génératifs","lvl2":"Avantages des modèles génératifs"},"content":"Apprentissage parfois plus simple: Naive Bayes s’apprend par comptage et moyennes, sans optimisation itérative\n\nGestion des caractéristiques manquantes: On peut marginaliser sur les valeurs manquantes\n\nApprentissage incrémental par classe: On peut ajouter de nouvelles classes sans réentraîner les existantes\n\nApprentissage semi-supervisé: On peut utiliser des données non étiquetées pour améliorer l’estimation de p(\\boldsymbol{x})\n\nRobustesse aux caractéristiques factices: Modéliser p(\\boldsymbol{x} \\mid y) peut capturer les mécanismes causaux sous-jacents","type":"content","url":"/generative-models#avantages-des-mod-les-g-n-ratifs","position":39},{"hierarchy":{"lvl1":"Modèles génératifs","lvl2":"Inconvénients des modèles génératifs"},"type":"lvl2","url":"/generative-models#inconv-nients-des-mod-les-g-n-ratifs","position":40},{"hierarchy":{"lvl1":"Modèles génératifs","lvl2":"Inconvénients des modèles génératifs"},"content":"Hypothèses plus fortes: Le modèle génératif doit spécifier p(\\boldsymbol{x} \\mid y), ce qui est plus contraignant\n\nPrécision souvent inférieure: Les modèles discriminatifs optimisent directement ce qui nous intéresse\n\nDifficultés avec les caractéristiques prétraitées: Après extraction de caractéristiques, les nouvelles variables peuvent avoir des corrélations complexes difficiles à modéliser\n\nProbabilités mal calibrées: Les hypothèses fortes (comme l’indépendance) peuvent donner des probabilités biaisées","type":"content","url":"/generative-models#inconv-nients-des-mod-les-g-n-ratifs","position":41},{"hierarchy":{"lvl1":"Modèles génératifs","lvl2":"Résumé"},"type":"lvl2","url":"/generative-models#r-sum","position":42},{"hierarchy":{"lvl1":"Modèles génératifs","lvl2":"Résumé"},"content":"Les modèles génératifs modélisent la distribution jointe des entrées et des étiquettes:\n\nLe classifieur naïf bayésien suppose l’indépendance conditionnelle des caractéristiques\n\nL’analyse discriminante gaussienne modélise les classes par des gaussiennes\n\nLDA utilise une covariance partagée (frontière linéaire)\n\nQDA permet des covariances différentes (frontière quadratique)\n\nCes modèles sont simples à entraîner (formules fermées pour l’EMV), interprétables, et permettent de générer des données synthétiques.","type":"content","url":"/generative-models#r-sum","position":43},{"hierarchy":{"lvl1":"Modèles génératifs","lvl2":"Exercices"},"type":"lvl2","url":"/generative-models#exercices","position":44},{"hierarchy":{"lvl1":"Modèles génératifs","lvl2":"Exercices"},"content":"Exercice 1: Naive Bayes binaire\n\nUn classifieur Naive Bayes est entraîné sur des emails (spam vs non-spam) avec 2 traits binaires: présence du mot “gratuit” (x_1) et présence du mot “urgent” (x_2).\n\nDonnées d’entraînement:\n\nSpam (10 emails): 8 avec “gratuit”, 6 avec “urgent”\n\nNon-spam (20 emails): 2 avec “gratuit”, 4 avec “urgent”\n\nCalculez les EMV de tous les paramètres.\n\nClassifiez un email contenant “gratuit” mais pas “urgent”.\n\nAppliquez le lissage de Laplace et recalculez.\n\nExercice 2: LDA en 1D\n\nDeux classes en 1D:\n\nClasse 0: \\mu_0 = 0, exemples: \\{-1, 0, 1\\}\n\nClasse 1: \\mu_1 = 3, exemples: \\{2, 3, 4\\}\n\nAvec une covariance partagée \\sigma^2 = 1 et des a priori égaux:\n\nÉcrivez la fonction discriminante pour chaque classe.\n\nTrouvez le seuil de décision.\n\nClassifiez x = 1.5.\n\nExercice 3: QDA vs LDA\n\nExpliquez géométriquement pourquoi:\n\nLDA produit des frontières de décision linéaires\n\nQDA produit des frontières de décision quadratiques\n\nDonnez un exemple de données où QDA serait nettement meilleur que LDA\n\nExercice 4: Distance de Mahalanobis\n\nSoit \\boldsymbol{\\mu} = (0, 0) et \\boldsymbol{\\Sigma} = \\begin{pmatrix} 4 & 0 \\\\ 0 & 1 \\end{pmatrix}.\n\nCalculez la distance de Mahalanobis de (2, 0) à \\boldsymbol{\\mu}.\n\nCalculez la distance de Mahalanobis de (0, 2) à \\boldsymbol{\\mu}.\n\nPourquoi ces distances sont-elles différentes malgré la même distance euclidienne?","type":"content","url":"/generative-models#exercices","position":45},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes"},"type":"lvl1","url":"/graphical-models","position":0},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes"},"content":"Objectifs d’apprentissage\n\nÀ la fin de ce chapitre, vous serez en mesure de:\n\nReprésenter des distributions de probabilité avec des graphes\n\nDistinguer les modèles graphiques dirigés et non-dirigés\n\nLire l’indépendance conditionnelle à partir de la structure du graphe\n\nDécrire l’inférence et l’apprentissage dans des modèles graphiques\n\nAppliquer l’algorithme d’élimination de variables\n\nComprendre l’inférence variationnelle et l’ELBO\n\nExpliquer les auto-encodeurs variationnels (VAE)","type":"content","url":"/graphical-models","position":1},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Introduction"},"type":"lvl2","url":"/graphical-models#introduction","position":2},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Introduction"},"content":"Les modèles graphiques probabilistes (PGM) utilisent un graphe pour représenter une distribution de probabilité conjointe et ses relations d’indépendance conditionnelle. Ils fournissent un cadre unifiant pour modéliser l’incertitude et raisonner sur des systèmes complexes.","type":"content","url":"/graphical-models#introduction","position":3},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Rappels de probabilités"},"type":"lvl2","url":"/graphical-models#rappels-de-probabilit-s","position":4},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Rappels de probabilités"},"content":"","type":"content","url":"/graphical-models#rappels-de-probabilit-s","position":5},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Règles fondamentales","lvl2":"Rappels de probabilités"},"type":"lvl3","url":"/graphical-models#r-gles-fondamentales","position":6},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Règles fondamentales","lvl2":"Rappels de probabilités"},"content":"La distribution marginale d’une variable aléatoire:p(X=x) = \\sum_y p(X=x, Y=y)\n\nCette relation est aussi appelée la règle de la somme ou règle de la probabilité totale.\n\nLa probabilité conditionnelle:p(Y=y \\mid X=x) = \\frac{p(X=x, Y=y)}{p(X=x)}\n\nCe qui donne la règle du produit:p(x, y) = p(y \\mid x) p(x)","type":"content","url":"/graphical-models#r-gles-fondamentales","position":7},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Règle de la chaîne","lvl2":"Rappels de probabilités"},"type":"lvl3","url":"/graphical-models#r-gle-de-la-cha-ne","position":8},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Règle de la chaîne","lvl2":"Rappels de probabilités"},"content":"En étendant la règle du produit à D variables, nous obtenons la règle de la chaîne de probabilité:p(x_{1:D}) = p(x_1) p(x_2 \\mid x_1) p(x_3 \\mid x_1, x_2) \\cdots p(x_D \\mid x_{1:D-1})\n\nCette règle permet d’exprimer une distribution conjointe de grande dimension à partir de distributions conditionnelles.","type":"content","url":"/graphical-models#r-gle-de-la-cha-ne","position":9},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Indépendance"},"type":"lvl2","url":"/graphical-models#ind-pendance","position":10},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Indépendance"},"content":"","type":"content","url":"/graphical-models#ind-pendance","position":11},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Indépendance marginale","lvl2":"Indépendance"},"type":"lvl3","url":"/graphical-models#ind-pendance-marginale","position":12},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Indépendance marginale","lvl2":"Indépendance"},"content":"X et Y sont indépendants marginalement (noté X \\perp Y) si:X \\perp Y \\Longleftrightarrow p(X, Y) = p(X) p(Y)\n\nUn ensemble de variables X_1, \\ldots, X_n est mutuellement indépendant si pour tous les sous-ensembles \\{X_1, \\ldots, X_m\\} \\subseteq \\{X_1, \\ldots, X_n\\}:p(X_1, \\ldots, X_m) = \\prod_{i=1}^m p(X_i)","type":"content","url":"/graphical-models#ind-pendance-marginale","position":13},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Exemple","lvl2":"Indépendance"},"type":"lvl3","url":"/graphical-models#exemple","position":14},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Exemple","lvl2":"Indépendance"},"content":"Pour que X_1, X_2, X_3 soient mutuellement indépendantes, il faut:\n\np(X_1, X_2, X_3) = p(X_1) p(X_2) p(X_3)\n\np(X_1, X_2) = p(X_1) p(X_2)\n\np(X_2, X_3) = p(X_2) p(X_3)\n\np(X_1, X_3) = p(X_1) p(X_3)\n\nL’indépendance marginale est rare, car la plupart des variables peuvent influencer d’autres variables.","type":"content","url":"/graphical-models#exemple","position":15},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Indépendance conditionnelle","lvl2":"Indépendance"},"type":"lvl3","url":"/graphical-models#ind-pendance-conditionnelle","position":16},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Indépendance conditionnelle","lvl2":"Indépendance"},"content":"L’influence entre des variables se fait généralement par l’entremise d’autres variables (médiation).\n\nX et Y sont conditionnellement indépendants étant donné Z si:X \\perp Y \\mid Z \\Longleftrightarrow p(X, Y \\mid Z) = p(X \\mid Z) p(Y \\mid Z)\n\nIntuitivement: X n’informe pas Y si Z est observé (et vice versa). Toute l’information nécessaire est déjà contenue dans Z.","type":"content","url":"/graphical-models#ind-pendance-conditionnelle","position":17},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Modèles graphiques probabilistes"},"type":"lvl2","url":"/graphical-models#mod-les-graphiques-probabilistes","position":18},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Modèles graphiques probabilistes"},"content":"","type":"content","url":"/graphical-models#mod-les-graphiques-probabilistes","position":19},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Définition","lvl2":"Modèles graphiques probabilistes"},"type":"lvl3","url":"/graphical-models#d-finition","position":20},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Définition","lvl2":"Modèles graphiques probabilistes"},"content":"Un modèle graphique probabiliste (PGM) utilise un graphe pour représenter une distribution de probabilité conjointe et ses relations d’indépendance conditionnelle.\n\nDans le graphe:\n\nChaque nœud représente une variable aléatoire\n\nChaque arête représente une dépendance directe\n\nChaque absence d’arête représente une indépendance conditionnelle","type":"content","url":"/graphical-models#d-finition","position":21},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Types de modèles graphiques","lvl2":"Modèles graphiques probabilistes"},"type":"lvl3","url":"/graphical-models#types-de-mod-les-graphiques","position":22},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Types de modèles graphiques","lvl2":"Modèles graphiques probabilistes"},"content":"Type\n\nGraphe\n\nExemple\n\nRéseau bayésien\n\nGraphe acyclique dirigé (DAG)\n\nClassification naïve de Bayes\n\nChamp de Markov\n\nGraphe non-dirigé\n\nModèles de Boltzmann","type":"content","url":"/graphical-models#types-de-mod-les-graphiques","position":23},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Réseaux bayésiens"},"type":"lvl2","url":"/graphical-models#r-seaux-bay-siens","position":24},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Réseaux bayésiens"},"content":"","type":"content","url":"/graphical-models#r-seaux-bay-siens","position":25},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Factorisation","lvl2":"Réseaux bayésiens"},"type":"lvl3","url":"/graphical-models#factorisation","position":26},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Factorisation","lvl2":"Réseaux bayésiens"},"content":"Pour un graphe acyclique dirigé (DAG), chaque nœud est conditionnellement indépendant de tous ses prédécesseurs étant donné ses parents:Y_i \\perp \\mathbf{Y}_{\\text{pred}(i) \\setminus \\text{pa}(i)} \\mid \\mathbf{Y}_{\\text{pa}(i)}\n\noù \\text{pa}(i) sont les parents du nœud i et \\text{pred}(i) sont ses prédécesseurs dans l’ordre topologique.\n\nLa probabilité conjointe se factorise:p(Y_{1:N_G}) = \\prod_{i=1}^{N_G} p(Y_i \\mid \\mathbf{Y}_{\\text{pa}(i)})\n\noù N_G est le nombre de nœuds dans le graphe.","type":"content","url":"/graphical-models#factorisation","position":27},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Exemple: Réseau de l’arroseur automatique","lvl2":"Réseaux bayésiens"},"type":"lvl3","url":"/graphical-models#exemple-r-seau-de-larroseur-automatique","position":28},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Exemple: Réseau de l’arroseur automatique","lvl2":"Réseaux bayésiens"},"content":"Considérons les variables:\n\nC: saison nuageuse ou non\n\nR: pluie ou non\n\nS: arroseur automatique allumé ou non\n\nW: herbe mouillée ou non\n\nLa probabilité conjointe:p(C, S, R, W) = p(C) p(S \\mid C) p(R \\mid C) p(W \\mid S, R)\n\nLes relations de CI simplifient la factorisation:\n\nS \\perp R \\mid C (l’arroseur et la pluie sont indépendants étant donné la météo)","type":"content","url":"/graphical-models#exemple-r-seau-de-larroseur-automatique","position":29},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Distribution de probabilité conditionnelle (CPD)","lvl2":"Réseaux bayésiens"},"type":"lvl3","url":"/graphical-models#distribution-de-probabilit-conditionnelle-cpd","position":30},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Distribution de probabilité conditionnelle (CPD)","lvl2":"Réseaux bayésiens"},"content":"Chaque p(Y_i \\mid \\mathbf{Y}_{\\text{pa}(i)}) est appelée la distribution de probabilité conditionnelle (CPD) pour le nœud i.\n\nPour des variables catégorielles, on utilise un tableau de probabilité conditionnelle (CPT):\\theta_{ijk} \\triangleq p(Y_i = k \\mid \\mathbf{Y}_{\\text{pa}(i)} = j)\n\noù:\n\ni indexe les nœuds, i \\in [N_G]\n\nk indexe les états du nœud, k \\in [K_i]\n\nj indexe les configurations des parents, j \\in [J_i] avec J_i = \\prod_{p \\in \\text{pa}(i)} K_p\n\nLes contraintes sont: 0 \\leq \\theta_{ijk} \\leq 1 et \\sum_{k=1}^{K_i} \\theta_{ijk} = 1.","type":"content","url":"/graphical-models#distribution-de-probabilit-conditionnelle-cpd","position":31},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Notation de plaque","lvl2":"Réseaux bayésiens"},"type":"lvl3","url":"/graphical-models#notation-de-plaque","position":32},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Notation de plaque","lvl2":"Réseaux bayésiens"},"content":"Pour éviter les encombrements visuels dans les graphes avec des variables répétées, on utilise la notation de plaque: une boîte (plaque) entoure les variables qui se répètent N fois.","type":"content","url":"/graphical-models#notation-de-plaque","position":33},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Modèles de séquences"},"type":"lvl2","url":"/graphical-models#mod-les-de-s-quences","position":34},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Modèles de séquences"},"content":"","type":"content","url":"/graphical-models#mod-les-de-s-quences","position":35},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Modèles autorégressifs","lvl2":"Modèles de séquences"},"type":"lvl3","url":"/graphical-models#mod-les-autor-gressifs","position":36},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Modèles autorégressifs","lvl2":"Modèles de séquences"},"content":"Pour une séquence y_{1:T}, la règle de la chaîne donne:p(y_{1:T}) = \\prod_{t=1}^T p(y_t \\mid y_{1:t-1})\n\nLe nombre de paramètres pour p(y_t \\mid y_{1:t-1}) augmente exponentiellement avec t.","type":"content","url":"/graphical-models#mod-les-autor-gressifs","position":37},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Chaînes de Markov","lvl2":"Modèles de séquences"},"type":"lvl3","url":"/graphical-models#cha-nes-de-markov","position":38},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Chaînes de Markov","lvl2":"Modèles de séquences"},"content":"Si nous faisons l’hypothèse que l’avenir est indépendant du passé étant donné le présent (condition de Markov d’ordre 1):p(y_{1:T}) = p(y_1) \\prod_{t=2}^T p(y_t \\mid y_{t-1})\n\nCe modèle est appelé chaîne de Markov ou modèle autorégressif d’ordre 1.","type":"content","url":"/graphical-models#cha-nes-de-markov","position":39},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Noyau de transition","lvl2":"Modèles de séquences"},"type":"lvl3","url":"/graphical-models#noyau-de-transition","position":40},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Noyau de transition","lvl2":"Modèles de séquences"},"content":"La fonction p(y_t \\mid y_{t-1}) est le noyau de transition ou noyau de Markov:p(y_t \\mid y_{t-1}) \\geq 0, \\quad \\sum_{k=1}^K p(y_t = k \\mid y_{t-1} = j) = 1\n\nLe CPT de ce modèle est contenu dans une matrice stochastique de transition:A_{jk} = p(y_t = k \\mid y_{t-1} = j)\n\noù chaque ligne somme à 1. Si cette matrice est la même pour toutes les étapes, le modèle est dit homogène ou invariant dans le temps.","type":"content","url":"/graphical-models#noyau-de-transition","position":41},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Modèles d’ordre supérieur","lvl2":"Modèles de séquences"},"type":"lvl3","url":"/graphical-models#mod-les-dordre-sup-rieur","position":42},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Modèles d’ordre supérieur","lvl2":"Modèles de séquences"},"content":"L’hypothèse de Markov peut être généralisée à des dépendances sur les M observations précédentes:p(y_{1:T}) = p(y_{1:M}) \\prod_{t=M+1}^T p(y_t \\mid y_{t-M:t-1})\n\nEn modélisation du langage:\n\nBigramme: modèle de Markov d’ordre 1 (M=1)\n\nTrigramme: modèle de Markov d’ordre 2 (M=2)","type":"content","url":"/graphical-models#mod-les-dordre-sup-rieur","position":43},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Échantillonnage et génération"},"type":"lvl2","url":"/graphical-models#id-chantillonnage-et-g-n-ration","position":44},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Échantillonnage et génération"},"content":"","type":"content","url":"/graphical-models#id-chantillonnage-et-g-n-ration","position":45},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Échantillonnage ancestral","lvl2":"Échantillonnage et génération"},"type":"lvl3","url":"/graphical-models#id-chantillonnage-ancestral","position":46},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Échantillonnage ancestral","lvl2":"Échantillonnage et génération"},"content":"Pour générer des échantillons d’un réseau bayésien:\n\nVisiter les nœuds dans l’ordre topologique (parents avant enfants)\n\nÉchantillonner une valeur pour chaque nœud en fonction de ses parents\n\nCette technique s’appelle l’échantillonnage ancestral et permet d’obtenir des échantillons indépendants de la distribution conjointe:(x_1, \\ldots, x_{N_G}) \\sim p(\\boldsymbol{x} \\mid \\boldsymbol{\\theta})","type":"content","url":"/graphical-models#id-chantillonnage-ancestral","position":47},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Apprentissage dans les PGM"},"type":"lvl2","url":"/graphical-models#apprentissage-dans-les-pgm","position":48},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Apprentissage dans les PGM"},"content":"Si les paramètres des CPD sont inconnus, nous pouvons les traiter comme des variables à inférer:\\begin{aligned}\n\\boldsymbol{\\theta} &\\sim p(\\boldsymbol{\\theta}) \\\\\n\\boldsymbol{y}_n &\\sim p(\\boldsymbol{y} \\mid \\boldsymbol{\\theta})\n\\end{aligned}\n\noù p(\\boldsymbol{\\theta}) est un a priori sur les paramètres.\n\nLa distribution conjointe:p(\\mathcal{D}, \\boldsymbol{\\theta}) = p(\\boldsymbol{\\theta}) p(\\mathcal{D} \\mid \\boldsymbol{\\theta})\n\nSous l’hypothèse i.i.d., la vraisemblance:p(\\mathcal{D} \\mid \\boldsymbol{\\theta}) = \\prod_{n=1}^N p(\\boldsymbol{y}_n \\mid \\boldsymbol{\\theta})","type":"content","url":"/graphical-models#apprentissage-dans-les-pgm","position":49},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Inférence"},"type":"lvl2","url":"/graphical-models#inf-rence","position":50},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Inférence"},"content":"","type":"content","url":"/graphical-models#inf-rence","position":51},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Le problème d’inférence","lvl2":"Inférence"},"type":"lvl3","url":"/graphical-models#le-probl-me-dinf-rence","position":52},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Le problème d’inférence","lvl2":"Inférence"},"content":"En inférence, nous calculons le a posteriori sur un ensemble de nœuds de requête Q étant donné les valeurs observées pour un ensemble de nœuds visibles V. Nous devons marginaliser sur les variables de nuisance R:p_{\\boldsymbol{\\theta}}(Q \\mid V) = \\frac{p_{\\boldsymbol{\\theta}}(Q, V)}{p_{\\boldsymbol{\\theta}}(V)} = \\frac{\\sum_R p_{\\boldsymbol{\\theta}}(Q, V, R)}{p_{\\boldsymbol{\\theta}}(V)}\n\nEn général, l’inférence est NP-difficile.","type":"content","url":"/graphical-models#le-probl-me-dinf-rence","position":53},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Élimination de variables","lvl2":"Inférence"},"type":"lvl3","url":"/graphical-models#id-limination-de-variables","position":54},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Élimination de variables","lvl2":"Inférence"},"content":"L’élimination de variables est un algorithme exact qui exploite la structure du graphe pour calculer efficacement les marginales.\n\nL’idée clé est d’intégrer les sommes dans les produits:p(J) = \\sum_{L,S,G,H,I,D,C} p(C,D,I,G,S,L,J,H)\n\nAu lieu d’énumérer toutes les \n\n27 configurations, on procède par étapes:\n\nMultiplier les facteurs dans la portée de la somme la plus interne\n\nMarginaliser pour créer un nouveau facteur\n\nRépéter jusqu’à ce que toutes les variables de nuisance soient éliminées\n\nExemple:\\tau_1(D) = \\sum_C \\psi_C(C) \\psi_D(D, C)\n\n\\tau_2(G, I) = \\sum_D \\psi_G(G, I, D) \\tau_1(D)\n\nEt ainsi de suite.","type":"content","url":"/graphical-models#id-limination-de-variables","position":55},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Inférence variationnelle"},"type":"lvl2","url":"/graphical-models#inf-rence-variationnelle","position":56},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Inférence variationnelle"},"content":"","type":"content","url":"/graphical-models#inf-rence-variationnelle","position":57},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Motivation","lvl2":"Inférence variationnelle"},"type":"lvl3","url":"/graphical-models#motivation","position":58},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Motivation","lvl2":"Inférence variationnelle"},"content":"Considérons un modèle avec:\n\nVariables latentes (inconnues): \\boldsymbol{z}\n\nVariables observées: \\boldsymbol{x}\n\nParamètres fixes: \\boldsymbol{\\theta}\n\nL’a posteriori exact:p_{\\boldsymbol{\\theta}}(\\boldsymbol{z} \\mid \\boldsymbol{x}) = \\frac{p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}, \\boldsymbol{z})}{p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})}\n\nLa normalisation p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}) = \\int p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}, \\boldsymbol{z}) d\\boldsymbol{z} est souvent impossible à calculer.","type":"content","url":"/graphical-models#motivation","position":59},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Approximation variationnelle","lvl2":"Inférence variationnelle"},"type":"lvl3","url":"/graphical-models#approximation-variationnelle","position":60},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Approximation variationnelle","lvl2":"Inférence variationnelle"},"content":"L’idée est de trouver une approximation q(\\boldsymbol{z}) de l’a posteriori en minimisant:q^* = \\underset{q \\in \\mathcal{Q}}{\\text{argmin}} \\, D_{\\text{KL}}(q(\\boldsymbol{z}) \\| p_{\\boldsymbol{\\theta}}(\\boldsymbol{z} \\mid \\boldsymbol{x}))","type":"content","url":"/graphical-models#approximation-variationnelle","position":61},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Borne inférieure de l’évidence (ELBO)","lvl2":"Inférence variationnelle"},"type":"lvl3","url":"/graphical-models#borne-inf-rieure-de-l-vidence-elbo","position":62},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Borne inférieure de l’évidence (ELBO)","lvl2":"Inférence variationnelle"},"content":"En développant la divergence KL:D_{\\text{KL}}(q_{\\boldsymbol{\\psi}}(\\boldsymbol{z}) \\| p_{\\boldsymbol{\\theta}}(\\boldsymbol{z} \\mid \\boldsymbol{x})) = \\mathcal{L}(\\boldsymbol{\\theta}, \\boldsymbol{\\psi} \\mid \\boldsymbol{x}) + \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})\n\noù:\\mathcal{L}(\\boldsymbol{\\theta}, \\boldsymbol{\\psi} \\mid \\boldsymbol{x}) = \\mathbb{E}_{q_{\\boldsymbol{\\psi}}(\\boldsymbol{z})}[-\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}, \\boldsymbol{z}) + \\log q_{\\boldsymbol{\\psi}}(\\boldsymbol{z})]\n\nPuisque D_{\\text{KL}} \\geq 0:-\\mathcal{L}(\\boldsymbol{\\theta}, \\boldsymbol{\\psi} \\mid \\boldsymbol{x}) \\leq \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x})\n\nLe négatif de \\mathcal{L} est la borne inférieure de l’évidence (ELBO):\\text{ELBO} = \\mathbb{E}_{q_{\\boldsymbol{\\psi}}(\\boldsymbol{z})}[\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}, \\boldsymbol{z}) - \\log q_{\\boldsymbol{\\psi}}(\\boldsymbol{z})]\n\nMaximiser l’ELBO équivaut à minimiser la divergence KL.","type":"content","url":"/graphical-models#borne-inf-rieure-de-l-vidence-elbo","position":63},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Interprétations de l’ELBO","lvl2":"Inférence variationnelle"},"type":"lvl3","url":"/graphical-models#interpr-tations-de-lelbo","position":64},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Interprétations de l’ELBO","lvl2":"Inférence variationnelle"},"content":"Forme 1: Énergie et entropie\\text{ELBO} = \\mathbb{E}_{q_{\\boldsymbol{\\psi}}(\\boldsymbol{z})}[\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}, \\boldsymbol{z})] + \\mathbb{H}(q_{\\boldsymbol{\\psi}})= \\text{log-probabilité conjointe attendue} + \\text{entropie}\n\nForme 2: Vraisemblance et régularisation\\text{ELBO} = \\mathbb{E}_{q_{\\boldsymbol{\\psi}}(\\boldsymbol{z})}[\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x} \\mid \\boldsymbol{z})] - D_{\\text{KL}}(q_{\\boldsymbol{\\psi}}(\\boldsymbol{z}) \\| p_{\\boldsymbol{\\theta}}(\\boldsymbol{z}))= \\text{vraisemblance attendue} - \\text{régularisation}\n\nLe terme KL empêche l’a posteriori approché de s’éloigner trop de l’a priori.","type":"content","url":"/graphical-models#interpr-tations-de-lelbo","position":65},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Inférence variationnelle par gradient"},"type":"lvl2","url":"/graphical-models#inf-rence-variationnelle-par-gradient","position":66},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Inférence variationnelle par gradient"},"content":"","type":"content","url":"/graphical-models#inf-rence-variationnelle-par-gradient","position":67},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Gradient par rapport à \\boldsymbol{\\theta}","lvl2":"Inférence variationnelle par gradient"},"type":"lvl3","url":"/graphical-models#gradient-par-rapport-boldsymbol-theta","position":68},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Gradient par rapport à \\boldsymbol{\\theta}","lvl2":"Inférence variationnelle par gradient"},"content":"Le gradient par rapport aux paramètres du modèle est facile car la dépendance est structurelle:\\nabla_{\\boldsymbol{\\theta}} \\text{ELBO} = \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z})}[\\nabla_{\\boldsymbol{\\theta}} \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}, \\boldsymbol{z})] \\approx \\nabla_{\\boldsymbol{\\theta}} \\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x}, \\boldsymbol{z}^s)\n\noù \\boldsymbol{z}^s \\sim q_{\\boldsymbol{\\phi}}(\\boldsymbol{z}).","type":"content","url":"/graphical-models#gradient-par-rapport-boldsymbol-theta","position":69},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Gradient par rapport à \\boldsymbol{\\phi}","lvl2":"Inférence variationnelle par gradient"},"type":"lvl3","url":"/graphical-models#gradient-par-rapport-boldsymbol-phi","position":70},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Gradient par rapport à \\boldsymbol{\\phi}","lvl2":"Inférence variationnelle par gradient"},"content":"Le gradient par rapport aux paramètres variationnels est plus difficile car la dépendance est distributionnelle:\\nabla_{\\boldsymbol{\\phi}} \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z})}[f(\\boldsymbol{z})] \\neq \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z})}[\\nabla_{\\boldsymbol{\\phi}} f(\\boldsymbol{z})]","type":"content","url":"/graphical-models#gradient-par-rapport-boldsymbol-phi","position":71},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"L’astuce de reparamétrisation","lvl2":"Inférence variationnelle par gradient"},"type":"lvl3","url":"/graphical-models#lastuce-de-reparam-trisation","position":72},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"L’astuce de reparamétrisation","lvl2":"Inférence variationnelle par gradient"},"content":"La solution est de réécrire \\boldsymbol{z} \\sim q_{\\boldsymbol{\\phi}}(\\boldsymbol{z}) comme une transformation d’une variable aléatoire indépendante de \\boldsymbol{\\phi}.\n\nPour une gaussienne:\\boldsymbol{z} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\text{diag}(\\boldsymbol{\\sigma})) \\Longleftrightarrow \\boldsymbol{z} = \\boldsymbol{\\mu} + \\boldsymbol{\\epsilon} \\odot \\boldsymbol{\\sigma}, \\quad \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})\n\nMaintenant:\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z})}[f(\\boldsymbol{z})] = \\mathbb{E}_{p(\\boldsymbol{\\epsilon})}[f(g(\\boldsymbol{\\phi}, \\boldsymbol{\\epsilon}))]\n\nEt le gradient devient:\\nabla_{\\boldsymbol{\\phi}} \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z})}[f(\\boldsymbol{z})] = \\mathbb{E}_{p(\\boldsymbol{\\epsilon})}[\\nabla_{\\boldsymbol{\\phi}} f(g(\\boldsymbol{\\phi}, \\boldsymbol{\\epsilon}))]","type":"content","url":"/graphical-models#lastuce-de-reparam-trisation","position":73},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Auto-encodeurs variationnels (VAE)"},"type":"lvl2","url":"/graphical-models#auto-encodeurs-variationnels-vae","position":74},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Auto-encodeurs variationnels (VAE)"},"content":"","type":"content","url":"/graphical-models#auto-encodeurs-variationnels-vae","position":75},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Modèles de variables latentes profondes","lvl2":"Auto-encodeurs variationnels (VAE)"},"type":"lvl3","url":"/graphical-models#mod-les-de-variables-latentes-profondes","position":76},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Modèles de variables latentes profondes","lvl2":"Auto-encodeurs variationnels (VAE)"},"content":"Un modèle de variables latentes profondes (DLVM) est un modèle génératif:\\begin{aligned}\n\\boldsymbol{z} &\\sim p_{\\boldsymbol{\\theta}}(\\boldsymbol{z}) \\\\\n\\boldsymbol{x} \\mid \\boldsymbol{z} &\\sim \\text{Expfam}(\\boldsymbol{x} \\mid d_{\\boldsymbol{\\theta}}(\\boldsymbol{z}))\n\\end{aligned}\n\noù:\n\np_{\\boldsymbol{\\theta}}(\\boldsymbol{z}) est un a priori sur le code latent (souvent \\mathcal{N}(\\mathbf{0}, \\mathbf{I}))\n\nd_{\\boldsymbol{\\theta}}(\\boldsymbol{z}) est un réseau de neurones profond appelé décodeur","type":"content","url":"/graphical-models#mod-les-de-variables-latentes-profondes","position":77},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Architecture du VAE","lvl2":"Auto-encodeurs variationnels (VAE)"},"type":"lvl3","url":"/graphical-models#architecture-du-vae","position":78},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Architecture du VAE","lvl2":"Auto-encodeurs variationnels (VAE)"},"content":"Un auto-encodeur variationnel (VAE) combine:\n\nModèle génératif (décodeur):p_{\\boldsymbol{\\theta}}(\\boldsymbol{z}, \\boldsymbol{x}) = p_{\\boldsymbol{\\theta}}(\\boldsymbol{z}) p_{\\boldsymbol{\\theta}}(\\boldsymbol{x} \\mid \\boldsymbol{z})\n\nPour des observations binaires:p_{\\boldsymbol{\\theta}}(\\boldsymbol{x} \\mid \\boldsymbol{z}) = \\prod_{d=1}^D \\text{Ber}(x_d \\mid \\sigma(d_{\\boldsymbol{\\theta}}(\\boldsymbol{z})))\n\nModèle de reconnaissance (encodeur):q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x}) = \\mathcal{N}(\\boldsymbol{z} \\mid \\boldsymbol{\\mu}, \\text{diag}(\\exp(\\boldsymbol{\\ell})))\n\noù (\\boldsymbol{\\mu}, \\boldsymbol{\\ell}) = e_{\\boldsymbol{\\phi}}(\\boldsymbol{x}) sont les sorties de l’encodeur.","type":"content","url":"/graphical-models#architecture-du-vae","position":79},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Entraînement du VAE","lvl2":"Auto-encodeurs variationnels (VAE)"},"type":"lvl3","url":"/graphical-models#entra-nement-du-vae","position":80},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Entraînement du VAE","lvl2":"Auto-encodeurs variationnels (VAE)"},"content":"L’ELBO pour un VAE:\\mathcal{L}(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}) = \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x})}[\\log p_{\\boldsymbol{\\theta}}(\\boldsymbol{x} \\mid \\boldsymbol{z})] - D_{\\text{KL}}(q_{\\boldsymbol{\\phi}}(\\boldsymbol{z} \\mid \\boldsymbol{x}) \\| p_{\\boldsymbol{\\theta}}(\\boldsymbol{z}))\n\nPremier terme: reconstruction (l’encodage permet-il de reconstruire \\boldsymbol{x}?)\n\nSecond terme: régularisation (l’a posteriori reste proche de l’a priori)\n\nL’astuce de reparamétrisation permet d’entraîner les deux réseaux par descente de gradient.","type":"content","url":"/graphical-models#entra-nement-du-vae","position":81},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Inférence variationnelle stochastique","lvl2":"Auto-encodeurs variationnels (VAE)"},"type":"lvl3","url":"/graphical-models#inf-rence-variationnelle-stochastique","position":82},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl3":"Inférence variationnelle stochastique","lvl2":"Auto-encodeurs variationnels (VAE)"},"content":"Pour les grands ensembles de données, on utilise des mini-lots:\\mathcal{L}(\\boldsymbol{\\theta}, \\boldsymbol{\\phi} \\mid \\mathcal{D}) \\approx \\frac{N}{B} \\sum_{\\boldsymbol{x}_n \\in \\mathcal{B}} \\mathcal{L}(\\boldsymbol{\\theta}, \\boldsymbol{\\phi} \\mid \\boldsymbol{x}_n)\n\nCette technique est appelée inférence variationnelle stochastique (SVI).","type":"content","url":"/graphical-models#inf-rence-variationnelle-stochastique","position":83},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Résumé"},"type":"lvl2","url":"/graphical-models#r-sum","position":84},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Résumé"},"content":"Les modèles graphiques probabilistes offrent un cadre puissant pour:\n\nConcept\n\nDescription\n\nReprésentation\n\nFactoriser les distributions jointes via les indépendances\n\nÉchantillonnage\n\nGénérer des données via l’échantillonnage ancestral\n\nInférence\n\nCalculer les marginales et conditionnelles\n\nApprentissage\n\nEstimer les paramètres des CPD\n\nL’inférence variationnelle permet d’approximer des a posteriori intractables, et les VAE combinent cette approche avec des réseaux de neurones profonds pour apprendre des représentations latentes.","type":"content","url":"/graphical-models#r-sum","position":85},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Exercices"},"type":"lvl2","url":"/graphical-models#exercices","position":86},{"hierarchy":{"lvl1":"Modèles graphiques probabilistes","lvl2":"Exercices"},"content":"Exercice 1: Factorisation\n\nConsidérez un réseau bayésien avec les arêtes: A \\to B, A \\to C, B \\to D, C \\to D.\n\nÉcrivez la factorisation de la distribution conjointe p(A, B, C, D).\n\nQuelles sont les relations d’indépendance conditionnelle impliquées par ce graphe?\n\nCombien de paramètres sont nécessaires si toutes les variables sont binaires?\n\nExercice 2: Élimination de variables\n\nPour le réseau A \\to B \\to C:\n\nCalculez p(C) par élimination de variables.\n\nMontrez les facteurs intermédiaires créés à chaque étape.\n\nExercice 3: ELBO\n\nPour un VAE avec a priori p(\\boldsymbol{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) et a posteriori approché q_{\\boldsymbol{\\phi}}(\\boldsymbol{z}) = \\mathcal{N}(\\boldsymbol{\\mu}, \\text{diag}(\\boldsymbol{\\sigma}^2)):\n\nCalculez analytiquement D_{\\text{KL}}(q_{\\boldsymbol{\\phi}}(\\boldsymbol{z}) \\| p(\\boldsymbol{z})).\n\nPourquoi cette forme analytique est-elle utile pour l’entraînement?\n\nExercice 4: Reparamétrisation\n\nMontrez comment appliquer l’astuce de reparamétrisation pour:\n\nUne distribution uniforme z \\sim \\text{Unif}(a, b)\n\nUne distribution exponentielle z \\sim \\text{Exp}(\\lambda)","type":"content","url":"/graphical-models#exercices","position":87},{"hierarchy":{"lvl1":"Introduction"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Introduction"},"content":"L’apprentissage machine étudie les algorithmes qui s’améliorent par l’expérience. Plutôt que de suivre des instructions explicites programmées à l’avance, un algorithme d’apprentissage utilise les données pour découvrir des régularités, faire des prédictions et informer des décisions. Ce livre introduit les fondements mathématiques et les algorithmes pratiques qui rendent un tel apprentissage possible.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Introduction","lvl2":"Perspective historique"},"type":"lvl2","url":"/#perspective-historique","position":2},{"hierarchy":{"lvl1":"Introduction","lvl2":"Perspective historique"},"content":"L’idée d’apprendre à partir des données n’est pas récente. En 1693, Edmund Halley tentait de découvrir des régularités dans les naissances et mortalités de la ville de Breslau. Son objectif était de dériver une formule pour prédire les montants espérés en taxes qui seraient collectés pour chaque individu. Cette table de mortalité représente l’une des premières tentatives systématiques d’utiliser des données pour faire des prédictions.\n\nLa cybernétique, discipline introduite par Norbert Wiener en 1947, a unifié les théories de l’information et de la commande optimale. Cette approche ambitieuse visait à expliquer le cerveau à l’aide des ordinateurs, à comprendre les organisations sociales et les interactions entre humains et machines. Ces travaux ont posé les bases conceptuelles de l’apprentissage automatique moderne.\n\nEn 1957, Frank Rosenblatt a introduit le perceptron pour classifier des images. Utilisant les idées de McCulloch et Pitts ainsi que de Donald Hebb sur la formalisation du calcul neuronal, le perceptron représente l’ancêtre des réseaux de neurones modernes. Cette machine pouvait apprendre à distinguer différentes formes à partir d’exemples, démontrant pour la première fois qu’une machine pouvait véritablement apprendre.","type":"content","url":"/#perspective-historique","position":3},{"hierarchy":{"lvl1":"Introduction","lvl2":"Types d’apprentissage"},"type":"lvl2","url":"/#types-dapprentissage","position":4},{"hierarchy":{"lvl1":"Introduction","lvl2":"Types d’apprentissage"},"content":"Les problèmes d’apprentissage machine se divisent en plusieurs catégories selon la nature des données disponibles et l’objectif visé.","type":"content","url":"/#types-dapprentissage","position":5},{"hierarchy":{"lvl1":"Introduction","lvl3":"Apprentissage supervisé","lvl2":"Types d’apprentissage"},"type":"lvl3","url":"/#apprentissage-supervis","position":6},{"hierarchy":{"lvl1":"Introduction","lvl3":"Apprentissage supervisé","lvl2":"Types d’apprentissage"},"content":"Dans l’apprentissage supervisé, nous disposons de paires d’entrées et de sorties. L’objectif est d’identifier automatiquement une relation entre ces paires pour pouvoir prédire la sortie associée à de nouvelles entrées. Nous supposons qu’il existe une vraie fonction f: \\mathcal{X} \\to \\mathcal{Y} pouvant expliquer les données, et nous cherchons à l’approximer.\n\nSoit d la dimension du vecteur d’entrée. Selon la nature de la sortie, nous distinguons:\n\nLa classification binaire: \\hat{f}: \\mathbb{R}^d \\to \\{0, 1\\}\n\nLa classification multiclasse: \\hat{f}: \\mathbb{R}^d \\to \\{0, \\ldots, m\\}\n\nLa régression: \\hat{f}: \\mathbb{R}^d \\to \\mathbb{R}^p\n\nLes problèmes de classification ont trait à des questions de nature qualitative, alors que la régression traite de questions quantitatives. Par exemple, déterminer si un courriel est un pourriel relève de la classification, tandis que prédire le prix d’une maison relève de la régression.","type":"content","url":"/#apprentissage-supervis","position":7},{"hierarchy":{"lvl1":"Introduction","lvl3":"Apprentissage non supervisé","lvl2":"Types d’apprentissage"},"type":"lvl3","url":"/#apprentissage-non-supervis","position":8},{"hierarchy":{"lvl1":"Introduction","lvl3":"Apprentissage non supervisé","lvl2":"Types d’apprentissage"},"content":"Dans l’apprentissage non supervisé, nous n’avons accès qu’aux données d’entrée, sans étiquettes associées. L’objectif est de découvrir la structure cachée dans les données. L’estimation de densité, qui consiste à estimer la fonction de densité d’une variable aléatoire à partir d’observations, constitue un exemple classique. Le partitionnement (clustering), qui vise à regrouper des exemples similaires, en est un autre.","type":"content","url":"/#apprentissage-non-supervis","position":9},{"hierarchy":{"lvl1":"Introduction","lvl3":"Apprentissage auto-supervisé","lvl2":"Types d’apprentissage"},"type":"lvl3","url":"/#apprentissage-auto-supervis","position":10},{"hierarchy":{"lvl1":"Introduction","lvl3":"Apprentissage auto-supervisé","lvl2":"Types d’apprentissage"},"content":"L’apprentissage auto-supervisé génère automatiquement des cibles utiles à partir des données elles-mêmes. Par exemple, on peut masquer une partie d’une image et entraîner un modèle à la reconstruire, ou prédire le mot suivant dans une phrase. Cette approche permet d’exploiter de grandes quantités de données non étiquetées.","type":"content","url":"/#apprentissage-auto-supervis","position":11},{"hierarchy":{"lvl1":"Introduction","lvl3":"Apprentissage par renforcement","lvl2":"Types d’apprentissage"},"type":"lvl3","url":"/#apprentissage-par-renforcement","position":12},{"hierarchy":{"lvl1":"Introduction","lvl3":"Apprentissage par renforcement","lvl2":"Types d’apprentissage"},"content":"L’apprentissage par renforcement traite des problèmes d’actions plutôt que de prédictions. Un agent interagit avec un environnement et apprend à maximiser une récompense cumulative. Ce paradigme s’applique aux jeux, à la robotique et aux systèmes de recommandation.","type":"content","url":"/#apprentissage-par-renforcement","position":13},{"hierarchy":{"lvl1":"Introduction","lvl2":"Aperçu du cours"},"type":"lvl2","url":"/#aper-u-du-cours","position":14},{"hierarchy":{"lvl1":"Introduction","lvl2":"Aperçu du cours"},"content":"Ce livre couvre les concepts et méthodes fondamentaux de l’apprentissage machine. Nous commençons par la formalisation du problème d’apprentissage: le risque, le risque empirique, et ce que signifie pour un algorithme de généraliser. Ce cadre théorique guide notre étude des algorithmes.\n\nNous étudions d’abord les méthodes non paramétriques comme les k plus proches voisins et les méthodes à noyau. Ces approches simples illustrent les concepts fondamentaux sans imposer de forme paramétrique forte aux données.\n\nLes modèles linéaires forment ensuite le coeur du livre: régression linéaire, régression logistique, et machines à vecteurs de support. Ces méthodes, bien que conceptuellement simples, sont puissantes et constituent la base des approches plus complexes.\n\nLes réseaux de neurones étendent les modèles linéaires en composant des transformations simples. Nous couvrons les perceptrons multicouches, les réseaux convolutifs pour les images, et les réseaux récurrents pour les séquences.\n\nLes méthodes d’ensemble combinent plusieurs modèles: le bagging réduit la variance, le boosting réduit le biais. Les forêts aléatoires et le gradient boosting comptent parmi les algorithmes les plus performants en pratique.\n\nL’apprentissage non supervisé occupe la dernière partie: l’analyse en composantes principales pour la réduction de dimensionnalité, et les mélanges gaussiens pour le partitionnement. Nous concluons par une introduction aux modèles graphiques probabilistes.","type":"content","url":"/#aper-u-du-cours","position":15},{"hierarchy":{"lvl1":"Introduction","lvl2":"Prérequis"},"type":"lvl2","url":"/#pr-requis","position":16},{"hierarchy":{"lvl1":"Introduction","lvl2":"Prérequis"},"content":"Ce cours suppose une familiarité avec les domaines suivants.\n\nAlgèbre linéaire. Vecteurs, matrices, produits matriciels, valeurs propres et vecteurs propres, décompositions matricielles. La capacité à manipuler des expressions vectorielles et matricielles est essentielle.\n\nProbabilités et statistiques. Variables aléatoires, distributions de probabilité, espérance, variance, covariance. Les distributions de Bernoulli, catégorique et gaussienne apparaissent fréquemment.\n\nCalcul différentiel. Dérivées partielles, gradients, règle de la chaîne. L’optimisation par descente de gradient est omniprésente en apprentissage machine.\n\nProgrammation. Capacité à implémenter des algorithmes en Python, avec les bibliothèques NumPy et Matplotlib. Les exemples de code utilisent également scikit-learn.\n\nLes annexes fournissent des révisions brèves du matériel prérequis pour référence.","type":"content","url":"/#pr-requis","position":17},{"hierarchy":{"lvl1":"Introduction","lvl2":"Comment utiliser ce livre"},"type":"lvl2","url":"/#comment-utiliser-ce-livre","position":18},{"hierarchy":{"lvl1":"Introduction","lvl2":"Comment utiliser ce livre"},"content":"Chaque chapitre commence par une liste d’objectifs d’apprentissage formulés comme des capacités: “À la fin de ce chapitre, vous serez en mesure de...” Ces objectifs guident la lecture et correspondent à ce qui est évalué dans les exercices.\n\nLes concepts sont introduits par des exemples concrets avant les définitions formelles. Un exemple simple en deux dimensions précède souvent la formulation générale. Après avoir présenté la théorie, nous revenons à l’exemple initial pour montrer comment il s’inscrit dans le cadre abstrait.\n\nLes exemples travaillés montrent les étapes intermédiaires des calculs. Nous indiquons explicitement quelles hypothèses ou lemmes sont utilisés, et pourquoi chaque étape est effectuée.\n\nLes exercices varient en difficulté: rappel de définitions, applications directes, preuves conceptuelles, et implémentations. Les exercices de codage demandent d’implémenter les algorithmes à partir de zéro avant d’utiliser les fonctions de bibliothèque.","type":"content","url":"/#comment-utiliser-ce-livre","position":19},{"hierarchy":{"lvl1":"Introduction","lvl2":"Ressources complémentaires"},"type":"lvl2","url":"/#ressources-compl-mentaires","position":20},{"hierarchy":{"lvl1":"Introduction","lvl2":"Ressources complémentaires"},"content":"Les ouvrages suivants complètent ce livre:\n\nProbabilistic Machine Learning: An Introduction par Kevin P. Murphy, disponible gratuitement en ligne\n\nThe Elements of Statistical Learning par Hastie, Tibshirani et Friedman\n\nPattern Recognition and Machine Learning par Christopher M. Bishop\n\nDeep Learning par Goodfellow, Bengio et Courville","type":"content","url":"/#ressources-compl-mentaires","position":21},{"hierarchy":{"lvl1":"Introduction","lvl2":"Notation"},"type":"lvl2","url":"/#notation","position":22},{"hierarchy":{"lvl1":"Introduction","lvl2":"Notation"},"content":"Nous adoptons les conventions suivantes tout au long du livre:\n\nSymbole\n\nSignification\n\n\\mathcal{D}\n\nEnsemble de données\n\n\\boldsymbol{x}\n\nVecteur d’entrée\n\ny\n\nCible ou étiquette\n\n\\boldsymbol{\\theta}\n\nParamètres du modèle\n\n\\mathcal{R}\n\nRisque (vrai risque)\n\n\\hat{\\mathcal{R}}\n\nRisque empirique\n\n\\ell\n\nFonction de perte\n\n\\mathcal{H}\n\nClasse d’hypothèses\n\n\\mathbb{E}[\\cdot]\n\nEspérance\n\n\\mathbb{P}(\\cdot)\n\nProbabilité\n\n\\mathbb{1}_A\n\nFonction indicatrice de l’ensemble A\n\nLes vecteurs sont notés en gras minuscule (\\boldsymbol{x}), les matrices en gras majuscule (\\boldsymbol{X}), et les scalaires en italique (x).","type":"content","url":"/#notation","position":23},{"hierarchy":{"lvl1":"Méthodes à noyau"},"type":"lvl1","url":"/kernel-methods","position":0},{"hierarchy":{"lvl1":"Méthodes à noyau"},"content":"Objectifs d’apprentissage\n\nÀ la fin de ce chapitre, vous serez en mesure de:\n\nDéfinir un noyau et ses propriétés mathématiques\n\nComprendre le lissage par noyau et ses applications\n\nImplémenter l’estimateur de densité de Parzen-Rosenblatt\n\nAppliquer la régression de Nadaraya-Watson\n\nExpliquer l’astuce du noyau et le théorème du représentant\n\nRelier les différentes méthodes à noyau vues dans ce livre","type":"content","url":"/kernel-methods","position":1},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl2":"Introduction aux noyaux"},"type":"lvl2","url":"/kernel-methods#introduction-aux-noyaux","position":2},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl2":"Introduction aux noyaux"},"content":"","type":"content","url":"/kernel-methods#introduction-aux-noyaux","position":3},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Qu’est-ce qu’un noyau?","lvl2":"Introduction aux noyaux"},"type":"lvl3","url":"/kernel-methods#quest-ce-quun-noyau","position":4},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Qu’est-ce qu’un noyau?","lvl2":"Introduction aux noyaux"},"content":"Un noyau K est une fonction de pondération qui mesure la similarité ou la proximité entre deux points. Il existe plusieurs utilisations distinctes des noyaux en apprentissage automatique:\n\nNoyaux de lissage: fonctions de pondération pour les méthodes non-paramétriques\n\nNoyaux de Mercer: fonctions de similarité pour l’astuce du noyau\n\nCe chapitre unifie ces concepts et montre comment ils sont reliés aux méthodes vues précédemment.","type":"content","url":"/kernel-methods#quest-ce-quun-noyau","position":5},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl2":"Noyaux de lissage"},"type":"lvl2","url":"/kernel-methods#noyaux-de-lissage","position":6},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl2":"Noyaux de lissage"},"content":"","type":"content","url":"/kernel-methods#noyaux-de-lissage","position":7},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Définition","lvl2":"Noyaux de lissage"},"type":"lvl3","url":"/kernel-methods#d-finition","position":8},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Définition","lvl2":"Noyaux de lissage"},"content":"Un noyau de lissage (ou noyau de densité) est une fonction K: \\mathbb{R} \\to \\mathbb{R} satisfaisant:\n\nK(u) \\geq 0 pour tout u\n\n\\int K(u) \\, du = 1\n\nK(u) = K(-u) (symétrie)\n\nCes propriétés font de K une densité de probabilité symétrique.","type":"content","url":"/kernel-methods#d-finition","position":9},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Exemples de noyaux de lissage","lvl2":"Noyaux de lissage"},"type":"lvl3","url":"/kernel-methods#exemples-de-noyaux-de-lissage","position":10},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Exemples de noyaux de lissage","lvl2":"Noyaux de lissage"},"content":"Noyau uniforme (boxcar):K(u) = \\frac{1}{2} \\mathbb{I}(|u| \\leq 1)\n\nNoyau gaussien:K(u) = \\frac{1}{\\sqrt{2\\pi}} e^{-u^2/2}\n\nNoyau d’Epanechnikov:K(u) = \\frac{3}{4}(1 - u^2) \\mathbb{I}(|u| \\leq 1)\n\nNoyau tri-cube:K(u) = \\frac{70}{81}(1 - |u|^3)^3 \\mathbb{I}(|u| \\leq 1)","type":"content","url":"/kernel-methods#exemples-de-noyaux-de-lissage","position":11},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Paramètre de largeur","lvl2":"Noyaux de lissage"},"type":"lvl3","url":"/kernel-methods#param-tre-de-largeur","position":12},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Paramètre de largeur","lvl2":"Noyaux de lissage"},"content":"Pour contrôler l’étendue du noyau, on introduit un paramètre de largeur (ou bandwidth) \\lambda > 0:K_\\lambda(u) = \\frac{1}{\\lambda} K\\left(\\frac{u}{\\lambda}\\right)\n\nGrand \\lambda: beaucoup de lissage, modèle simple\n\nPetit \\lambda: peu de lissage, modèle complexe\n\nCe paramètre joue un rôle analogue à k dans k-NN ou à \\lambda en régularisation.","type":"content","url":"/kernel-methods#param-tre-de-largeur","position":13},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl2":"Estimation de densité par noyau"},"type":"lvl2","url":"/kernel-methods#estimation-de-densit-par-noyau","position":14},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl2":"Estimation de densité par noyau"},"content":"","type":"content","url":"/kernel-methods#estimation-de-densit-par-noyau","position":15},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Estimateur de Parzen-Rosenblatt","lvl2":"Estimation de densité par noyau"},"type":"lvl3","url":"/kernel-methods#estimateur-de-parzen-rosenblatt","position":16},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Estimateur de Parzen-Rosenblatt","lvl2":"Estimation de densité par noyau"},"content":"L’estimateur de densité à noyau (ou estimateur de Parzen-Rosenblatt) estime la densité de probabilité p(x) en plaçant un noyau sur chaque observation:\\hat{p}(x \\mid \\mathcal{D}) = \\frac{1}{N} \\sum_{i=1}^N K_\\lambda(x - x_i)","type":"content","url":"/kernel-methods#estimateur-de-parzen-rosenblatt","position":17},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Interprétation","lvl2":"Estimation de densité par noyau"},"type":"lvl3","url":"/kernel-methods#interpr-tation","position":18},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Interprétation","lvl2":"Estimation de densité par noyau"},"content":"Intuitivement, cet estimateur:\n\nPlace une “bosse” sur chaque point de données\n\nSomme toutes ces bosses (normalisées)\n\nLe résultat est une approximation lisse de la densité","type":"content","url":"/kernel-methods#interpr-tation","position":19},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Effet du paramètre de largeur","lvl2":"Estimation de densité par noyau"},"type":"lvl3","url":"/kernel-methods#effet-du-param-tre-de-largeur","position":20},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Effet du paramètre de largeur","lvl2":"Estimation de densité par noyau"},"content":"\\lambda trop petit: l’estimateur est trop “piquant”, chaque observation crée un pic\n\n\\lambda trop grand: l’estimateur est trop lisse, tous les détails sont perdus\n\n\\lambda optimal: compromis entre biais et variance","type":"content","url":"/kernel-methods#effet-du-param-tre-de-largeur","position":21},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Extension multidimensionnelle","lvl2":"Estimation de densité par noyau"},"type":"lvl3","url":"/kernel-methods#extension-multidimensionnelle","position":22},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Extension multidimensionnelle","lvl2":"Estimation de densité par noyau"},"content":"Pour des données en dimension D, on utilise souvent un noyau produit:K_\\lambda(\\boldsymbol{x} - \\boldsymbol{x}_i) = \\prod_{d=1}^D K_\\lambda(x_d - x_{id})\n\nou un noyau gaussien multivarié:K_\\mathbf{H}(\\boldsymbol{x} - \\boldsymbol{x}_i) = \\frac{1}{(2\\pi)^{D/2}|\\mathbf{H}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\boldsymbol{x} - \\boldsymbol{x}_i)^\\top \\mathbf{H}^{-1}(\\boldsymbol{x} - \\boldsymbol{x}_i)\\right)\n\noù \\mathbf{H} est la matrice de largeur de bande.","type":"content","url":"/kernel-methods#extension-multidimensionnelle","position":23},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl2":"Régression par noyau"},"type":"lvl2","url":"/kernel-methods#r-gression-par-noyau","position":24},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl2":"Régression par noyau"},"content":"","type":"content","url":"/kernel-methods#r-gression-par-noyau","position":25},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Estimateur de Nadaraya-Watson","lvl2":"Régression par noyau"},"type":"lvl3","url":"/kernel-methods#estimateur-de-nadaraya-watson","position":26},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Estimateur de Nadaraya-Watson","lvl2":"Régression par noyau"},"content":"Pour la régression, nous voulons estimer \\mathbb{E}[Y \\mid \\boldsymbol{x}]. L’estimateur de Nadaraya-Watson utilise une moyenne pondérée par noyau:\\hat{f}(\\boldsymbol{x}) = \\sum_{i=1}^N w_i(\\boldsymbol{x}) \\, y_i\n\noù les poids sont normalisés:w_i(\\boldsymbol{x}) = \\frac{K_\\lambda(\\boldsymbol{x} - \\boldsymbol{x}_i)}{\\sum_{j=1}^N K_\\lambda(\\boldsymbol{x} - \\boldsymbol{x}_j)}","type":"content","url":"/kernel-methods#estimateur-de-nadaraya-watson","position":27},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Dérivation","lvl2":"Régression par noyau"},"type":"lvl3","url":"/kernel-methods#d-rivation","position":28},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Dérivation","lvl2":"Régression par noyau"},"content":"En utilisant l’estimation de densité par noyau pour p(x, y):\\hat{p}(x, y) = \\frac{1}{N} \\sum_{i=1}^N K_\\lambda(x - x_i) K_\\lambda(y - y_i)\n\nOn peut montrer que:\\mathbb{E}[Y \\mid x] = \\frac{\\int y \\, p(x, y) \\, dy}{\\int p(x, y) \\, dy} \\approx \\sum_{i=1}^N w_i(x) \\, y_i","type":"content","url":"/kernel-methods#d-rivation","position":29},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Propriétés","lvl2":"Régression par noyau"},"type":"lvl3","url":"/kernel-methods#propri-t-s","position":30},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Propriétés","lvl2":"Régression par noyau"},"content":"Non-paramétrique: pas de forme fonctionnelle supposée\n\nLocal: la prédiction dépend surtout des voisins proches\n\nLisse: produit une fonction continue (contrairement à k-NN)","type":"content","url":"/kernel-methods#propri-t-s","position":31},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl2":"Lien avec k-NN"},"type":"lvl2","url":"/kernel-methods#lien-avec-k-nn","position":32},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl2":"Lien avec k-NN"},"content":"","type":"content","url":"/kernel-methods#lien-avec-k-nn","position":33},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"K-NN comme cas particulier","lvl2":"Lien avec k-NN"},"type":"lvl3","url":"/kernel-methods#k-nn-comme-cas-particulier","position":34},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"K-NN comme cas particulier","lvl2":"Lien avec k-NN"},"content":"Le k-NN peut être vu comme une méthode à noyau avec un noyau adaptatif:K(x, x_i) = \\begin{cases} 1 & \\text{si } x_i \\in \\mathcal{N}_k(x) \\\\ 0 & \\text{sinon} \\end{cases}\n\noù \\mathcal{N}_k(x) est l’ensemble des k plus proches voisins de x.\n\nLa largeur du noyau s’adapte à la densité locale:\n\nDans les régions denses: le voisinage est petit\n\nDans les régions éparses: le voisinage est grand","type":"content","url":"/kernel-methods#k-nn-comme-cas-particulier","position":35},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Comparaison","lvl2":"Lien avec k-NN"},"type":"lvl3","url":"/kernel-methods#comparaison","position":36},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Comparaison","lvl2":"Lien avec k-NN"},"content":"Aspect\n\nNoyau fixe\n\nK-NN\n\nLargeur\n\nFixe (\\lambda)\n\nAdaptative\n\nNombre de voisins\n\nVariable\n\nFixe (k)\n\nPoids\n\nContinus\n\nUniformes\n\nDiscontinuités\n\nNon\n\nOui (aux frontières)","type":"content","url":"/kernel-methods#comparaison","position":37},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl2":"L’astuce du noyau"},"type":"lvl2","url":"/kernel-methods#lastuce-du-noyau","position":38},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl2":"L’astuce du noyau"},"content":"","type":"content","url":"/kernel-methods#lastuce-du-noyau","position":39},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Motivation","lvl2":"L’astuce du noyau"},"type":"lvl3","url":"/kernel-methods#motivation","position":40},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Motivation","lvl2":"L’astuce du noyau"},"content":"Dans de nombreux algorithmes, les données n’apparaissent qu’à travers des produits scalaires \\langle \\boldsymbol{x}_i, \\boldsymbol{x}_j \\rangle.\n\nL’astuce du noyau consiste à remplacer ces produits scalaires par une fonction noyau:\\langle \\boldsymbol{x}_i, \\boldsymbol{x}_j \\rangle \\to \\mathcal{K}(\\boldsymbol{x}_i, \\boldsymbol{x}_j)","type":"content","url":"/kernel-methods#motivation","position":41},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Noyaux de Mercer","lvl2":"L’astuce du noyau"},"type":"lvl3","url":"/kernel-methods#noyaux-de-mercer","position":42},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Noyaux de Mercer","lvl2":"L’astuce du noyau"},"content":"Un noyau \\mathcal{K} est un noyau de Mercer s’il correspond à un produit scalaire dans un espace de redescription:\\mathcal{K}(\\boldsymbol{x}, \\boldsymbol{x}') = \\langle \\boldsymbol{\\phi}(\\boldsymbol{x}), \\boldsymbol{\\phi}(\\boldsymbol{x}') \\rangle\n\noù \\boldsymbol{\\phi}: \\mathbb{R}^D \\to \\mathcal{H} est une transformation vers un espace de caractéristiques \\mathcal{H} (possiblement de dimension infinie).","type":"content","url":"/kernel-methods#noyaux-de-mercer","position":43},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Condition de Mercer","lvl2":"L’astuce du noyau"},"type":"lvl3","url":"/kernel-methods#condition-de-mercer","position":44},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Condition de Mercer","lvl2":"L’astuce du noyau"},"content":"Un noyau est de Mercer si et seulement si sa matrice de Gram est semi-définie positive pour tout ensemble de points:\\mathbf{K} = \\begin{pmatrix}\n\\mathcal{K}(\\boldsymbol{x}_1, \\boldsymbol{x}_1) & \\cdots & \\mathcal{K}(\\boldsymbol{x}_1, \\boldsymbol{x}_N) \\\\\n\\vdots & \\ddots & \\vdots \\\\\n\\mathcal{K}(\\boldsymbol{x}_N, \\boldsymbol{x}_1) & \\cdots & \\mathcal{K}(\\boldsymbol{x}_N, \\boldsymbol{x}_N)\n\\end{pmatrix} \\succeq 0","type":"content","url":"/kernel-methods#condition-de-mercer","position":45},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Exemples de noyaux de Mercer","lvl2":"L’astuce du noyau"},"type":"lvl3","url":"/kernel-methods#exemples-de-noyaux-de-mercer","position":46},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Exemples de noyaux de Mercer","lvl2":"L’astuce du noyau"},"content":"Noyau linéaire:\\mathcal{K}(\\boldsymbol{x}, \\boldsymbol{x}') = \\boldsymbol{x}^\\top \\boldsymbol{x}'\n\nNoyau polynomial:\\mathcal{K}(\\boldsymbol{x}, \\boldsymbol{x}') = (1 + \\boldsymbol{x}^\\top \\boldsymbol{x}')^p\n\nNoyau gaussien (RBF):\\mathcal{K}(\\boldsymbol{x}, \\boldsymbol{x}') = \\exp\\left(-\\frac{\\|\\boldsymbol{x} - \\boldsymbol{x}'\\|^2}{2\\ell^2}\\right)\n\nLe noyau gaussien correspond à un espace de caractéristiques de dimension infinie.","type":"content","url":"/kernel-methods#exemples-de-noyaux-de-mercer","position":47},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl2":"Le théorème du représentant"},"type":"lvl2","url":"/kernel-methods#le-th-or-me-du-repr-sentant","position":48},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl2":"Le théorème du représentant"},"content":"","type":"content","url":"/kernel-methods#le-th-or-me-du-repr-sentant","position":49},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Énoncé","lvl2":"Le théorème du représentant"},"type":"lvl3","url":"/kernel-methods#id-nonc","position":50},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Énoncé","lvl2":"Le théorème du représentant"},"content":"Le théorème du représentant stipule que pour tout problème d’optimisation de la forme:\\min_{f \\in \\mathcal{H}_K} \\left[\\sum_{i=1}^N L(y_i, f(x_i)) + \\lambda \\|f\\|_{\\mathcal{H}_K}^2\\right]\n\noù \\mathcal{H}_K est un espace de Hilbert à noyau reproduisant (RKHS), la solution optimale s’écrit:f^*(x) = \\sum_{i=1}^N \\alpha_i \\mathcal{K}(x, x_i)","type":"content","url":"/kernel-methods#id-nonc","position":51},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Implications","lvl2":"Le théorème du représentant"},"type":"lvl3","url":"/kernel-methods#implications","position":52},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Implications","lvl2":"Le théorème du représentant"},"content":"Même si \\mathcal{H}_K est de dimension infinie, la solution optimale est une combinaison linéaire finie des noyaux centrés sur les données\n\nLe problème de dimension infinie devient un problème d’optimisation sur N paramètres \\alpha_i\n\nC’est le fondement théorique des SVM et de la régression ridge à noyau","type":"content","url":"/kernel-methods#implications","position":53},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl2":"Applications"},"type":"lvl2","url":"/kernel-methods#applications","position":54},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl2":"Applications"},"content":"","type":"content","url":"/kernel-methods#applications","position":55},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Régression ridge à noyau","lvl2":"Applications"},"type":"lvl3","url":"/kernel-methods#r-gression-ridge-noyau","position":56},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"Régression ridge à noyau","lvl2":"Applications"},"content":"En appliquant le théorème du représentant à la régression ridge:\\min_{\\boldsymbol{\\alpha}} \\|\\mathbf{y} - \\mathbf{K}\\boldsymbol{\\alpha}\\|^2 + \\lambda \\boldsymbol{\\alpha}^\\top \\mathbf{K} \\boldsymbol{\\alpha}\n\nLa solution est:\\boldsymbol{\\alpha} = (\\mathbf{K} + \\lambda \\mathbf{I})^{-1} \\mathbf{y}","type":"content","url":"/kernel-methods#r-gression-ridge-noyau","position":57},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"SVM à noyau","lvl2":"Applications"},"type":"lvl3","url":"/kernel-methods#svm-noyau","position":58},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl3":"SVM à noyau","lvl2":"Applications"},"content":"Les SVM utilisent l’astuce du noyau dans leur forme duale pour créer des frontières de décision non-linéaires (voir chapitre SVM).","type":"content","url":"/kernel-methods#svm-noyau","position":59},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl2":"Résumé"},"type":"lvl2","url":"/kernel-methods#r-sum","position":60},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl2":"Résumé"},"content":"Les méthodes à noyau forment une famille puissante de techniques:\n\nLes noyaux de lissage pondèrent les observations par distance\n\nL’estimation de densité de Parzen utilise des noyaux pour estimer p(x)\n\nLa régression de Nadaraya-Watson fait de la régression locale pondérée\n\nL’astuce du noyau permet de travailler implicitement dans des espaces de grande dimension\n\nLe théorème du représentant garantit des solutions finies même dans des espaces infinis\n\nCes concepts unifient k-NN, les SVM, la régression ridge à noyau et de nombreuses autres méthodes.","type":"content","url":"/kernel-methods#r-sum","position":61},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl2":"Exercices"},"type":"lvl2","url":"/kernel-methods#exercices","position":62},{"hierarchy":{"lvl1":"Méthodes à noyau","lvl2":"Exercices"},"content":"Exercice 1: Estimation de densité\n\nSoit les données \\{-1, 0, 2\\} en 1D.\n\nCalculez l’estimateur de Parzen avec un noyau gaussien (\\sigma = 1) en x = 0.\n\nCalculez-le en x = 1.\n\nEsquissez la forme générale de \\hat{p}(x).\n\nExercice 2: Nadaraya-Watson\n\nDonnées: (x, y) = \\{(0, 1), (1, 2), (2, 1)\\}.\n\nAvec un noyau gaussien de \\sigma = 0.5:\n\nCalculez les poids pour prédire en x = 0.5.\n\nCalculez la prédiction \\hat{f}(0.5).\n\nExercice 3: Noyau polynomial\n\nMontrez que le noyau polynomial en 2D \\mathcal{K}(\\boldsymbol{x}, \\boldsymbol{x}') = (\\boldsymbol{x}^\\top \\boldsymbol{x}')^2 correspond à la transformation:\\boldsymbol{\\phi}(x_1, x_2) = (x_1^2, \\sqrt{2}x_1 x_2, x_2^2)\n\n(Calculez explicitement \\boldsymbol{\\phi}(\\boldsymbol{x})^\\top \\boldsymbol{\\phi}(\\boldsymbol{x}').)\n\nExercice 4: Matrice de Gram\n\nPour les points \\{0, 1, 2\\} et le noyau gaussien avec \\ell = 1:\n\nCalculez la matrice de Gram \\mathbf{K}.\n\nVérifiez qu’elle est symétrique.\n\nCalculez ses valeurs propres pour vérifier qu’elle est semi-définie positive.","type":"content","url":"/kernel-methods#exercices","position":63},{"hierarchy":{"lvl1":"K plus proches voisins"},"type":"lvl1","url":"/knn","position":0},{"hierarchy":{"lvl1":"K plus proches voisins"},"content":"Objectifs d’apprentissage\n\nÀ la fin de ce chapitre, vous serez en mesure de:\n\nExpliquer le fonctionnement de l’algorithme des k plus proches voisins\n\nDéfinir et appliquer différentes fonctions de distance\n\nInterpréter le diagramme de Voronoï\n\nExpliquer le fléau de la dimensionnalité\n\nAnalyser l’effet du paramètre k sur le compromis biais-variance\n\nImplémenter l’algorithme k-ppv pour la classification et la régression","type":"content","url":"/knn","position":1},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Introduction"},"type":"lvl2","url":"/knn#introduction","position":2},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Introduction"},"content":"Les k plus proches voisins (k-ppv, en anglais k-nearest neighbors, k-NN) constituent l’une des méthodes d’apprentissage les plus simples et les plus intuitives. L’idée fondamentale est que des exemples similaires devraient avoir des étiquettes similaires. Pour prédire l’étiquette d’un nouvel exemple, nous identifions ses voisins les plus proches dans l’ensemble d’entraînement et combinons leurs étiquettes.\n\nCette méthode est dite non paramétrique: elle ne suppose pas de forme fonctionnelle particulière pour la relation entre entrées et sorties. Elle est également à mémoire (en anglais memory-based ou instance-based): l’entraînement consiste simplement à stocker les données, et le travail se fait au moment de l’inférence.","type":"content","url":"/knn#introduction","position":3},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"L’algorithme k-ppv"},"type":"lvl2","url":"/knn#lalgorithme-k-ppv","position":4},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"L’algorithme k-ppv"},"content":"","type":"content","url":"/knn#lalgorithme-k-ppv","position":5},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Formulation","lvl2":"L’algorithme k-ppv"},"type":"lvl3","url":"/knn#formulation","position":6},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Formulation","lvl2":"L’algorithme k-ppv"},"content":"Soit \\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N un ensemble d’entraînement. Pour classifier un nouveau point x, nous procédons en deux étapes:\n\nTrouver les voisins: Identifier les k points de \\mathcal{D} les plus proches de x selon une métrique donnée\n\nAgréger les votes: Prédire la classe majoritaire parmi ces k voisins\n\nFormellement, soit \\mathcal{N}_k(x) l’ensemble des indices des k plus proches voisins de x. La prédiction est:\\hat{y} = \\arg\\max_{c} \\sum_{i \\in \\mathcal{N}_k(x)} \\mathbb{1}_{y_i = c}\n\noù la somme compte le nombre de voisins appartenant à chaque classe c.","type":"content","url":"/knn#formulation","position":7},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Version probabiliste","lvl2":"L’algorithme k-ppv"},"type":"lvl3","url":"/knn#version-probabiliste","position":8},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Version probabiliste","lvl2":"L’algorithme k-ppv"},"content":"Plutôt que de retourner uniquement la classe prédite, nous pouvons estimer la distribution sur les classes:p(y = c | x, \\mathcal{D}) = \\frac{1}{k} \\sum_{i \\in \\mathcal{N}_k(x)} \\mathbb{1}_{y_i = c}\n\nCette probabilité est simplement la proportion de voisins appartenant à la classe c. La prédiction déterministe correspond au mode de cette distribution.","type":"content","url":"/knn#version-probabiliste","position":9},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Exemple","lvl2":"L’algorithme k-ppv"},"type":"lvl3","url":"/knn#exemple","position":10},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Exemple","lvl2":"L’algorithme k-ppv"},"content":"Considérons le jeu de données Iris avec trois classes: Setosa, Versicolor et Virginica. Pour un nouveau point x avec k = 5, supposons que les 5 plus proches voisins aient les étiquettes:\n\nVoisin\n\nÉtiquette\n\n1\n\nSetosa\n\n2\n\nSetosa\n\n3\n\nVersicolor\n\n4\n\nSetosa\n\n5\n\nVersicolor\n\nLes probabilités estimées sont:\n\np(\\text{Setosa} | x) = 3/5 = 0.6\n\np(\\text{Versicolor} | x) = 2/5 = 0.4\n\np(\\text{Virginica} | x) = 0/5 = 0\n\nLa classe prédite est Setosa.","type":"content","url":"/knn#exemple","position":11},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Fonctions de distance"},"type":"lvl2","url":"/knn#fonctions-de-distance","position":12},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Fonctions de distance"},"content":"Le choix de la distance détermine la notion de similarité entre exemples. Ce choix encode nos hypothèses sur la structure du problème.","type":"content","url":"/knn#fonctions-de-distance","position":13},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Définition formelle","lvl2":"Fonctions de distance"},"type":"lvl3","url":"/knn#d-finition-formelle","position":14},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Définition formelle","lvl2":"Fonctions de distance"},"content":"Une fonction de distance (ou métrique) d: \\mathcal{X} \\times \\mathcal{X} \\to [0, \\infty) doit satisfaire trois axiomes:\n\nIdentité: d(x, y) = 0 \\Leftrightarrow x = y\n\nSymétrie: d(x, y) = d(y, x)\n\nInégalité triangulaire: d(x, z) \\leq d(x, y) + d(y, z)\n\nCes conditions impliquent la non-négativité: d(x, y) \\geq 0 pour tous x, y.","type":"content","url":"/knn#d-finition-formelle","position":15},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Distance euclidienne","lvl2":"Fonctions de distance"},"type":"lvl3","url":"/knn#distance-euclidienne","position":16},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Distance euclidienne","lvl2":"Fonctions de distance"},"content":"La distance euclidienne (ou distance L2) est le choix le plus courant:d_2(x, y) = \\sqrt{\\sum_{j=1}^{d} (x_j - y_j)^2} = \\|x - y\\|_2\n\nCette distance correspond à la longueur du segment de droite entre les deux points, la distance “à vol d’oiseau”.","type":"content","url":"/knn#distance-euclidienne","position":17},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Distance de Manhattan","lvl2":"Fonctions de distance"},"type":"lvl3","url":"/knn#distance-de-manhattan","position":18},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Distance de Manhattan","lvl2":"Fonctions de distance"},"content":"La distance de Manhattan (ou distance L1, taxicab distance) est:d_1(x, y) = \\sum_{j=1}^{d} |x_j - y_j| = \\|x - y\\|_1\n\nElle mesure la distance parcourue en suivant une grille, comme dans les rues d’une ville. Contrairement à la distance euclidienne, elle favorise les chemins alignés avec les axes.","type":"content","url":"/knn#distance-de-manhattan","position":19},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Distance de Mahalanobis","lvl2":"Fonctions de distance"},"type":"lvl3","url":"/knn#distance-de-mahalanobis","position":20},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Distance de Mahalanobis","lvl2":"Fonctions de distance"},"content":"La distance de Mahalanobis prend en compte la corrélation entre les variables:d_M(x, y) = \\sqrt{(x - y)^\\top M (x - y)}\n\noù M est une matrice définie positive. Si M = I, nous retrouvons la distance euclidienne. Si M = \\Sigma^{-1} où \\Sigma est la matrice de covariance des données, la distance de Mahalanobis normalise les variables et supprime les corrélations.","type":"content","url":"/knn#distance-de-mahalanobis","position":21},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Normes et distances","lvl2":"Fonctions de distance"},"type":"lvl3","url":"/knn#normes-et-distances","position":22},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Normes et distances","lvl2":"Fonctions de distance"},"content":"Dans un espace vectoriel, toute norme \\|\\cdot\\| induit une distance:d(x, y) = \\|x - y\\|\n\nLa norme \\ell_p est définie par:\\|x\\|_p = \\left(\\sum_{j=1}^{d} |x_j|^p\\right)^{1/p}\n\nLes cas p = 1 et p = 2 correspondent aux distances de Manhattan et euclidienne respectivement. La limite p \\to \\infty donne la norme infinie \\|x\\|_\\infty = \\max_j |x_j|.","type":"content","url":"/knn#normes-et-distances","position":23},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Diagramme de Voronoï"},"type":"lvl2","url":"/knn#diagramme-de-vorono","position":24},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Diagramme de Voronoï"},"content":"Le cas k = 1 induit une structure géométrique particulière appelée diagramme de Voronoï (ou tesselation de Voronoï).","type":"content","url":"/knn#diagramme-de-vorono","position":25},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Définition","lvl2":"Diagramme de Voronoï"},"type":"lvl3","url":"/knn#d-finition","position":26},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Définition","lvl2":"Diagramme de Voronoï"},"content":"Pour un ensemble de points \\{x_1, \\ldots, x_N\\} appelés germes, le diagramme de Voronoï partitionne l’espace en cellules. La cellule V_i associée au germe x_i contient tous les points plus proches de x_i que de tout autre germe:V_i = \\{x \\in \\mathbb{R}^d : d(x, x_i) \\leq d(x, x_j) \\text{ pour tout } j \\neq i\\}\n\nLes frontières entre cellules sont des hyperplans (en dimension d > 2) ou des segments (en dimension 2).","type":"content","url":"/knn#d-finition","position":27},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Interprétation","lvl2":"Diagramme de Voronoï"},"type":"lvl3","url":"/knn#interpr-tation","position":28},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Interprétation","lvl2":"Diagramme de Voronoï"},"content":"Avec le 1-ppv, la frontière de décision suit exactement le diagramme de Voronoï. Tout point dans la cellule V_i est classifié selon l’étiquette de x_i.\n\nPour k = 1, l’erreur d’entraînement est exactement zéro: chaque point d’entraînement est son propre plus proche voisin et est donc correctement classifié. Cette interpolation parfaite des données d’entraînement peut mener à un surapprentissage sévère.","type":"content","url":"/knn#interpr-tation","position":29},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Effet du paramètre k"},"type":"lvl2","url":"/knn#effet-du-param-tre-k","position":30},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Effet du paramètre k"},"content":"Le paramètre k contrôle la complexité du modèle et influence le compromis biais-variance.","type":"content","url":"/knn#effet-du-param-tre-k","position":31},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Petites valeurs de k","lvl2":"Effet du paramètre k"},"type":"lvl3","url":"/knn#petites-valeurs-de-k","position":32},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Petites valeurs de k","lvl2":"Effet du paramètre k"},"content":"Avec un petit k (notamment k = 1):\n\nLa frontière de décision est très irrégulière\n\nLe modèle s’adapte étroitement aux données d’entraînement\n\nLa variance est élevée: de petits changements dans les données produisent des prédictions très différentes\n\nLe biais est faible: le modèle peut capturer des structures complexes\n\nRisque de surapprentissage","type":"content","url":"/knn#petites-valeurs-de-k","position":33},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Grandes valeurs de k","lvl2":"Effet du paramètre k"},"type":"lvl3","url":"/knn#grandes-valeurs-de-k","position":34},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Grandes valeurs de k","lvl2":"Effet du paramètre k"},"content":"Avec un grand k (jusqu’à k = N):\n\nLa frontière de décision devient plus lisse\n\nLe modèle “moyenne” sur de nombreux exemples\n\nLa variance est faible: les prédictions sont stables\n\nLe biais est élevé: le modèle peut manquer des structures locales\n\nRisque de sous-apprentissage\n\nLe cas extrême k = N prédit toujours la classe majoritaire globale, ignorant complètement l’entrée x.","type":"content","url":"/knn#grandes-valeurs-de-k","position":35},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Choix de k","lvl2":"Effet du paramètre k"},"type":"lvl3","url":"/knn#choix-de-k","position":36},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Choix de k","lvl2":"Effet du paramètre k"},"content":"Le choix optimal de k dépend du problème et se fait typiquement par validation croisée. En pratique:\n\nDes valeurs impaires évitent les égalités dans les votes\n\nUne règle empirique suggère k \\approx \\sqrt{N}\n\nLa courbe d’erreur de validation en fonction de k guide le choix","type":"content","url":"/knn#choix-de-k","position":37},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Fléau de la dimensionnalité"},"type":"lvl2","url":"/knn#fl-au-de-la-dimensionnalit","position":38},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Fléau de la dimensionnalité"},"content":"Les méthodes basées sur la distance souffrent particulièrement du fléau de la dimensionnalité (en anglais curse of dimensionality). En haute dimension, les distances perdent leur pouvoir discriminant.","type":"content","url":"/knn#fl-au-de-la-dimensionnalit","position":39},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Phénomène","lvl2":"Fléau de la dimensionnalité"},"type":"lvl3","url":"/knn#ph-nom-ne","position":40},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Phénomène","lvl2":"Fléau de la dimensionnalité"},"content":"Considérons des points uniformément distribués dans un hypercube [0, 1]^d. Pour contenir une fraction p des points, nous avons besoin d’un hypercube de côté r = p^{1/d}.\n\nDimension d\n\nCôté r pour p = 0.1\n\n1\n\n0.10\n\n2\n\n0.32\n\n5\n\n0.63\n\n10\n\n0.79\n\n100\n\n0.98\n\nEn dimension 100, pour capturer 10% des points, nous avons besoin d’un hypercube couvrant 98% de chaque dimension. Les “voisins” ne sont plus vraiment proches.","type":"content","url":"/knn#ph-nom-ne","position":41},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Conséquences","lvl2":"Fléau de la dimensionnalité"},"type":"lvl3","url":"/knn#cons-quences","position":42},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Conséquences","lvl2":"Fléau de la dimensionnalité"},"content":"Distance minimale croissante: La distance au plus proche voisin augmente avec la dimension. Les voisins deviennent tous approximativement équidistants.\n\nVolume des hypersphères: Le volume d’une hypersphère de rayon fixe décroît exponentiellement avec la dimension. La plupart du volume d’un hypercube se concentre près de sa surface.\n\nDensité des données: Pour maintenir une densité constante de points, le nombre d’exemples requis croît exponentiellement avec la dimension.","type":"content","url":"/knn#cons-quences","position":43},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Atténuation","lvl2":"Fléau de la dimensionnalité"},"type":"lvl3","url":"/knn#att-nuation","position":44},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Atténuation","lvl2":"Fléau de la dimensionnalité"},"content":"Plusieurs stratégies atténuent le fléau de la dimensionnalité:\n\nRéduction de dimension: PCA, sélection de variables\n\nDistances adaptatives: Distance de Mahalanobis, apprentissage de métrique\n\nRégularisation: Contraintes sur le modèle","type":"content","url":"/knn#att-nuation","position":45},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Régression par k-ppv"},"type":"lvl2","url":"/knn#r-gression-par-k-ppv","position":46},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Régression par k-ppv"},"content":"L’algorithme k-ppv s’adapte naturellement à la régression. Plutôt que de prendre un vote majoritaire, nous moyennons les valeurs cibles des voisins:\\hat{y} = \\frac{1}{k} \\sum_{i \\in \\mathcal{N}_k(x)} y_i\n\nCette moyenne locale estime l’espérance conditionnelle \\mathbb{E}[Y | X = x].","type":"content","url":"/knn#r-gression-par-k-ppv","position":47},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Pondération","lvl2":"Régression par k-ppv"},"type":"lvl3","url":"/knn#pond-ration","position":48},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Pondération","lvl2":"Régression par k-ppv"},"content":"Une variante pondère les contributions des voisins selon leur distance:\\hat{y} = \\frac{\\sum_{i \\in \\mathcal{N}_k(x)} w_i y_i}{\\sum_{i \\in \\mathcal{N}_k(x)} w_i}\n\noù w_i = 1/d(x, x_i) ou w_i = \\exp(-d(x, x_i)^2/\\sigma^2). Les voisins plus proches ont plus d’influence sur la prédiction.","type":"content","url":"/knn#pond-ration","position":49},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Complexité computationnelle"},"type":"lvl2","url":"/knn#complexit-computationnelle","position":50},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Complexité computationnelle"},"content":"","type":"content","url":"/knn#complexit-computationnelle","position":51},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Entraînement","lvl2":"Complexité computationnelle"},"type":"lvl3","url":"/knn#entra-nement","position":52},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Entraînement","lvl2":"Complexité computationnelle"},"content":"L’entraînement est trivial: O(1) si nous comptons simplement stocker les données, ou O(N) pour les copier.","type":"content","url":"/knn#entra-nement","position":53},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Inférence","lvl2":"Complexité computationnelle"},"type":"lvl3","url":"/knn#inf-rence","position":54},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Inférence","lvl2":"Complexité computationnelle"},"content":"L’inférence naïve requiert O(Nd) opérations par requête:\n\nCalculer la distance à chaque exemple: O(d) par exemple\n\nTrouver les k plus petites: O(N) ou O(N \\log k) avec un tas\n\nPour de grands ensembles de données, c’est prohibitif. Des structures de données accélèrent la recherche:\n\nArbres k-d: O(\\log N) en moyenne pour la recherche du plus proche voisin\n\nHachage sensible à la localité (LSH): recherche approximative en temps sous-linéaire\n\nIndexation par graphe: HNSW et autres méthodes modernes","type":"content","url":"/knn#inf-rence","position":55},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Implémentation"},"type":"lvl2","url":"/knn#impl-mentation","position":56},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Implémentation"},"content":"Voici une implémentation simple en Python:import numpy as np\n\ndef knn_classify(X_train, y_train, X_test, k=3):\n    \"\"\"Classification par k plus proches voisins.\n    \n    Args:\n        X_train: Matrice d'entraînement (N x d)\n        y_train: Étiquettes d'entraînement (N,)\n        X_test: Points à classifier (M x d)\n        k: Nombre de voisins\n        \n    Returns:\n        Prédictions (M,)\n    \"\"\"\n    predictions = []\n    for x in X_test:\n        # Calculer les distances à tous les points d'entraînement\n        distances = np.sqrt(np.sum((X_train - x)**2, axis=1))\n        \n        # Trouver les indices des k plus proches\n        k_nearest_idx = np.argsort(distances)[:k]\n        \n        # Vote majoritaire\n        k_nearest_labels = y_train[k_nearest_idx]\n        unique, counts = np.unique(k_nearest_labels, return_counts=True)\n        predictions.append(unique[np.argmax(counts)])\n        \n    return np.array(predictions)def knn_regression(X_train, y_train, X_test, k=3):\n    \"\"\"Régression par k plus proches voisins.\n    \n    Args:\n        X_train: Matrice d'entraînement (N x d)\n        y_train: Cibles d'entraînement (N,)\n        X_test: Points à prédire (M x d)\n        k: Nombre de voisins\n        \n    Returns:\n        Prédictions (M,)\n    \"\"\"\n    predictions = []\n    for x in X_test:\n        distances = np.sqrt(np.sum((X_train - x)**2, axis=1))\n        k_nearest_idx = np.argsort(distances)[:k]\n        k_nearest_values = y_train[k_nearest_idx]\n        predictions.append(np.mean(k_nearest_values))\n        \n    return np.array(predictions)","type":"content","url":"/knn#impl-mentation","position":57},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Avantages et limitations"},"type":"lvl2","url":"/knn#avantages-et-limitations","position":58},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Avantages et limitations"},"content":"","type":"content","url":"/knn#avantages-et-limitations","position":59},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Avantages","lvl2":"Avantages et limitations"},"type":"lvl3","url":"/knn#avantages","position":60},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Avantages","lvl2":"Avantages et limitations"},"content":"Simplicité: Algorithme intuitif, facile à comprendre et implémenter\n\nPas d’hypothèse paramétrique: S’adapte à des frontières de décision complexes\n\nPas d’entraînement: Nouveau données ajoutées sans réentraînement\n\nInterprétabilité locale: Les prédictions s’expliquent par les exemples voisins","type":"content","url":"/knn#avantages","position":61},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Limitations","lvl2":"Avantages et limitations"},"type":"lvl3","url":"/knn#limitations","position":62},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Limitations","lvl2":"Avantages et limitations"},"content":"Coût d’inférence: O(Nd) par requête dans le cas naïf\n\nStockage: Tout l’ensemble d’entraînement doit être conservé\n\nSensibilité aux échelles: Les variables doivent être normalisées\n\nFléau de la dimensionnalité: Performance dégradée en haute dimension\n\nSensibilité au bruit: Un seul exemple mal étiqueté affecte les voisinages","type":"content","url":"/knn#limitations","position":63},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Résumé"},"type":"lvl2","url":"/knn#r-sum","position":64},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Résumé"},"content":"Ce chapitre a présenté l’algorithme des k plus proches voisins:\n\nLe k-ppv classifie un exemple par vote majoritaire parmi ses k plus proches voisins\n\nLe choix de la distance encode les hypothèses sur la similarité\n\nLe paramètre k contrôle le compromis biais-variance\n\nLe diagramme de Voronoï décrit la partition de l’espace pour k = 1\n\nLe fléau de la dimensionnalité limite l’efficacité en haute dimension\n\nL’algorithme s’adapte naturellement à la régression par moyennage local\n\nLe chapitre suivant généralise ces idées aux méthodes à noyau, où le voisinage est défini par une fonction continue plutôt que par un ensemble discret de voisins.","type":"content","url":"/knn#r-sum","position":65},{"hierarchy":{"lvl1":"Le problème d’apprentissage"},"type":"lvl1","url":"/learning-problem","position":0},{"hierarchy":{"lvl1":"Le problème d’apprentissage"},"content":"Objectifs d’apprentissage\n\nÀ la fin de ce chapitre, vous serez en mesure de:\n\nDéfinir formellement le problème d’apprentissage supervisé\n\nDistinguer les tâches de classification et de régression\n\nDéfinir le risque et le risque empirique\n\nExpliquer le principe de minimisation du risque empirique\n\nDériver l’estimateur du maximum de vraisemblance (EMV)\n\nRelier le EMV à la divergence de Kullback-Leibler\n\nDécrire les fonctions de perte courantes et leurs propriétés","type":"content","url":"/learning-problem","position":1},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Formalisation de l’apprentissage supervisé"},"type":"lvl2","url":"/learning-problem#formalisation-de-lapprentissage-supervis","position":2},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Formalisation de l’apprentissage supervisé"},"content":"L’apprentissage supervisé consiste à identifier automatiquement une relation entre des paires d’entrées et de sorties. Nous disposons d’un jeu de données \\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N composé de N paires, où chaque x_i \\in \\mathcal{X} est une entrée et y_i \\in \\mathcal{Y} est la sortie correspondante.\n\nL’objectif est de trouver une fonction \\hat{f}: \\mathcal{X} \\to \\mathcal{Y} qui approxime bien les données. Nous supposons implicitement qu’il existe une vraie fonction f: \\mathcal{X} \\to \\mathcal{Y} pouvant expliquer les données, et nous cherchons à l’estimer. La capacité à faire de bonnes prédictions au-delà des données déjà observées est ce que nous appelons la généralisation.","type":"content","url":"/learning-problem#formalisation-de-lapprentissage-supervis","position":3},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Données tabulaires et caractéristiques","lvl2":"Formalisation de l’apprentissage supervisé"},"type":"lvl3","url":"/learning-problem#donn-es-tabulaires-et-caract-ristiques","position":4},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Données tabulaires et caractéristiques","lvl2":"Formalisation de l’apprentissage supervisé"},"content":"Dans de nombreuses applications, les entrées prennent la forme de vecteurs de caractéristiques. Chaque exemple x_i \\in \\mathbb{R}^d est un vecteur de dimension d, où chaque composante représente une caractéristique (ou trait, en anglais feature) choisie pour sa valeur prédictive.\n\nConsidérons le jeu de données Iris, un exemple classique en apprentissage machine. Chaque fleur est décrite par quatre caractéristiques: la longueur et la largeur du sépale, ainsi que la longueur et la largeur du pétale. L’objectif est de prédire l’espèce parmi trois catégories: Setosa, Versicolor et Virginica.\n\nLongueur sépale (cm)\n\nLargeur sépale (cm)\n\nLongueur pétale (cm)\n\nLargeur pétale (cm)\n\n5.1\n\n3.5\n\n1.4\n\n0.2\n\n4.9\n\n3.0\n\n1.4\n\n0.2\n\n4.7\n\n3.2\n\n1.3\n\n0.2\n\nParfois, nous transformons les caractéristiques d’origine dans un autre espace, plus riche et expressif. Cet espace transformé est appelé espace de redescription (en anglais feature space). Par exemple, nous pourrions ajouter des termes quadratiques ou des interactions entre variables.","type":"content","url":"/learning-problem#donn-es-tabulaires-et-caract-ristiques","position":5},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Classification et régression","lvl2":"Formalisation de l’apprentissage supervisé"},"type":"lvl3","url":"/learning-problem#classification-et-r-gression","position":6},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Classification et régression","lvl2":"Formalisation de l’apprentissage supervisé"},"content":"Selon la nature de l’espace de sortie \\mathcal{Y}, nous distinguons deux types de problèmes.\n\nEn classification, la sortie est une catégorie parmi un ensemble fini. Pour la classification binaire, \\hat{f}: \\mathbb{R}^d \\to \\{0, 1\\}. Pour la classification multiclasse avec m catégories, \\hat{f}: \\mathbb{R}^d \\to \\{0, \\ldots, m-1\\}. Les problèmes de classification ont trait à des questions de nature qualitative: déterminer si un courriel est un pourriel, diagnostiquer une maladie, ou reconnaître un chiffre manuscrit.\n\nEn régression, la sortie est une valeur continue. Pour une sortie scalaire, \\hat{f}: \\mathbb{R}^d \\to \\mathbb{R}. Pour une sortie vectorielle, \\hat{f}: \\mathbb{R}^d \\to \\mathbb{R}^p. Les problèmes de régression concernent des questions quantitatives: prédire le prix d’une maison, estimer la température demain, ou prévoir la demande d’électricité.","type":"content","url":"/learning-problem#classification-et-r-gression","position":7},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Risque et risque empirique"},"type":"lvl2","url":"/learning-problem#risque-et-risque-empirique","position":8},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Risque et risque empirique"},"content":"La qualité d’un modèle se mesure par sa capacité à faire de bonnes prédictions. Pour quantifier cette notion, nous introduisons les concepts de fonction de perte et de risque.","type":"content","url":"/learning-problem#risque-et-risque-empirique","position":9},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Fonction de perte","lvl2":"Risque et risque empirique"},"type":"lvl3","url":"/learning-problem#fonction-de-perte","position":10},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Fonction de perte","lvl2":"Risque et risque empirique"},"content":"Une fonction de perte \\ell: \\mathcal{Y} \\times \\mathcal{Y} \\to \\mathbb{R}_+ mesure l’erreur commise lorsque nous prédisons \\hat{y} alors que la vraie valeur est y. Une perte de zéro indique une prédiction parfaite; plus la perte est grande, plus l’erreur est importante.\n\nPour la classification, un choix naturel est la perte 0-1:\\ell_{0-1}(y, \\hat{y}) = \\begin{cases}\n0 & \\text{si } y = \\hat{y} \\\\\n1 & \\text{si } y \\neq \\hat{y}\n\\end{cases} = \\mathbb{1}_{y \\neq \\hat{y}}\n\nCette perte vaut 0 pour une prédiction correcte et 1 pour une erreur. Le risque empirique correspondant est le taux d’erreur de classification.\n\nPour la régression, nous utilisons généralement la perte quadratique:\\ell_2(y, \\hat{y}) = (y - \\hat{y})^2\n\nCette perte pénalise les grandes erreurs de manière quadratique.","type":"content","url":"/learning-problem#fonction-de-perte","position":11},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Le risque","lvl2":"Risque et risque empirique"},"type":"lvl3","url":"/learning-problem#le-risque","position":12},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Le risque","lvl2":"Risque et risque empirique"},"content":"Le risque (ou risque de la population) d’un modèle f est l’espérance de la fonction de perte sur la distribution intrinsèque des données:\\mathcal{R}(f) = \\mathbb{E}_{(X,Y) \\sim p}\\left[\\ell(Y, f(X))\\right] = \\int \\ell(y, f(x)) \\, p(x, y) \\, dx \\, dy\n\nLe risque mesure la performance moyenne du modèle sur toutes les données possibles, pas seulement celles que nous avons observées. Un modèle avec un faible risque fait de bonnes prédictions en général.\n\nLe problème fondamental est que nous ne connaissons pas la distribution p(x, y). Nous n’y avons accès qu’indirectement, via un échantillon fini \\mathcal{D}.","type":"content","url":"/learning-problem#le-risque","position":13},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Le risque empirique","lvl2":"Risque et risque empirique"},"type":"lvl3","url":"/learning-problem#le-risque-empirique","position":14},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Le risque empirique","lvl2":"Risque et risque empirique"},"content":"Le risque empirique est une approximation du vrai risque calculée sur l’échantillon disponible:\\hat{\\mathcal{R}}(f, \\mathcal{D}) = \\frac{1}{|\\mathcal{D}|} \\sum_{i=1}^{|\\mathcal{D}|} \\ell(y_i, f(x_i))\n\nLe risque empirique est la moyenne des pertes sur les exemples d’entraînement. C’est une quantité que nous pouvons calculer directement.","type":"content","url":"/learning-problem#le-risque-empirique","position":15},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Principe de minimisation du risque empirique","lvl2":"Risque et risque empirique"},"type":"lvl3","url":"/learning-problem#principe-de-minimisation-du-risque-empirique","position":16},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Principe de minimisation du risque empirique","lvl2":"Risque et risque empirique"},"content":"Vapnik (1992) propose un cadre théorique pour formaliser l’apprentissage. L’idée centrale est de voir l’apprentissage comme un problème d’estimation de fonction. Nous cherchons la fonction qui minimise le risque:\\min_{f \\in \\mathcal{F}} \\mathcal{R}(f)\n\noù \\mathcal{F} est une famille de fonctions (notre classe d’hypothèses). Puisque le vrai risque est inaccessible, nous le remplaçons par le risque empirique:\\min_{f \\in \\mathcal{F}} \\hat{\\mathcal{R}}(f, \\mathcal{D})\n\nCe principe est appelé minimisation du risque empirique (MRE, en anglais empirical risk minimization, ERM).\n\nLa question centrale de la théorie de l’apprentissage statistique est: quand le minimum du risque empirique s’approche-t-il du minimum du vrai risque? Formellement, si f^\\star minimise \\mathcal{R} et \\hat{f}_N minimise \\hat{\\mathcal{R}} sur N exemples:\n\nEst-ce que \\mathcal{R}(\\hat{f}_N) converge vers \\mathcal{R}(f^\\star) lorsque N \\to \\infty?\n\nÀ quelle vitesse cette convergence se produit-elle?\n\nCes questions relèvent de la cohérence statistique et des bornes de généralisation, que nous étudierons au chapitre suivant.","type":"content","url":"/learning-problem#principe-de-minimisation-du-risque-empirique","position":17},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Fonctions de perte de substitution"},"type":"lvl2","url":"/learning-problem#fonctions-de-perte-de-substitution","position":18},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Fonctions de perte de substitution"},"content":"La perte 0-1 pose un problème pratique: elle n’est pas différentiable. Les méthodes d’optimisation par gradient, omniprésentes en apprentissage machine, requièrent des fonctions lisses. Nous utilisons donc des fonctions de perte de substitution (en anglais surrogate loss functions).\n\nUne bonne fonction de substitution est une borne supérieure convexe de la perte originale. Elle est plus facile à optimiser tout en conservant des garanties théoriques.","type":"content","url":"/learning-problem#fonctions-de-perte-de-substitution","position":19},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Perte logistique","lvl2":"Fonctions de perte de substitution"},"type":"lvl3","url":"/learning-problem#perte-logistique","position":20},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Perte logistique","lvl2":"Fonctions de perte de substitution"},"content":"Dans un contexte probabiliste de classification binaire où y \\in \\{-1, +1\\}, nous modélisons la probabilité de la classe positive par une fonction sigmoïde:p(y = 1 | x) = \\sigma(f(x)) = \\frac{1}{1 + e^{-f(x)}}\n\noù f(x) est appelé le logit (ou log-odds). La perte logistique (ou log-vraisemblance négative) est:\\ell_{\\text{log}}(y, f(x)) = \\log(1 + e^{-y \\cdot f(x)})\n\nCette fonction est convexe et différentiable partout. Elle constitue une borne supérieure de la perte 0-1.","type":"content","url":"/learning-problem#perte-logistique","position":21},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Perte à charnière","lvl2":"Fonctions de perte de substitution"},"type":"lvl3","url":"/learning-problem#perte-charni-re","position":22},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Perte à charnière","lvl2":"Fonctions de perte de substitution"},"content":"La perte à charnière (en anglais hinge loss) est utilisée dans les machines à vecteurs de support:\\ell_{\\text{hinge}}(y, f(x)) = \\max(0, 1 - y \\cdot f(x))\n\nCette fonction est convexe mais différentiable seulement par morceaux. Elle pénalise non seulement les erreurs de classification, mais aussi les prédictions correctes avec une marge insuffisante.\n\nLes deux fonctions majorent la perte 0-1: pour tout y et f(x), \\ell_{0-1} \\leq \\ell_{\\text{log}} et \\ell_{0-1} \\leq \\ell_{\\text{hinge}}. Minimiser ces substituts garantit donc un contrôle sur la perte originale.","type":"content","url":"/learning-problem#perte-charni-re","position":23},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Maximum de vraisemblance"},"type":"lvl2","url":"/learning-problem#maximum-de-vraisemblance","position":24},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Maximum de vraisemblance"},"content":"L’approche probabiliste de l’apprentissage modélise explicitement les incertitudes. Nous supposons que les données sont générées par une distribution paramétrique p(y|x; \\theta), et nous cherchons les paramètres \\theta qui expliquent le mieux les observations.","type":"content","url":"/learning-problem#maximum-de-vraisemblance","position":25},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Vraisemblance et log-vraisemblance","lvl2":"Maximum de vraisemblance"},"type":"lvl3","url":"/learning-problem#vraisemblance-et-log-vraisemblance","position":26},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Vraisemblance et log-vraisemblance","lvl2":"Maximum de vraisemblance"},"content":"La vraisemblance des paramètres \\theta étant données les données \\mathcal{D} est:p(\\mathcal{D}|\\theta) = \\prod_{i=1}^N p(y_i | x_i, \\theta)\n\nLe produit découle de l’hypothèse que les exemples sont indépendants et identiquement distribués (i.i.d.). L’estimateur du maximum de vraisemblance (EMV, en anglais maximum likelihood estimator, MLE) est:\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_\\theta p(\\mathcal{D}|\\theta)\n\nEn pratique, nous maximisons le logarithme de la vraisemblance, ce qui transforme le produit en somme:\\log p(\\mathcal{D}|\\theta) = \\sum_{i=1}^N \\log p(y_i | x_i, \\theta)\n\nPour l’optimisation, nous minimisons la log-vraisemblance négative (NLL):\\text{NLL}(\\theta) = -\\sum_{i=1}^N \\log p(y_i | x_i, \\theta)","type":"content","url":"/learning-problem#vraisemblance-et-log-vraisemblance","position":27},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Lien avec le risque empirique","lvl2":"Maximum de vraisemblance"},"type":"lvl3","url":"/learning-problem#lien-avec-le-risque-empirique","position":28},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Lien avec le risque empirique","lvl2":"Maximum de vraisemblance"},"content":"Le principe de minimisation du risque empirique et l’estimation par maximum de vraisemblance coïncident lorsque nous choisissons la perte logarithmique \\ell(y, f(x)) = -\\log p(y | f(x)). Le risque empirique devient alors:\\hat{\\mathcal{R}}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N -\\log p(y_i | x_i, \\theta) = \\frac{1}{N} \\text{NLL}(\\theta)\n\nLe minimiseur du risque empirique avec perte logarithmique est donc l’estimateur du maximum de vraisemblance.","type":"content","url":"/learning-problem#lien-avec-le-risque-empirique","position":29},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Application à la régression","lvl2":"Maximum de vraisemblance"},"type":"lvl3","url":"/learning-problem#application-la-r-gression","position":30},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Application à la régression","lvl2":"Maximum de vraisemblance"},"content":"En régression, nous modélisons souvent la distribution conditionnelle par une gaussienne:p(y|x; \\theta) = \\mathcal{N}(y | f(x; \\theta), \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - f(x; \\theta))^2}{2\\sigma^2}\\right)\n\nLa log-vraisemblance négative est alors:\\text{NLL}(\\theta) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y_i - f(x_i; \\theta))^2 + \\text{constante}\n\nLe terme dominant est l’erreur quadratique moyenne (EQM, en anglais mean squared error, MSE):\\text{MSE}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N (y_i - f(x_i; \\theta))^2\n\nMinimiser la NLL sous un modèle gaussien équivaut donc à minimiser l’erreur quadratique moyenne. Cette connexion justifie l’utilisation de la perte quadratique en régression d’un point de vue probabiliste.","type":"content","url":"/learning-problem#application-la-r-gression","position":31},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Justification informationnelle du MLE"},"type":"lvl2","url":"/learning-problem#justification-informationnelle-du-mle","position":32},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Justification informationnelle du MLE"},"content":"Nous pouvons justifier l’estimation par maximum de vraisemblance du point de vue de la théorie de l’information. L’EMV trouve la distribution paramétrique qui s’approche le plus de la distribution empirique des données.","type":"content","url":"/learning-problem#justification-informationnelle-du-mle","position":33},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Distribution empirique","lvl2":"Justification informationnelle du MLE"},"type":"lvl3","url":"/learning-problem#distribution-empirique","position":34},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Distribution empirique","lvl2":"Justification informationnelle du MLE"},"content":"La distribution empirique des données est:p_{\\mathcal{D}}(y) = \\frac{1}{N} \\sum_{i=1}^N \\delta(y - y_i)\n\noù \\delta est la fonction delta de Dirac. Cette distribution place une masse de probabilité 1/N sur chaque observation.","type":"content","url":"/learning-problem#distribution-empirique","position":35},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Divergence de Kullback-Leibler","lvl2":"Justification informationnelle du MLE"},"type":"lvl3","url":"/learning-problem#divergence-de-kullback-leibler","position":36},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Divergence de Kullback-Leibler","lvl2":"Justification informationnelle du MLE"},"content":"La divergence de Kullback-Leibler mesure la dissimilarité entre deux distributions:D_{\\text{KL}}(p \\| q) = \\sum_y p(y) \\log \\frac{p(y)}{q(y)} = -\\mathbb{H}(p) - \\sum_y p(y) \\log q(y)\n\nLe premier terme est l’entropie négative de p. Le second est l’entropie croisée entre p et q. La divergence KL satisfait D_{\\text{KL}}(p \\| q) \\geq 0 avec égalité si et seulement si p = q.","type":"content","url":"/learning-problem#divergence-de-kullback-leibler","position":37},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Connexion avec le MLE","lvl2":"Justification informationnelle du MLE"},"type":"lvl3","url":"/learning-problem#connexion-avec-le-mle","position":38},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Connexion avec le MLE","lvl2":"Justification informationnelle du MLE"},"content":"En posant p = p_{\\mathcal{D}} et q = p(\\cdot | \\theta), nous obtenons:D_{\\text{KL}}(p_{\\mathcal{D}} \\| p(\\cdot|\\theta)) = -\\mathbb{H}(p_{\\mathcal{D}}) - \\frac{1}{N} \\sum_{i=1}^N \\log p(y_i | \\theta)\n\nLe premier terme est une constante indépendante de \\theta. Minimiser la divergence KL revient donc à minimiser la log-vraisemblance négative:\\arg\\min_\\theta D_{\\text{KL}}(p_{\\mathcal{D}} \\| p(\\cdot|\\theta)) = \\arg\\min_\\theta \\text{NLL}(\\theta) = \\hat{\\theta}_{\\text{MLE}}\n\nL’EMV trouve les paramètres qui rendent le modèle aussi proche que possible de la distribution empirique au sens de la divergence KL.","type":"content","url":"/learning-problem#connexion-avec-le-mle","position":39},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Modèles de régression linéaire"},"type":"lvl2","url":"/learning-problem#mod-les-de-r-gression-lin-aire","position":40},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Modèles de régression linéaire"},"content":"Le modèle de régression le plus simple est la régression linéaire, où la prédiction est une combinaison linéaire des caractéristiques:f(x; w, b) = w^\\top x + b = \\sum_{j=1}^d w_j x_j + b\n\nLes paramètres du modèle sont le vecteur de poids w \\in \\mathbb{R}^d et le biais b \\in \\mathbb{R}. Nous regroupons souvent tous les paramètres sous le symbole \\theta = (w, b).","type":"content","url":"/learning-problem#mod-les-de-r-gression-lin-aire","position":41},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Régression polynomiale","lvl2":"Modèles de régression linéaire"},"type":"lvl3","url":"/learning-problem#r-gression-polynomiale","position":42},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Régression polynomiale","lvl2":"Modèles de régression linéaire"},"content":"Pour capturer des relations non linéaires, nous pouvons d’abord transformer les entrées. En régression polynomiale, nous appliquons une fonction de redescription \\phi: \\mathbb{R} \\to \\mathbb{R}^{k+1}:\\phi(x) = [1, x, x^2, \\ldots, x^k]\n\nLa prédiction devient f(x; w) = w^\\top \\phi(x), ce qui reste linéaire dans les paramètres mais polynomial dans l’entrée originale.\n\nLe degré k du polynôme contrôle la capacité du modèle. Avec k = N - 1, nous avons autant de paramètres que d’exemples et pouvons interpoler parfaitement les données d’entraînement. L’EQM sur l’ensemble d’entraînement atteint zéro, mais ce modèle ne généralise généralement pas bien aux nouvelles données.","type":"content","url":"/learning-problem#r-gression-polynomiale","position":43},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Vocabulaire de la généralisation"},"type":"lvl2","url":"/learning-problem#vocabulaire-de-la-g-n-ralisation","position":44},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Vocabulaire de la généralisation"},"content":"La différence entre le risque et le risque empirique sur l’ensemble d’entraînement est l’écart de généralisation:\\text{Écart de généralisation} = \\mathcal{R}(\\theta) - \\hat{\\mathcal{R}}(\\theta; \\mathcal{D}_{\\text{train}})\n\nEn pratique, nous estimons le risque par le risque empirique sur un ensemble de test \\mathcal{D}_{\\text{test}} disjoint de l’ensemble d’entraînement.\n\nUn modèle peut souffrir de deux maux opposés. Le surapprentissage (en anglais overfitting) se produit lorsque le modèle mémorise les données d’entraînement sans apprendre les régularités sous-jacentes. L’erreur d’entraînement est faible, mais l’erreur de test est élevée. À l’inverse, le sous-apprentissage (en anglais underfitting) se produit lorsque le modèle est trop simple pour capturer la structure des données. Les erreurs d’entraînement et de test sont toutes deux élevées.\n\nUn troisième ensemble, l’ensemble de validation, sert à la sélection de modèles et au réglage des hyperparamètres. L’ensemble de test doit être gardé intact jusqu’à l’évaluation finale, pour fournir une estimation non biaisée de la performance.","type":"content","url":"/learning-problem#vocabulaire-de-la-g-n-ralisation","position":45},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Biais inductifs"},"type":"lvl2","url":"/learning-problem#biais-inductifs","position":46},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Biais inductifs"},"content":"Il n’existe pas de modèle universel qui fonctionne optimalement pour tous les problèmes. Ce résultat, formalisé par Wolpert (1996), est le théorème du no free lunch.\n\nL’apprentissage dépend des biais inductifs exprimés dans nos modèles, c’est-à-dire des suppositions implicites ou explicites que nous faisons. Par exemple, la régression linéaire suppose que la relation entre entrées et sorties est linéaire. Les k plus proches voisins supposent que les points proches ont des étiquettes similaires.\n\nCes biais inductifs sont spécifiques à une classe de problèmes. Un biais approprié pour un problème peut être inapproprié pour un autre. Le choix du modèle encode donc notre connaissance a priori sur la structure du problème.","type":"content","url":"/learning-problem#biais-inductifs","position":47},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Résumé"},"type":"lvl2","url":"/learning-problem#r-sum","position":48},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Résumé"},"content":"Ce chapitre a établi le cadre formel de l’apprentissage supervisé:\n\nL’apprentissage supervisé consiste à trouver une fonction qui prédit les sorties à partir des entrées, en se basant sur des exemples étiquetés.\n\nLe risque mesure la performance attendue d’un modèle; le risque empirique est son approximation sur un échantillon.\n\nLe principe de minimisation du risque empirique guide l’apprentissage: nous choisissons le modèle qui minimise la perte moyenne sur les données d’entraînement.\n\nL’estimateur du maximum de vraisemblance coïncide avec la minimisation du risque empirique pour la perte logarithmique.\n\nLes fonctions de perte de substitution remplacent les pertes non différentiables par des approximations convexes.\n\nLe MLE trouve la distribution paramétrique qui minimise la divergence KL avec la distribution empirique.\n\nLe chapitre suivant étudie la généralisation: comment contrôler l’écart entre le risque empirique et le vrai risque, et comment choisir la complexité appropriée pour un modèle.","type":"content","url":"/learning-problem#r-sum","position":49},{"hierarchy":{"lvl1":"Classifieurs linéaires"},"type":"lvl1","url":"/linear-classifiers","position":0},{"hierarchy":{"lvl1":"Classifieurs linéaires"},"content":"Objectifs d’apprentissage\n\nÀ la fin de ce chapitre, vous serez en mesure de:\n\nFormuler la régression logistique binaire et multiclasse\n\nDériver le gradient de la log-vraisemblance négative\n\nExpliquer la fonction softmax et l’entropie croisée\n\nImplémenter la descente de gradient et ses variantes (SGD, momentum, Adam)\n\nDistinguer modèles génératifs et discriminatifs\n\nDécrire le perceptron et ses limitations\n\nInterpréter géométriquement la frontière de décision linéaire","type":"content","url":"/linear-classifiers","position":1},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl2":"Introduction"},"type":"lvl2","url":"/linear-classifiers#introduction","position":2},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl2":"Introduction"},"content":"Les classifieurs linéaires prédisent une classe à partir d’une combinaison linéaire des caractéristiques. Malgré leur simplicité, ils forment la base de nombreuses méthodes plus avancées et restent utiles en pratique lorsque les données sont approximativement séparables linéairement.\n\nCe chapitre présente la régression logistique, le modèle discriminatif de classification par excellence, ainsi que les méthodes d’optimisation par gradient nécessaires à son apprentissage.","type":"content","url":"/linear-classifiers#introduction","position":3},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl2":"Modèles génératifs vs discriminatifs"},"type":"lvl2","url":"/linear-classifiers#mod-les-g-n-ratifs-vs-discriminatifs","position":4},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl2":"Modèles génératifs vs discriminatifs"},"content":"Avant d’aborder la régression logistique, distinguons deux approches fondamentales de la classification probabiliste.","type":"content","url":"/linear-classifiers#mod-les-g-n-ratifs-vs-discriminatifs","position":5},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Modèles génératifs","lvl2":"Modèles génératifs vs discriminatifs"},"type":"lvl3","url":"/linear-classifiers#mod-les-g-n-ratifs","position":6},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Modèles génératifs","lvl2":"Modèles génératifs vs discriminatifs"},"content":"Un modèle génératif modélise la distribution jointe p(x, y) ou, de manière équivalente, la vraisemblance p(x | y) et l’a priori p(y). Par le théorème de Bayes:p(y = c | x) = \\frac{p(x | y = c) p(y = c)}{\\sum_{c'} p(x | y = c') p(y = c')}\n\nLe classifieur naïf bayésien et l’analyse discriminante gaussienne sont des exemples de modèles génératifs.\n\nAvantages:\n\nParfois plus faciles à apprendre (comptage, moyennes)\n\nGèrent naturellement les caractéristiques manquantes\n\nPeuvent générer de nouvelles instances\n\nPermettent d’ajouter de nouvelles classes sans réentraîner\n\nInconvénients:\n\nSuppositions de modélisation parfois irréalistes\n\nEstimation de p(x | y) peut être complexe en haute dimension","type":"content","url":"/linear-classifiers#mod-les-g-n-ratifs","position":7},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Modèles discriminatifs","lvl2":"Modèles génératifs vs discriminatifs"},"type":"lvl3","url":"/linear-classifiers#mod-les-discriminatifs","position":8},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Modèles discriminatifs","lvl2":"Modèles génératifs vs discriminatifs"},"content":"Un modèle discriminatif modélise directement la distribution conditionnelle p(y | x) sans passer par la vraisemblance. La régression logistique en est l’exemple canonique.\n\nAvantages:\n\nMeilleure précision prédictive (objectif directement optimisé)\n\nCompatibles avec le prétraitement des caractéristiques\n\nProbabilités souvent mieux calibrées\n\nInconvénients:\n\nNe peuvent pas générer de nouvelles instances\n\nRequièrent une optimisation numérique\n\nEn pratique, les modèles discriminatifs dominent lorsque l’objectif est uniquement la classification.","type":"content","url":"/linear-classifiers#mod-les-discriminatifs","position":9},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl2":"Régression logistique binaire"},"type":"lvl2","url":"/linear-classifiers#r-gression-logistique-binaire","position":10},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl2":"Régression logistique binaire"},"content":"","type":"content","url":"/linear-classifiers#r-gression-logistique-binaire","position":11},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Modèle probabiliste","lvl2":"Régression logistique binaire"},"type":"lvl3","url":"/linear-classifiers#mod-le-probabiliste","position":12},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Modèle probabiliste","lvl2":"Régression logistique binaire"},"content":"En régression logistique binaire, nous modélisons la probabilité de la classe positive par une fonction sigmoïde appliquée à une combinaison linéaire:p(y = 1 | x; \\theta) = \\sigma(w^\\top x + b) = \\frac{1}{1 + e^{-(w^\\top x + b)}}\n\noù \\sigma: \\mathbb{R} \\to (0, 1) est la fonction sigmoïde:\\sigma(a) = \\frac{1}{1 + e^{-a}}\n\nLe modèle complet s’écrit:p(y | x; \\theta) = \\text{Ber}(y | \\sigma(w^\\top x + b))\n\noù \\text{Ber}(y | \\mu) = \\mu^y (1 - \\mu)^{1-y} est la distribution de Bernoulli.","type":"content","url":"/linear-classifiers#mod-le-probabiliste","position":13},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Logit et log-odds","lvl2":"Régression logistique binaire"},"type":"lvl3","url":"/linear-classifiers#logit-et-log-odds","position":14},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Logit et log-odds","lvl2":"Régression logistique binaire"},"content":"La quantité a = w^\\top x + b est appelée logit ou pré-activation. Elle est reliée aux probabilités par:a = \\log \\frac{p(y = 1 | x)}{p(y = 0 | x)} = \\log \\frac{p(y = 1 | x)}{1 - p(y = 1 | x)}\n\nCette quantité est le logarithme de la cote (log-odds). La fonction sigmoïde transforme ce log-odds en probabilité.","type":"content","url":"/linear-classifiers#logit-et-log-odds","position":15},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Frontière de décision","lvl2":"Régression logistique binaire"},"type":"lvl3","url":"/linear-classifiers#fronti-re-de-d-cision","position":16},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Frontière de décision","lvl2":"Régression logistique binaire"},"content":"La frontière de décision est l’ensemble des points où les deux classes sont équiprobables:p(y = 1 | x) = p(y = 0 | x) = 0.5 \\iff w^\\top x + b = 0\n\nCette équation définit un hyperplan de vecteur normal w et de décalage b. La classification s’effectue par:\\hat{y} = \\mathbb{1}(w^\\top x + b > 0)\n\nLes données sont dites linéairement séparables si une frontière de décision linéaire peut les séparer parfaitement.","type":"content","url":"/linear-classifiers#fronti-re-de-d-cision","position":17},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl2":"Estimation par maximum de vraisemblance"},"type":"lvl2","url":"/linear-classifiers#estimation-par-maximum-de-vraisemblance","position":18},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl2":"Estimation par maximum de vraisemblance"},"content":"","type":"content","url":"/linear-classifiers#estimation-par-maximum-de-vraisemblance","position":19},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Log-vraisemblance négative","lvl2":"Estimation par maximum de vraisemblance"},"type":"lvl3","url":"/linear-classifiers#log-vraisemblance-n-gative","position":20},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Log-vraisemblance négative","lvl2":"Estimation par maximum de vraisemblance"},"content":"La log-vraisemblance négative (NLL) pour la régression logistique est:\\text{NLL}(w) = -\\frac{1}{N} \\sum_{n=1}^{N} \\left[y_n \\log \\mu_n + (1 - y_n) \\log(1 - \\mu_n)\\right]\n\noù \\mu_n = \\sigma(w^\\top x_n + b) est la probabilité prédite pour l’exemple n.\n\nCette expression est l’entropie croisée binaire moyennée:\\text{NLL}(w) = \\frac{1}{N} \\sum_{n=1}^{N} \\mathbb{H}_{\\text{ce}}(y_n, \\mu_n)\n\noù \\mathbb{H}_{\\text{ce}}(p, q) = -[p \\log q + (1-p) \\log(1-q)].","type":"content","url":"/linear-classifiers#log-vraisemblance-n-gative","position":21},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Calcul du gradient","lvl2":"Estimation par maximum de vraisemblance"},"type":"lvl3","url":"/linear-classifiers#calcul-du-gradient","position":22},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Calcul du gradient","lvl2":"Estimation par maximum de vraisemblance"},"content":"Le gradient de la NLL a une forme élégante:\\nabla_w \\text{NLL}(w) = \\frac{1}{N} \\sum_{n=1}^{N} (\\mu_n - y_n) x_n\n\nDémonstration: Pour un exemple (x_n, y_n), la perte est:\\ell_n = -y_n \\log \\mu_n - (1 - y_n) \\log(1 - \\mu_n)\n\nEn utilisant \\frac{d\\sigma}{da} = \\sigma(a)(1 - \\sigma(a)) = \\mu(1-\\mu):\\frac{\\partial \\ell_n}{\\partial w} = \\left(-\\frac{y_n}{\\mu_n} + \\frac{1 - y_n}{1 - \\mu_n}\\right) \\mu_n(1 - \\mu_n) x_n = (\\mu_n - y_n) x_n\n\nLe gradient est donc une moyenne pondérée des entrées, où le poids est l’erreur de prédiction e_n = \\mu_n - y_n.","type":"content","url":"/linear-classifiers#calcul-du-gradient","position":23},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Convexité","lvl2":"Estimation par maximum de vraisemblance"},"type":"lvl3","url":"/linear-classifiers#convexit","position":24},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Convexité","lvl2":"Estimation par maximum de vraisemblance"},"content":"La NLL de la régression logistique est strictement convexe. La matrice hessienne est:H(w) = \\frac{1}{N} X^\\top S X\n\noù S = \\text{diag}(\\mu_1(1-\\mu_1), \\ldots, \\mu_N(1-\\mu_N)) est une matrice diagonale positive.\n\nPour tout vecteur v \\neq 0:v^\\top X^\\top S X v = \\|S^{1/2} X v\\|_2^2 > 0\n\ndonc H est définie positive. La NLL admet donc un unique minimum global.","type":"content","url":"/linear-classifiers#convexit","position":25},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl2":"Régression logistique multiclasse"},"type":"lvl2","url":"/linear-classifiers#r-gression-logistique-multiclasse","position":26},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl2":"Régression logistique multiclasse"},"content":"","type":"content","url":"/linear-classifiers#r-gression-logistique-multiclasse","position":27},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Fonction softmax","lvl2":"Régression logistique multiclasse"},"type":"lvl3","url":"/linear-classifiers#fonction-softmax","position":28},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Fonction softmax","lvl2":"Régression logistique multiclasse"},"content":"Pour la classification à C classes, nous généralisons la sigmoïde par la fonction softmax:p(y = c | x; \\theta) = \\text{softmax}(a)_c = \\frac{e^{a_c}}{\\sum_{c'=1}^{C} e^{a_{c'}}}\n\noù a = Wx + b est le vecteur de logits, W \\in \\mathbb{R}^{C \\times d} est la matrice de poids, et b \\in \\mathbb{R}^C est le vecteur de biais.\n\nLa fonction softmax transforme un vecteur de scores arbitraires en distribution de probabilité: les sorties sont positives et somment à 1.","type":"content","url":"/linear-classifiers#fonction-softmax","position":29},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Entropie croisée catégorielle","lvl2":"Régression logistique multiclasse"},"type":"lvl3","url":"/linear-classifiers#entropie-crois-e-cat-gorielle","position":30},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Entropie croisée catégorielle","lvl2":"Régression logistique multiclasse"},"content":"La log-vraisemblance négative devient:\\text{NLL}(\\theta) = -\\frac{1}{N} \\sum_{n=1}^{N} \\sum_{c=1}^{C} y_{nc} \\log \\mu_{nc} = \\frac{1}{N} \\sum_{n=1}^{N} \\mathbb{H}_{\\text{ce}}(y_n, \\mu_n)\n\noù y_{nc} = \\mathbb{1}(y_n = c) est l’encodage one-hot et \\mu_{nc} = p(y = c | x_n).\n\nL’entropie croisée catégorielle est:\\mathbb{H}_{\\text{ce}}(p, q) = -\\sum_{c=1}^{C} p_c \\log q_c","type":"content","url":"/linear-classifiers#entropie-crois-e-cat-gorielle","position":31},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl2":"Optimisation par gradient"},"type":"lvl2","url":"/linear-classifiers#optimisation-par-gradient","position":32},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl2":"Optimisation par gradient"},"content":"Il n’existe pas de solution analytique pour la régression logistique. Nous devons recourir à l’optimisation numérique.","type":"content","url":"/linear-classifiers#optimisation-par-gradient","position":33},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Descente de gradient","lvl2":"Optimisation par gradient"},"type":"lvl3","url":"/linear-classifiers#descente-de-gradient","position":34},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Descente de gradient","lvl2":"Optimisation par gradient"},"content":"La descente de gradient met à jour itérativement les paramètres dans la direction opposée au gradient:\\theta_{t+1} = \\theta_t - \\eta_t \\nabla \\mathcal{L}(\\theta_t)\n\noù \\eta_t > 0 est le taux d’apprentissage (ou pas d’apprentissage).\n\nUn vecteur d est une direction de descente si d^\\top \\nabla \\mathcal{L} < 0. Le gradient négatif est la direction de descente la plus rapide (pour la norme euclidienne).","type":"content","url":"/linear-classifiers#descente-de-gradient","position":35},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Descente de gradient stochastique","lvl2":"Optimisation par gradient"},"type":"lvl3","url":"/linear-classifiers#descente-de-gradient-stochastique","position":36},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Descente de gradient stochastique","lvl2":"Optimisation par gradient"},"content":"La descente de gradient stochastique (SGD) remplace le gradient exact par une estimation basée sur un sous-ensemble des données:\\theta_{t+1} = \\theta_t - \\eta_t \\nabla \\mathcal{L}(\\theta_t, z_t)\n\noù z_t est un exemple (ou mini-lot) échantillonné aléatoirement.\n\nMini-lots: En pratique, nous utilisons des mini-lots de taille B:g_t = \\frac{1}{B} \\sum_{n \\in \\mathcal{B}_t} \\nabla \\ell(y_n, f(x_n; \\theta_t))\n\nCette estimation est non biaisée et réduit la variance par rapport au SGD pur (B = 1).\n\nAvantages du SGD:\n\nPlus rapide en pratique que le gradient complet\n\nRégularisation implicite par le bruit\n\nPermet l’apprentissage en ligne (streaming)","type":"content","url":"/linear-classifiers#descente-de-gradient-stochastique","position":37},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Momentum","lvl2":"Optimisation par gradient"},"type":"lvl3","url":"/linear-classifiers#momentum","position":38},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Momentum","lvl2":"Optimisation par gradient"},"content":"L’algorithme de momentum (Polyak, 1964) accumule les gradients passés pour atténuer les oscillations:\\begin{aligned}\nm_t &= \\beta m_{t-1} + g_{t-1} \\\\\n\\theta_t &= \\theta_{t-1} - \\eta_t m_t\n\\end{aligned}\n\nLe terme m_t est une moyenne mobile exponentielle des gradients:m_t = \\sum_{\\tau=0}^{t-1} \\beta^\\tau g_{t-\\tau-1}\n\nLe paramètre \\beta \\in [0, 1) contrôle la mémoire (typiquement \\beta = 0.9).","type":"content","url":"/linear-classifiers#momentum","position":39},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Taux d’apprentissage adaptatifs","lvl2":"Optimisation par gradient"},"type":"lvl3","url":"/linear-classifiers#taux-dapprentissage-adaptatifs","position":40},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Taux d’apprentissage adaptatifs","lvl2":"Optimisation par gradient"},"content":"RMSProp adapte le taux d’apprentissage à chaque paramètre:\\begin{aligned}\ns_{t+1,d} &= \\beta s_{t,d} + (1 - \\beta) g_{t,d}^2 \\\\\n\\theta_{t+1} &= \\theta_t - \\frac{\\eta_t}{\\sqrt{s_t + \\epsilon}} \\odot g_t\n\\end{aligned}\n\noù \\odot désigne le produit élément par élément.\n\nAdam combine momentum et RMSProp:\\begin{aligned}\nm_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) g_t \\\\\ns_t &= \\beta_2 s_{t-1} + (1 - \\beta_2) g_t^2 \\\\\n\\theta_{t+1} &= \\theta_t - \\frac{\\eta_t}{\\sqrt{s_t} + \\epsilon} m_t\n\\end{aligned}\n\navec correction de biais pour les premières itérations:\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{s}_t = \\frac{s_t}{1 - \\beta_2^t}\n\nValeurs typiques: \\beta_1 = 0.9, \\beta_2 = 0.999, \\epsilon = 10^{-8}.","type":"content","url":"/linear-classifiers#taux-dapprentissage-adaptatifs","position":41},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Planification du taux d’apprentissage","lvl2":"Optimisation par gradient"},"type":"lvl3","url":"/linear-classifiers#planification-du-taux-dapprentissage","position":42},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Planification du taux d’apprentissage","lvl2":"Optimisation par gradient"},"content":"Le taux d’apprentissage peut varier au cours de l’entraînement. Les conditions de Robbins-Monro garantissent la convergence:\\sum_{t=1}^{\\infty} \\eta_t = \\infty, \\quad \\sum_{t=1}^{\\infty} \\eta_t^2 < \\infty\n\nExemples de planifications:\n\nDécroissance polynomiale: \\eta_t = \\eta_0 (\\beta t + 1)^{-\\alpha}\n\nDécroissance exponentielle: \\eta_t = \\eta_0 e^{-\\lambda t}\n\nConstante par morceaux: réduction par facteur à des époques fixées\n\nLa planification en racine carrée \\eta_t = \\eta_0 / \\sqrt{t+1} est un choix courant.","type":"content","url":"/linear-classifiers#planification-du-taux-dapprentissage","position":43},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl2":"Le perceptron"},"type":"lvl2","url":"/linear-classifiers#le-perceptron","position":44},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl2":"Le perceptron"},"content":"Le perceptron (Rosenblatt, 1957) est un précurseur de la régression logistique:f(x; w, b) = \\mathbb{1}(w^\\top x + b > 0)\n\nLa règle de mise à jour est:w_{t+1} = w_t - \\eta_t (\\hat{y}_n - y_n) x_n\n\noù \\hat{y}_n = f(x_n) est la prédiction (0 ou 1).","type":"content","url":"/linear-classifiers#le-perceptron","position":45},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Comparaison avec la régression logistique","lvl2":"Le perceptron"},"type":"lvl3","url":"/linear-classifiers#comparaison-avec-la-r-gression-logistique","position":46},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Comparaison avec la régression logistique","lvl2":"Le perceptron"},"content":"Aspect\n\nPerceptron\n\nRégression logistique\n\nSortie\n\nBinaire \\{0, 1\\}\n\nProbabilité [0, 1]\n\nFonction\n\nMarche d’escalier\n\nSigmoïde\n\nDifférentiabilité\n\nNon\n\nOui\n\nConvergence\n\nSeulement si séparable\n\nToujours\n\nLe perceptron converge en un nombre fini d’itérations si les données sont linéairement séparables (théorème de convergence du perceptron). Sinon, il oscille indéfiniment.","type":"content","url":"/linear-classifiers#comparaison-avec-la-r-gression-logistique","position":47},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl2":"Transformation non linéaire"},"type":"lvl2","url":"/linear-classifiers#transformation-non-lin-aire","position":48},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl2":"Transformation non linéaire"},"content":"Lorsque les données ne sont pas linéairement séparables, nous pouvons les transformer dans un espace de dimension supérieure où elles le deviennent.\n\nPar exemple, l’expansion polynomiale de degré 2 transforme x = (x_1, x_2) en:\\phi(x) = [1, x_1, x_2, x_1^2, x_2^2, x_1 x_2]\n\nLa frontière de décision dans l’espace original devient quadratique.\n\nCette idée est généralisée par les méthodes à noyau (chapitre SVM), où la transformation peut être implicite.","type":"content","url":"/linear-classifiers#transformation-non-lin-aire","position":49},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl2":"Implémentation"},"type":"lvl2","url":"/linear-classifiers#impl-mentation","position":50},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl2":"Implémentation"},"content":"","type":"content","url":"/linear-classifiers#impl-mentation","position":51},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Régression logistique binaire","lvl2":"Implémentation"},"type":"lvl3","url":"/linear-classifiers#r-gression-logistique-binaire-1","position":52},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Régression logistique binaire","lvl2":"Implémentation"},"content":"import numpy as np\n\ndef sigmoid(a):\n    \"\"\"Fonction sigmoïde numériquement stable.\"\"\"\n    return np.where(a >= 0,\n                    1 / (1 + np.exp(-a)),\n                    np.exp(a) / (1 + np.exp(a)))\n\ndef logistic_regression_sgd(X, y, lr=0.01, n_epochs=100, batch_size=32):\n    \"\"\"Régression logistique par SGD.\n    \n    Args:\n        X: Matrice de design (N x d)\n        y: Étiquettes binaires (N,)\n        lr: Taux d'apprentissage\n        n_epochs: Nombre d'époques\n        batch_size: Taille des mini-lots\n        \n    Returns:\n        Vecteur de poids (d,)\n    \"\"\"\n    N, d = X.shape\n    w = np.zeros(d)\n    \n    for epoch in range(n_epochs):\n        # Mélanger les données\n        indices = np.random.permutation(N)\n        \n        for start in range(0, N, batch_size):\n            end = min(start + batch_size, N)\n            batch_idx = indices[start:end]\n            \n            X_batch = X[batch_idx]\n            y_batch = y[batch_idx]\n            \n            # Prédictions\n            mu = sigmoid(X_batch @ w)\n            \n            # Gradient\n            grad = X_batch.T @ (mu - y_batch) / len(batch_idx)\n            \n            # Mise à jour\n            w = w - lr * grad\n            \n    return w\n\ndef predict_proba(X, w):\n    \"\"\"Probabilités de la classe positive.\"\"\"\n    return sigmoid(X @ w)\n\ndef predict(X, w, threshold=0.5):\n    \"\"\"Prédictions binaires.\"\"\"\n    return (predict_proba(X, w) >= threshold).astype(int)","type":"content","url":"/linear-classifiers#r-gression-logistique-binaire-1","position":53},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Régression logistique multiclasse","lvl2":"Implémentation"},"type":"lvl3","url":"/linear-classifiers#r-gression-logistique-multiclasse-1","position":54},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl3":"Régression logistique multiclasse","lvl2":"Implémentation"},"content":"def softmax(a):\n    \"\"\"Fonction softmax numériquement stable.\"\"\"\n    a_max = np.max(a, axis=1, keepdims=True)\n    exp_a = np.exp(a - a_max)\n    return exp_a / np.sum(exp_a, axis=1, keepdims=True)\n\ndef multiclass_logistic_sgd(X, y, n_classes, lr=0.01, n_epochs=100, batch_size=32):\n    \"\"\"Régression logistique multiclasse par SGD.\n    \n    Args:\n        X: Matrice de design (N x d)\n        y: Étiquettes (N,) avec valeurs dans {0, ..., C-1}\n        n_classes: Nombre de classes C\n        lr: Taux d'apprentissage\n        n_epochs: Nombre d'époques\n        batch_size: Taille des mini-lots\n        \n    Returns:\n        Matrice de poids (C x d)\n    \"\"\"\n    N, d = X.shape\n    W = np.zeros((n_classes, d))\n    \n    # Encodage one-hot\n    Y_onehot = np.eye(n_classes)[y]\n    \n    for epoch in range(n_epochs):\n        indices = np.random.permutation(N)\n        \n        for start in range(0, N, batch_size):\n            end = min(start + batch_size, N)\n            batch_idx = indices[start:end]\n            \n            X_batch = X[batch_idx]\n            Y_batch = Y_onehot[batch_idx]\n            \n            # Prédictions\n            logits = X_batch @ W.T\n            probs = softmax(logits)\n            \n            # Gradient\n            grad = (probs - Y_batch).T @ X_batch / len(batch_idx)\n            \n            # Mise à jour\n            W = W - lr * grad\n            \n    return W","type":"content","url":"/linear-classifiers#r-gression-logistique-multiclasse-1","position":55},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl2":"Résumé"},"type":"lvl2","url":"/linear-classifiers#r-sum","position":56},{"hierarchy":{"lvl1":"Classifieurs linéaires","lvl2":"Résumé"},"content":"Ce chapitre a présenté les classifieurs linéaires discriminatifs:\n\nLa régression logistique modélise p(y|x) directement via une sigmoïde\n\nLa NLL équivaut à l’entropie croisée, objectif convexe avec minimum unique\n\nLe gradient a une forme simple: \\sum_n (\\mu_n - y_n) x_n\n\nLa fonction softmax généralise la sigmoïde à plusieurs classes\n\nSGD et ses variantes (momentum, Adam) permettent l’optimisation à grande échelle\n\nLe perceptron est un cas limite avec prédictions binaires\n\nLes transformations non linéaires permettent des frontières non linéaires\n\nLe chapitre suivant étend ces idées aux machines à vecteurs de support, qui optimisent la marge de classification.","type":"content","url":"/linear-classifiers#r-sum","position":57},{"hierarchy":{"lvl1":"Régression linéaire"},"type":"lvl1","url":"/linear-regression","position":0},{"hierarchy":{"lvl1":"Régression linéaire"},"content":"Objectifs d’apprentissage\n\nÀ la fin de ce chapitre, vous serez en mesure de:\n\nFormuler le problème de régression linéaire\n\nDériver la solution des moindres carrés ordinaires (MCO)\n\nRelier les MCO à l’estimateur du maximum de vraisemblance\n\nDistinguer régression homoscédastique et hétéroscédastique\n\nÉtendre le modèle à la régression polynomiale\n\nAppliquer la régression ridge pour la régularisation\n\nImplémenter la régression linéaire par formule analytique et gradient","type":"content","url":"/linear-regression","position":1},{"hierarchy":{"lvl1":"Régression linéaire","lvl2":"Introduction"},"type":"lvl2","url":"/linear-regression#introduction","position":2},{"hierarchy":{"lvl1":"Régression linéaire","lvl2":"Introduction"},"content":"La régression linéaire est le modèle fondamental de l’apprentissage supervisé pour les problèmes à sortie continue. Malgré sa simplicité, elle reste omniprésente en pratique: comme premier modèle de base, comme composante de modèles plus complexes, et comme outil d’analyse et d’interprétation.\n\nCe chapitre développe la régression linéaire sous trois angles complémentaires: géométrique (projection), statistique (maximum de vraisemblance), et algorithmique (optimisation).","type":"content","url":"/linear-regression#introduction","position":3},{"hierarchy":{"lvl1":"Régression linéaire","lvl2":"Formulation du problème"},"type":"lvl2","url":"/linear-regression#formulation-du-probl-me","position":4},{"hierarchy":{"lvl1":"Régression linéaire","lvl2":"Formulation du problème"},"content":"","type":"content","url":"/linear-regression#formulation-du-probl-me","position":5},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Le modèle linéaire","lvl2":"Formulation du problème"},"type":"lvl3","url":"/linear-regression#le-mod-le-lin-aire","position":6},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Le modèle linéaire","lvl2":"Formulation du problème"},"content":"En régression linéaire, nous modélisons la cible comme une combinaison linéaire des caractéristiques:f(x; w, b) = w^\\top x + b = \\sum_{j=1}^{d} w_j x_j + b\n\noù:\n\nx \\in \\mathbb{R}^d est le vecteur de caractéristiques\n\nw \\in \\mathbb{R}^d est le vecteur de poids\n\nb \\in \\mathbb{R} est le biais (ou ordonnée à l’origine)\n\nPour simplifier la notation, nous absorbons souvent le biais dans les poids en ajoutant une caractéristique constante égale à 1:\\tilde{x} = [1, x_1, \\ldots, x_d]^\\top \\in \\mathbb{R}^{d+1}, \\quad \\tilde{w} = [b, w_1, \\ldots, w_d]^\\top\n\nLa prédiction devient alors f(x; \\tilde{w}) = \\tilde{w}^\\top \\tilde{x}.","type":"content","url":"/linear-regression#le-mod-le-lin-aire","position":7},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Notation matricielle","lvl2":"Formulation du problème"},"type":"lvl3","url":"/linear-regression#notation-matricielle","position":8},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Notation matricielle","lvl2":"Formulation du problème"},"content":"Pour un ensemble de N exemples, nous formons la matrice de design X \\in \\mathbb{R}^{N \\times d} dont chaque ligne est un exemple:X = \\begin{bmatrix}\nx_1^\\top \\\\\nx_2^\\top \\\\\n\\vdots \\\\\nx_N^\\top\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_N\n\\end{bmatrix}\n\nLes prédictions pour tous les exemples s’écrivent \\hat{y} = Xw.","type":"content","url":"/linear-regression#notation-matricielle","position":9},{"hierarchy":{"lvl1":"Régression linéaire","lvl2":"Moindres carrés ordinaires"},"type":"lvl2","url":"/linear-regression#moindres-carr-s-ordinaires","position":10},{"hierarchy":{"lvl1":"Régression linéaire","lvl2":"Moindres carrés ordinaires"},"content":"","type":"content","url":"/linear-regression#moindres-carr-s-ordinaires","position":11},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Fonction objectif","lvl2":"Moindres carrés ordinaires"},"type":"lvl3","url":"/linear-regression#fonction-objectif","position":12},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Fonction objectif","lvl2":"Moindres carrés ordinaires"},"content":"L’objectif est de trouver les poids qui minimisent la somme des carrés des résidus (SCR, en anglais residual sum of squares, RSS):\\text{RSS}(w) = \\sum_{n=1}^{N} (y_n - w^\\top x_n)^2 = \\|y - Xw\\|_2^2\n\nLe résidu r_n = y_n - w^\\top x_n est l’écart entre la valeur observée et la prédiction.\n\nEn développant la norme:\\text{RSS}(w) = (y - Xw)^\\top (y - Xw) = y^\\top y - 2w^\\top X^\\top y + w^\\top X^\\top X w","type":"content","url":"/linear-regression#fonction-objectif","position":13},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Dérivation de la solution","lvl2":"Moindres carrés ordinaires"},"type":"lvl3","url":"/linear-regression#d-rivation-de-la-solution","position":14},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Dérivation de la solution","lvl2":"Moindres carrés ordinaires"},"content":"Pour trouver le minimum, nous annulons le gradient par rapport à w:\\nabla_w \\text{RSS}(w) = -2X^\\top y + 2X^\\top X w = 0\n\nCe qui donne les équations normales:X^\\top X w = X^\\top y\n\nSi X^\\top X est inversible (ce qui requiert que X soit de rang plein), la solution est:\\hat{w}_{\\text{MCO}} = (X^\\top X)^{-1} X^\\top y\n\nCette solution porte le nom d’estimateur des moindres carrés ordinaires (MCO, en anglais ordinary least squares, OLS).","type":"content","url":"/linear-regression#d-rivation-de-la-solution","position":15},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Interprétation géométrique","lvl2":"Moindres carrés ordinaires"},"type":"lvl3","url":"/linear-regression#interpr-tation-g-om-trique","position":16},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Interprétation géométrique","lvl2":"Moindres carrés ordinaires"},"content":"La solution des MCO a une interprétation géométrique élégante. Les colonnes de X définissent un sous-espace de \\mathbb{R}^N. Le vecteur \\hat{y} = X\\hat{w} est la projection orthogonale de y sur ce sous-espace.\n\nLe résidu r = y - \\hat{y} est orthogonal à l’espace colonnes de X:X^\\top (y - X\\hat{w}) = X^\\top y - X^\\top X \\hat{w} = 0\n\nLa matrice H = X(X^\\top X)^{-1}X^\\top est appelée matrice chapeau (hat matrix) car \\hat{y} = Hy. C’est une matrice de projection.","type":"content","url":"/linear-regression#interpr-tation-g-om-trique","position":17},{"hierarchy":{"lvl1":"Régression linéaire","lvl2":"Interprétation probabiliste"},"type":"lvl2","url":"/linear-regression#interpr-tation-probabiliste","position":18},{"hierarchy":{"lvl1":"Régression linéaire","lvl2":"Interprétation probabiliste"},"content":"","type":"content","url":"/linear-regression#interpr-tation-probabiliste","position":19},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Modèle génératif","lvl2":"Interprétation probabiliste"},"type":"lvl3","url":"/linear-regression#mod-le-g-n-ratif","position":20},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Modèle génératif","lvl2":"Interprétation probabiliste"},"content":"Nous pouvons interpréter la régression linéaire dans un cadre probabiliste. Supposons que les données soient générées selon:y = w^\\top x + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\n\nLe bruit \\epsilon capture les variations non expliquées par le modèle linéaire. Ceci implique que la distribution conditionnelle de y sachant x est:p(y | x; w, \\sigma^2) = \\mathcal{N}(y | w^\\top x, \\sigma^2)","type":"content","url":"/linear-regression#mod-le-g-n-ratif","position":21},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Maximum de vraisemblance","lvl2":"Interprétation probabiliste"},"type":"lvl3","url":"/linear-regression#maximum-de-vraisemblance","position":22},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Maximum de vraisemblance","lvl2":"Interprétation probabiliste"},"content":"Sous l’hypothèse que les exemples sont indépendants et identiquement distribués, la vraisemblance est:p(\\mathcal{D} | w, \\sigma^2) = \\prod_{n=1}^{N} \\mathcal{N}(y_n | w^\\top x_n, \\sigma^2)\n\nLa log-vraisemblance négative (NLL) est:\\text{NLL}(w) = -\\sum_{n=1}^{N} \\log \\mathcal{N}(y_n | w^\\top x_n, \\sigma^2)\n\nEn développant l’expression de la gaussienne:\\text{NLL}(w) = \\frac{N}{2} \\log(2\\pi\\sigma^2) + \\frac{1}{2\\sigma^2} \\sum_{n=1}^{N} (y_n - w^\\top x_n)^2\n\nLe premier terme est constant en w. Minimiser la NLL équivaut donc à minimiser la somme des carrés des résidus:\\hat{w}_{\\text{MLE}} = \\arg\\min_w \\text{NLL}(w) = \\arg\\min_w \\text{RSS}(w) = \\hat{w}_{\\text{MCO}}\n\nConclusion importante: l’estimateur des moindres carrés ordinaires coïncide avec l’estimateur du maximum de vraisemblance sous un modèle gaussien.","type":"content","url":"/linear-regression#maximum-de-vraisemblance","position":23},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Estimation de la variance","lvl2":"Interprétation probabiliste"},"type":"lvl3","url":"/linear-regression#estimation-de-la-variance","position":24},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Estimation de la variance","lvl2":"Interprétation probabiliste"},"content":"Une fois \\hat{w} obtenu, nous pouvons estimer la variance du bruit:\\hat{\\sigma}^2_{\\text{MLE}} = \\frac{1}{N} \\sum_{n=1}^{N} (y_n - \\hat{w}^\\top x_n)^2 = \\frac{\\text{RSS}(\\hat{w})}{N}\n\nCette estimation est biaisée. L’estimateur non biaisé utilise N - d au dénominateur pour tenir compte des degrés de liberté:\\hat{\\sigma}^2 = \\frac{\\text{RSS}(\\hat{w})}{N - d}","type":"content","url":"/linear-regression#estimation-de-la-variance","position":25},{"hierarchy":{"lvl1":"Régression linéaire","lvl2":"Métriques d’évaluation"},"type":"lvl2","url":"/linear-regression#m-triques-d-valuation","position":26},{"hierarchy":{"lvl1":"Régression linéaire","lvl2":"Métriques d’évaluation"},"content":"","type":"content","url":"/linear-regression#m-triques-d-valuation","position":27},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Erreur quadratique moyenne","lvl2":"Métriques d’évaluation"},"type":"lvl3","url":"/linear-regression#erreur-quadratique-moyenne","position":28},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Erreur quadratique moyenne","lvl2":"Métriques d’évaluation"},"content":"L’erreur quadratique moyenne (EQM, en anglais mean squared error, MSE) est la RSS divisée par N:\\text{MSE}(w) = \\frac{1}{N} \\text{RSS}(w) = \\frac{1}{N} \\sum_{n=1}^{N} (y_n - w^\\top x_n)^2","type":"content","url":"/linear-regression#erreur-quadratique-moyenne","position":29},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Racine de l’erreur quadratique moyenne","lvl2":"Métriques d’évaluation"},"type":"lvl3","url":"/linear-regression#racine-de-lerreur-quadratique-moyenne","position":30},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Racine de l’erreur quadratique moyenne","lvl2":"Métriques d’évaluation"},"content":"La racine de l’EQM (REQM, en anglais root mean squared error, RMSE) est dans les mêmes unités que la cible:\\text{RMSE}(w) = \\sqrt{\\text{MSE}(w)}","type":"content","url":"/linear-regression#racine-de-lerreur-quadratique-moyenne","position":31},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Coefficient de détermination","lvl2":"Métriques d’évaluation"},"type":"lvl3","url":"/linear-regression#coefficient-de-d-termination","position":32},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Coefficient de détermination","lvl2":"Métriques d’évaluation"},"content":"Le coefficient de détermination R^2 mesure la proportion de variance expliquée par le modèle:R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}} = 1 - \\frac{\\sum_n (y_n - \\hat{y}_n)^2}{\\sum_n (y_n - \\bar{y})^2}\n\noù TSS (total sum of squares) est la variance totale autour de la moyenne \\bar{y}.\n\nR^2 = 1: le modèle explique toute la variance\n\nR^2 = 0: le modèle n’est pas meilleur que prédire la moyenne\n\nR^2 < 0: le modèle est pire que prédire la moyenne (possible sur l’ensemble de test)","type":"content","url":"/linear-regression#coefficient-de-d-termination","position":33},{"hierarchy":{"lvl1":"Régression linéaire","lvl2":"Homoscédasticité et hétéroscédasticité"},"type":"lvl2","url":"/linear-regression#homosc-dasticit-et-h-t-rosc-dasticit","position":34},{"hierarchy":{"lvl1":"Régression linéaire","lvl2":"Homoscédasticité et hétéroscédasticité"},"content":"","type":"content","url":"/linear-regression#homosc-dasticit-et-h-t-rosc-dasticit","position":35},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Modèle homoscédastique","lvl2":"Homoscédasticité et hétéroscédasticité"},"type":"lvl3","url":"/linear-regression#mod-le-homosc-dastique","position":36},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Modèle homoscédastique","lvl2":"Homoscédasticité et hétéroscédasticité"},"content":"Le modèle standard suppose une variance constante (homoscédasticité):p(y | x; w, \\sigma^2) = \\mathcal{N}(y | w^\\top x, \\sigma^2)\n\nLa variance \\sigma^2 ne dépend pas de x.","type":"content","url":"/linear-regression#mod-le-homosc-dastique","position":37},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Modèle hétéroscédastique","lvl2":"Homoscédasticité et hétéroscédasticité"},"type":"lvl3","url":"/linear-regression#mod-le-h-t-rosc-dastique","position":38},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Modèle hétéroscédastique","lvl2":"Homoscédasticité et hétéroscédasticité"},"content":"Dans un modèle hétéroscédastique, la variance dépend aussi de l’entrée:p(y | x; w_\\mu, w_\\sigma) = \\mathcal{N}(y | w_\\mu^\\top x, \\sigma_+(w_\\sigma^\\top x))\n\noù \\sigma_+(a) = \\log(1 + e^a) est la fonction softplus qui garantit une variance positive.\n\nCe modèle exprime l’incertitude aleatoire (ou incertitude irréductible): pour certaines valeurs de x, les observations sont intrinsèquement plus variables. Par exemple, prédire le prix de maisons de luxe peut être plus incertain que celui de maisons modestes.\n\nLa log-vraisemblance négative devient:\\text{NLL}(w_\\mu, w_\\sigma) = \\sum_{n=1}^{N} \\left[\\log \\sigma_+(w_\\sigma^\\top x_n) + \\frac{(y_n - w_\\mu^\\top x_n)^2}{2\\sigma_+(w_\\sigma^\\top x_n)}\\right]\n\nCette expression n’a pas de solution analytique et doit être optimisée par gradient.","type":"content","url":"/linear-regression#mod-le-h-t-rosc-dastique","position":39},{"hierarchy":{"lvl1":"Régression linéaire","lvl2":"Régression polynomiale"},"type":"lvl2","url":"/linear-regression#r-gression-polynomiale","position":40},{"hierarchy":{"lvl1":"Régression linéaire","lvl2":"Régression polynomiale"},"content":"","type":"content","url":"/linear-regression#r-gression-polynomiale","position":41},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Extension non linéaire","lvl2":"Régression polynomiale"},"type":"lvl3","url":"/linear-regression#extension-non-lin-aire","position":42},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Extension non linéaire","lvl2":"Régression polynomiale"},"content":"La régression linéaire suppose une relation linéaire entre caractéristiques et cible. Pour capturer des relations non linéaires, nous pouvons transformer les caractéristiques.\n\nEn régression polynomiale, nous appliquons une fonction de redescription \\phi: \\mathbb{R} \\to \\mathbb{R}^{k+1}:\\phi(x) = [1, x, x^2, \\ldots, x^k]\n\nLe modèle devient:f(x; w) = w^\\top \\phi(x) = w_0 + w_1 x + w_2 x^2 + \\cdots + w_k x^k\n\nLe modèle reste linéaire dans les paramètres w, même s’il est non linéaire dans l’entrée x.","type":"content","url":"/linear-regression#extension-non-lin-aire","position":43},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Généralisation aux fonctions de base","lvl2":"Régression polynomiale"},"type":"lvl3","url":"/linear-regression#g-n-ralisation-aux-fonctions-de-base","position":44},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Généralisation aux fonctions de base","lvl2":"Régression polynomiale"},"content":"Plus généralement, nous pouvons utiliser n’importe quel ensemble de fonctions de base \\phi_1, \\ldots, \\phi_m:f(x; w) = \\sum_{j=1}^{m} w_j \\phi_j(x)\n\nExemples:\n\nPolynômes: \\phi_j(x) = x^{j-1}\n\nFonctions de base radiales: \\phi_j(x) = \\exp(-\\|x - c_j\\|^2 / 2\\sigma^2)\n\nFonctions trigonométriques: \\sin(j\\omega x), \\cos(j\\omega x)\n\nLa solution des MCO s’applique directement avec la matrice de design transformée \\Phi dont l’entrée (n, j) est \\phi_j(x_n).","type":"content","url":"/linear-regression#g-n-ralisation-aux-fonctions-de-base","position":45},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Surapprentissage","lvl2":"Régression polynomiale"},"type":"lvl3","url":"/linear-regression#surapprentissage","position":46},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Surapprentissage","lvl2":"Régression polynomiale"},"content":"Un degré polynomial trop élevé mène au surapprentissage. Avec k = N - 1 paramètres, nous pouvons interpoler parfaitement les N points d’entraînement, mais le polynôme oscillera violemment entre les points.\n\nLe choix du degré est un problème de sélection de modèle, résolu par validation croisée ou régularisation.","type":"content","url":"/linear-regression#surapprentissage","position":47},{"hierarchy":{"lvl1":"Régression linéaire","lvl2":"Régression ridge"},"type":"lvl2","url":"/linear-regression#r-gression-ridge","position":48},{"hierarchy":{"lvl1":"Régression linéaire","lvl2":"Régression ridge"},"content":"","type":"content","url":"/linear-regression#r-gression-ridge","position":49},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Motivation","lvl2":"Régression ridge"},"type":"lvl3","url":"/linear-regression#motivation","position":50},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Motivation","lvl2":"Régression ridge"},"content":"La solution MCO (X^\\top X)^{-1} X^\\top y peut être instable si:\n\nX^\\top X est mal conditionnée (proche de singulière)\n\nLe nombre de caractéristiques d est proche ou supérieur au nombre d’exemples N\n\nLes caractéristiques sont fortement corrélées (multicolinéarité)\n\nLa régression ridge ajoute une pénalité sur la norme des poids.","type":"content","url":"/linear-regression#motivation","position":51},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Formulation","lvl2":"Régression ridge"},"type":"lvl3","url":"/linear-regression#formulation","position":52},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Formulation","lvl2":"Régression ridge"},"content":"L’objectif de la régression ridge est:\\hat{w}_{\\text{ridge}} = \\arg\\min_w \\left[\\|y - Xw\\|_2^2 + \\lambda \\|w\\|_2^2\\right]\n\nLe terme \\lambda \\|w\\|_2^2 pénalise les poids de grande magnitude. Le coefficient de régularisation \\lambda > 0 contrôle la force de cette pénalité.","type":"content","url":"/linear-regression#formulation","position":53},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Solution analytique","lvl2":"Régression ridge"},"type":"lvl3","url":"/linear-regression#solution-analytique","position":54},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Solution analytique","lvl2":"Régression ridge"},"content":"En annulant le gradient:\\nabla_w \\left[\\|y - Xw\\|_2^2 + \\lambda \\|w\\|_2^2\\right] = -2X^\\top(y - Xw) + 2\\lambda w = 0\n\nLa solution est:\\hat{w}_{\\text{ridge}} = (X^\\top X + \\lambda I)^{-1} X^\\top y\n\nL’ajout de \\lambda I à X^\\top X garantit l’inversibilité et améliore le conditionnement.","type":"content","url":"/linear-regression#solution-analytique","position":55},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Interprétation bayésienne","lvl2":"Régression ridge"},"type":"lvl3","url":"/linear-regression#interpr-tation-bay-sienne","position":56},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Interprétation bayésienne","lvl2":"Régression ridge"},"content":"La régression ridge correspond à l’estimation MAP avec un a priori gaussien sur les poids:p(w) = \\mathcal{N}(w | 0, \\tau^2 I)\n\nLe coefficient de régularisation est relié à la variance de l’a priori par \\lambda = \\sigma^2 / \\tau^2.","type":"content","url":"/linear-regression#interpr-tation-bay-sienne","position":57},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Effet de la régularisation","lvl2":"Régression ridge"},"type":"lvl3","url":"/linear-regression#effet-de-la-r-gularisation","position":58},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Effet de la régularisation","lvl2":"Régression ridge"},"content":"\\lambda = 0: pas de régularisation, solution MCO\n\n\\lambda \\to \\infty: les poids tendent vers zéro\n\n\\lambda optimal: compromis biais-variance\n\nLa régularisation introduit un biais (les poids sont “rétrécis” vers zéro) mais réduit la variance. La valeur optimale de \\lambda se détermine par validation croisée.","type":"content","url":"/linear-regression#effet-de-la-r-gularisation","position":59},{"hierarchy":{"lvl1":"Régression linéaire","lvl2":"Implémentation"},"type":"lvl2","url":"/linear-regression#impl-mentation","position":60},{"hierarchy":{"lvl1":"Régression linéaire","lvl2":"Implémentation"},"content":"","type":"content","url":"/linear-regression#impl-mentation","position":61},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Solution analytique","lvl2":"Implémentation"},"type":"lvl3","url":"/linear-regression#solution-analytique-1","position":62},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Solution analytique","lvl2":"Implémentation"},"content":"import numpy as np\n\ndef linear_regression_closed(X, y, reg_lambda=0):\n    \"\"\"Régression linéaire par solution analytique.\n    \n    Args:\n        X: Matrice de design (N x d)\n        y: Vecteur cible (N,)\n        reg_lambda: Coefficient de régularisation (0 pour MCO)\n        \n    Returns:\n        Vecteur de poids (d,)\n    \"\"\"\n    d = X.shape[1]\n    # Ajouter régularisation à la diagonale\n    A = X.T @ X + reg_lambda * np.eye(d)\n    b = X.T @ y\n    w = np.linalg.solve(A, b)\n    return w","type":"content","url":"/linear-regression#solution-analytique-1","position":63},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Descente de gradient","lvl2":"Implémentation"},"type":"lvl3","url":"/linear-regression#descente-de-gradient","position":64},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Descente de gradient","lvl2":"Implémentation"},"content":"Pour les grands ensembles de données, la solution analytique peut être coûteuse (O(d^3) pour l’inversion). La descente de gradient offre une alternative:def linear_regression_gd(X, y, lr=0.01, n_iters=1000, reg_lambda=0):\n    \"\"\"Régression linéaire par descente de gradient.\n    \n    Args:\n        X: Matrice de design (N x d)\n        y: Vecteur cible (N,)\n        lr: Taux d'apprentissage\n        n_iters: Nombre d'itérations\n        reg_lambda: Coefficient de régularisation\n        \n    Returns:\n        Vecteur de poids (d,), historique des pertes\n    \"\"\"\n    N, d = X.shape\n    w = np.zeros(d)\n    losses = []\n    \n    for i in range(n_iters):\n        # Prédictions\n        y_pred = X @ w\n        \n        # Gradient de la MSE + régularisation\n        grad = (2/N) * X.T @ (y_pred - y) + 2 * reg_lambda * w\n        \n        # Mise à jour\n        w = w - lr * grad\n        \n        # Enregistrer la perte\n        loss = np.mean((y_pred - y)**2) + reg_lambda * np.sum(w**2)\n        losses.append(loss)\n        \n    return w, losses","type":"content","url":"/linear-regression#descente-de-gradient","position":65},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Utilisation avec scikit-learn","lvl2":"Implémentation"},"type":"lvl3","url":"/linear-regression#utilisation-avec-scikit-learn","position":66},{"hierarchy":{"lvl1":"Régression linéaire","lvl3":"Utilisation avec scikit-learn","lvl2":"Implémentation"},"content":"from sklearn.linear_model import LinearRegression, Ridge\n\n# Régression linéaire standard\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\n\n# Régression ridge\nridge = Ridge(alpha=1.0)  # alpha correspond à lambda\nridge.fit(X_train, y_train)\npredictions_ridge = ridge.predict(X_test)","type":"content","url":"/linear-regression#utilisation-avec-scikit-learn","position":67},{"hierarchy":{"lvl1":"Régression linéaire","lvl2":"Résumé"},"type":"lvl2","url":"/linear-regression#r-sum","position":68},{"hierarchy":{"lvl1":"Régression linéaire","lvl2":"Résumé"},"content":"Ce chapitre a présenté la régression linéaire sous plusieurs angles:\n\nLe modèle linéaire prédit \\hat{y} = w^\\top x + b\n\nLes moindres carrés ordinaires minimisent la somme des carrés des résidus\n\nLa solution analytique est \\hat{w} = (X^\\top X)^{-1} X^\\top y\n\nSous un modèle gaussien, les MCO coïncident avec le maximum de vraisemblance\n\nLa régression polynomiale étend le modèle à des relations non linéaires\n\nLa régression ridge régularise par pénalité L2, équivalent au MAP bayésien\n\nLe modèle hétéroscédastique permet une variance dépendant de l’entrée\n\nLe chapitre suivant étend ces idées à la classification avec les classifieurs linéaires.","type":"content","url":"/linear-regression#r-sum","position":69},{"hierarchy":{"lvl1":"Réseaux de neurones"},"type":"lvl1","url":"/neural-networks","position":0},{"hierarchy":{"lvl1":"Réseaux de neurones"},"content":"Objectifs d’apprentissage\n\nÀ la fin de ce chapitre, vous serez en mesure de:\n\nExpliquer le problème XOR et la motivation des réseaux multicouches\n\nDéfinir l’architecture d’un perceptron multicouche (MLP)\n\nDistinguer les principales fonctions d’activation et leurs propriétés\n\nDériver l’algorithme de rétropropagation\n\nExpliquer la différence entre JVP et VJP\n\nIdentifier les causes du problème du gradient qui disparaît\n\nImplémenter un MLP simple avec rétropropagation","type":"content","url":"/neural-networks","position":1},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl2":"Introduction"},"type":"lvl2","url":"/neural-networks#introduction","position":2},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl2":"Introduction"},"content":"Les réseaux de neurones étendent les modèles linéaires en composant plusieurs transformations non linéaires. Cette approche permet d’apprendre automatiquement des représentations hiérarchiques des données, sans spécifier manuellement les caractéristiques.\n\nCe chapitre introduit les perceptrons multicouches (MLP) et l’algorithme de rétropropagation qui permet de les entraîner.","type":"content","url":"/neural-networks#introduction","position":3},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl2":"Le problème du XOR"},"type":"lvl2","url":"/neural-networks#le-probl-me-du-xor","position":4},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl2":"Le problème du XOR"},"content":"","type":"content","url":"/neural-networks#le-probl-me-du-xor","position":5},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Limitations des modèles linéaires","lvl2":"Le problème du XOR"},"type":"lvl3","url":"/neural-networks#limitations-des-mod-les-lin-aires","position":6},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Limitations des modèles linéaires","lvl2":"Le problème du XOR"},"content":"Le perceptron et la régression logistique ne peuvent représenter que des frontières de décision linéaires. Certaines fonctions simples sont donc impossibles à apprendre.\n\nLa fonction XOR (ou exclusif) illustre cette limitation:\n\nx_1\n\nx_2\n\ny = x_1 \\oplus x_2\n\n0\n\n0\n\n0\n\n0\n\n1\n\n1\n\n1\n\n0\n\n1\n\n1\n\n1\n\n0\n\nAucun hyperplan ne peut séparer les deux classes. Les points (0,0) et (1,1) appartiennent à la classe 0, tandis que (0,1) et (1,0) appartiennent à la classe 1.\n\nLa publication du livre Perceptrons par Minsky et Papert en 1969, qui démontra formellement cette limitation, contribua à un ralentissement de la recherche en réseaux de neurones.","type":"content","url":"/neural-networks#limitations-des-mod-les-lin-aires","position":7},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Solution: composition de fonctions","lvl2":"Le problème du XOR"},"type":"lvl3","url":"/neural-networks#solution-composition-de-fonctions","position":8},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Solution: composition de fonctions","lvl2":"Le problème du XOR"},"content":"Minsky et Papert montrèrent également qu’en empilant deux couches de perceptrons, la fonction XOR peut être représentée parfaitement. L’idée est de transformer les entrées dans un espace où les classes deviennent linéairement séparables.","type":"content","url":"/neural-networks#solution-composition-de-fonctions","position":9},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl2":"Perceptrons multicouches"},"type":"lvl2","url":"/neural-networks#perceptrons-multicouches","position":10},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl2":"Perceptrons multicouches"},"content":"","type":"content","url":"/neural-networks#perceptrons-multicouches","position":11},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Architecture","lvl2":"Perceptrons multicouches"},"type":"lvl3","url":"/neural-networks#architecture","position":12},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Architecture","lvl2":"Perceptrons multicouches"},"content":"Un perceptron multicouche (MLP, de l’anglais multilayer perceptron) est une composition de transformations:f(x; \\theta) = f_L(f_{L-1}(\\cdots f_1(x) \\cdots))\n\noù chaque couche \\ell applique une transformation affine suivie d’une non-linéarité:z_\\ell = \\varphi_\\ell(W_\\ell z_{\\ell-1} + b_\\ell)\n\nLes composantes sont:\n\nz_{\\ell-1}: les unités cachées (ou activations) de la couche précédente\n\nW_\\ell: la matrice de poids\n\nb_\\ell: le vecteur de biais\n\na_\\ell = W_\\ell z_{\\ell-1} + b_\\ell: les pré-activations\n\n\\varphi_\\ell: la fonction d’activation\n\nz_\\ell: les activations de la couche \\ell\n\nLa première couche reçoit l’entrée z_0 = x. La dernière couche produit la sortie du réseau.","type":"content","url":"/neural-networks#architecture","position":13},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Exemple: classification binaire","lvl2":"Perceptrons multicouches"},"type":"lvl3","url":"/neural-networks#exemple-classification-binaire","position":14},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Exemple: classification binaire","lvl2":"Perceptrons multicouches"},"content":"Un MLP pour la classification binaire en 2D pourrait avoir l’architecture suivante:\\begin{aligned}\np(y | x; \\theta) &= \\text{Ber}(y | \\sigma(a_3)) \\\\\na_3 &= w_3^\\top z_2 + b_3 \\\\\nz_2 &= \\varphi(W_2 z_1 + b_2) \\\\\nz_1 &= \\varphi(W_1 x + b_1)\n\\end{aligned}\n\nCe réseau a deux couches cachées (z_1 et z_2) et une couche de sortie.","type":"content","url":"/neural-networks#exemple-classification-binaire","position":15},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Expressivité","lvl2":"Perceptrons multicouches"},"type":"lvl3","url":"/neural-networks#expressivit","position":16},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Expressivité","lvl2":"Perceptrons multicouches"},"content":"Un MLP avec une couche cachée suffisamment large peut approximer toute fonction continue sur un compact (théorème d’approximation universelle). Cependant, la taille de la couche peut croître exponentiellement avec la complexité de la fonction.\n\nLes réseaux profonds (avec plusieurs couches) peuvent représenter certaines fonctions de manière plus compacte que les réseaux larges mais peu profonds.","type":"content","url":"/neural-networks#expressivit","position":17},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl2":"Fonctions d’activation"},"type":"lvl2","url":"/neural-networks#fonctions-dactivation","position":18},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl2":"Fonctions d’activation"},"content":"","type":"content","url":"/neural-networks#fonctions-dactivation","position":19},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Nécessité de la non-linéarité","lvl2":"Fonctions d’activation"},"type":"lvl3","url":"/neural-networks#n-cessit-de-la-non-lin-arit","position":20},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Nécessité de la non-linéarité","lvl2":"Fonctions d’activation"},"content":"Sans fonction d’activation non linéaire, un MLP se réduit à un modèle linéaire. En effet, la composition de transformations linéaires reste linéaire:W_L(W_{L-1}(\\cdots W_1 x \\cdots)) = (W_L W_{L-1} \\cdots W_1) x = W' x\n\nLes fonctions d’activation non linéaires sont essentielles à l’expressivité du réseau.","type":"content","url":"/neural-networks#n-cessit-de-la-non-lin-arit","position":21},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Sigmoïde","lvl2":"Fonctions d’activation"},"type":"lvl3","url":"/neural-networks#sigmo-de","position":22},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Sigmoïde","lvl2":"Fonctions d’activation"},"content":"La fonction sigmoïde (ou logistique):\\sigma(a) = \\frac{1}{1 + e^{-a}}\n\ntransforme les valeurs réelles dans l’intervalle (0, 1). Sa dérivée est \\sigma'(a) = \\sigma(a)(1 - \\sigma(a)).\n\nProblème: la sigmoïde sature pour les grandes valeurs de |a|. Dans ces régions, la dérivée est proche de zéro, ce qui bloque la propagation du gradient.","type":"content","url":"/neural-networks#sigmo-de","position":23},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Tangente hyperbolique","lvl2":"Fonctions d’activation"},"type":"lvl3","url":"/neural-networks#tangente-hyperbolique","position":24},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Tangente hyperbolique","lvl2":"Fonctions d’activation"},"content":"La tangente hyperbolique:\\tanh(a) = \\frac{e^a - e^{-a}}{e^a + e^{-a}} = 2\\sigma(2a) - 1\n\nest similaire à la sigmoïde mais centrée autour de zéro, avec des sorties dans (-1, 1). Elle souffre du même problème de saturation.","type":"content","url":"/neural-networks#tangente-hyperbolique","position":25},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"ReLU","lvl2":"Fonctions d’activation"},"type":"lvl3","url":"/neural-networks#relu","position":26},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"ReLU","lvl2":"Fonctions d’activation"},"content":"L’unité linéaire rectifiée (ReLU, de l’anglais rectified linear unit):\\text{ReLU}(a) = \\max(0, a) = a \\cdot \\mathbb{1}(a > 0)\n\nest la fonction d’activation la plus utilisée en apprentissage profond moderne. Ses avantages:\n\nPas de saturation pour les valeurs positives\n\nCalcul efficace\n\nSparsité: de nombreuses unités sont à zéro\n\nSa dérivée est:\\text{ReLU}'(a) = \\begin{cases}\n0 & \\text{si } a < 0 \\\\\n1 & \\text{si } a > 0\n\\end{cases}\n\nProblème: les neurones peuvent “mourir” si leurs pré-activations sont toujours négatives. Le gradient est alors zéro et l’apprentissage s’arrête.","type":"content","url":"/neural-networks#relu","position":27},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Variantes de ReLU","lvl2":"Fonctions d’activation"},"type":"lvl3","url":"/neural-networks#variantes-de-relu","position":28},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Variantes de ReLU","lvl2":"Fonctions d’activation"},"content":"Leaky ReLU:\\text{LeakyReLU}(a) = \\max(\\alpha a, a)\n\noù \\alpha \\approx 0.01 permet un petit gradient pour les valeurs négatives.\n\nELU (Exponential Linear Unit):\\text{ELU}(a) = \\begin{cases}\na & \\text{si } a > 0 \\\\\n\\alpha(e^a - 1) & \\text{si } a \\leq 0\n\\end{cases}\n\nGELU (Gaussian Error Linear Unit):\\text{GELU}(a) = a \\cdot \\Phi(a)\n\noù \\Phi est la fonction de répartition de la loi normale standard. Utilisée dans les transformers modernes.","type":"content","url":"/neural-networks#variantes-de-relu","position":29},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl2":"Rétropropagation"},"type":"lvl2","url":"/neural-networks#r-tropropagation","position":30},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl2":"Rétropropagation"},"content":"","type":"content","url":"/neural-networks#r-tropropagation","position":31},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Motivation","lvl2":"Rétropropagation"},"type":"lvl3","url":"/neural-networks#motivation","position":32},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Motivation","lvl2":"Rétropropagation"},"content":"L’entraînement d’un réseau de neurones requiert le gradient de la perte par rapport à tous les paramètres. La rétropropagation (backpropagation) est l’algorithme standard pour ce calcul.\n\nLa rétropropagation exploite la structure de composition du réseau pour calculer efficacement les gradients par la règle de la chaîne.","type":"content","url":"/neural-networks#motivation","position":33},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Règle de la chaîne","lvl2":"Rétropropagation"},"type":"lvl3","url":"/neural-networks#r-gle-de-la-cha-ne","position":34},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Règle de la chaîne","lvl2":"Rétropropagation"},"content":"Pour une composition f = f_L \\circ \\cdots \\circ f_1, la jacobienne est:\\mathbf{J}_f(x) = \\mathbf{J}_{f_L}(x_L) \\cdot \\mathbf{J}_{f_{L-1}}(x_{L-1}) \\cdot \\ldots \\cdot \\mathbf{J}_{f_1}(x)\n\noù x_\\ell = f_\\ell(x_{\\ell-1}) sont les valeurs intermédiaires.","type":"content","url":"/neural-networks#r-gle-de-la-cha-ne","position":35},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"JVP et VJP","lvl2":"Rétropropagation"},"type":"lvl3","url":"/neural-networks#jvp-et-vjp","position":36},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"JVP et VJP","lvl2":"Rétropropagation"},"content":"Le calcul de la jacobienne complète est coûteux. En pratique, nous calculons des produits matrice-vecteur.\n\nJVP (Jacobian-Vector Product): multiplication à droite par un vecteur v:\\mathbf{J}_f(x) v = \\mathbf{J}_{f_L} \\cdot \\mathbf{J}_{f_{L-1}} \\cdot \\ldots \\cdot \\mathbf{J}_{f_1} \\cdot v\n\nLe calcul se fait de droite à gauche, propageant v à travers les couches.\n\nVJP (Vector-Jacobian Product): multiplication à gauche par un vecteur ligne u^\\top:u^\\top \\mathbf{J}_f(x) = u^\\top \\cdot \\mathbf{J}_{f_L} \\cdot \\mathbf{J}_{f_{L-1}} \\cdot \\ldots \\cdot \\mathbf{J}_{f_1}\n\nLe calcul se fait de gauche à droite.","type":"content","url":"/neural-networks#jvp-et-vjp","position":37},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Accumulation avant vs arrière","lvl2":"Rétropropagation"},"type":"lvl3","url":"/neural-networks#accumulation-avant-vs-arri-re","position":38},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Accumulation avant vs arrière","lvl2":"Rétropropagation"},"content":"Mode avant (JVP): efficace quand la sortie a plus de dimensions que l’entrée (m > n)\n\nMode arrière (VJP): efficace quand l’entrée a plus de dimensions que la sortie (n > m)\n\nPour une fonction de perte scalaire (m = 1), le mode arrière est optimal. C’est le mode utilisé par la rétropropagation.","type":"content","url":"/neural-networks#accumulation-avant-vs-arri-re","position":39},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Algorithme","lvl2":"Rétropropagation"},"type":"lvl3","url":"/neural-networks#algorithme","position":40},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Algorithme","lvl2":"Rétropropagation"},"content":"Rétropropagation\n\nEntrée: Entrée x, cible y, paramètres \\theta\n\nSortie: Perte \\mathcal{L}, gradients \\nabla_{\\theta_k} \\mathcal{L} pour k = 1, \\ldots, K\n\n// Passe avant\n\nx_1 := x\n\nPour k = 1, \\ldots, K:\n\nx_{k+1} = f_k(x_k; \\theta_k)\n\n// Passe arrière\n3. u_{K+1} := 1\n4. Pour k = K, \\ldots, 1:\n\ng_k := u_{k+1}^\\top \\frac{\\partial f_k(x_k; \\theta_k)}{\\partial \\theta_k} (gradient des paramètres)\n\nu_k^\\top := u_{k+1}^\\top \\frac{\\partial f_k(x_k; \\theta_k)}{\\partial x_k} (propagation de l’adjoint)\n\nRetourner \\mathcal{L} = x_{K+1}, \\{\\nabla_{\\theta_k} \\mathcal{L} = g_k^\\top\\}\n\nLe vecteur u_k est appelé adjoint ou delta. Il accumule la sensibilité de la perte aux activations de la couche k.","type":"content","url":"/neural-networks#algorithme","position":41},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Exemple: MLP avec une couche cachée","lvl2":"Rétropropagation"},"type":"lvl3","url":"/neural-networks#exemple-mlp-avec-une-couche-cach-e","position":42},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Exemple: MLP avec une couche cachée","lvl2":"Rétropropagation"},"content":"Considérons la perte MSE pour un MLP:\\mathcal{L} = \\frac{1}{2}\\|y - W_2 \\varphi(W_1 x)\\|^2\n\nPasse avant:\\begin{aligned}\na_1 &= W_1 x \\\\\nz_1 &= \\varphi(a_1) \\\\\na_2 &= W_2 z_1 \\\\\n\\mathcal{L} &= \\frac{1}{2}\\|a_2 - y\\|^2\n\\end{aligned}\n\nPasse arrière:\\begin{aligned}\n\\frac{\\partial \\mathcal{L}}{\\partial a_2} &= a_2 - y \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial W_2} &= \\frac{\\partial \\mathcal{L}}{\\partial a_2} z_1^\\top \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial z_1} &= W_2^\\top \\frac{\\partial \\mathcal{L}}{\\partial a_2} \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial a_1} &= \\frac{\\partial \\mathcal{L}}{\\partial z_1} \\odot \\varphi'(a_1) \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial W_1} &= \\frac{\\partial \\mathcal{L}}{\\partial a_1} x^\\top\n\\end{aligned}\n\noù \\odot désigne le produit élément par élément.","type":"content","url":"/neural-networks#exemple-mlp-avec-une-couche-cach-e","position":43},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl2":"Problème du gradient qui disparaît"},"type":"lvl2","url":"/neural-networks#probl-me-du-gradient-qui-dispara-t","position":44},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl2":"Problème du gradient qui disparaît"},"content":"","type":"content","url":"/neural-networks#probl-me-du-gradient-qui-dispara-t","position":45},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Cause","lvl2":"Problème du gradient qui disparaît"},"type":"lvl3","url":"/neural-networks#cause","position":46},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Cause","lvl2":"Problème du gradient qui disparaît"},"content":"Dans un réseau profond, le gradient doit traverser de nombreuses couches. À chaque couche, il est multiplié par la jacobienne locale:\\frac{\\partial \\mathcal{L}}{\\partial z_\\ell} = \\frac{\\partial \\mathcal{L}}{\\partial z_{\\ell+1}} \\cdot \\frac{\\partial z_{\\ell+1}}{\\partial z_\\ell}\n\nSi les jacobiennes ont un rayon spectral inférieur à 1, le gradient diminue exponentiellement avec la profondeur. C’est le gradient qui disparaît (vanishing gradient).\n\nInversement, si le rayon spectral est supérieur à 1, le gradient explose.","type":"content","url":"/neural-networks#cause","position":47},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Solutions","lvl2":"Problème du gradient qui disparaît"},"type":"lvl3","url":"/neural-networks#solutions","position":48},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Solutions","lvl2":"Problème du gradient qui disparaît"},"content":"Fonctions d’activation appropriées: ReLU et ses variantes évitent la saturation.\n\nNormalisation par lots (Batch Normalization): normalise les activations à chaque couche pour stabiliser l’entraînement.\n\nConnexions résiduelles: permettent au gradient de “sauter” des couches.\n\nInitialisation appropriée: les poids sont initialisés pour que les activations et gradients restent dans une plage raisonnable.\n\nÉcrêtage du gradient (Gradient Clipping): limite la norme du gradient:g' = \\min\\left(1, \\frac{c}{\\|g\\|}\\right) g","type":"content","url":"/neural-networks#solutions","position":49},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl2":"Implémentation"},"type":"lvl2","url":"/neural-networks#impl-mentation","position":50},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl2":"Implémentation"},"content":"import numpy as np\n\ndef relu(x):\n    return np.maximum(0, x)\n\ndef relu_grad(x):\n    return (x > 0).astype(float)\n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n\ndef mse_loss(y_pred, y_true):\n    return 0.5 * np.mean((y_pred - y_true)**2)\n\nclass MLP:\n    \"\"\"Perceptron multicouche simple avec une couche cachée.\"\"\"\n    \n    def __init__(self, input_dim, hidden_dim, output_dim):\n        # Initialisation Xavier/Glorot\n        self.W1 = np.random.randn(hidden_dim, input_dim) * np.sqrt(2.0 / input_dim)\n        self.b1 = np.zeros((hidden_dim, 1))\n        self.W2 = np.random.randn(output_dim, hidden_dim) * np.sqrt(2.0 / hidden_dim)\n        self.b2 = np.zeros((output_dim, 1))\n        \n    def forward(self, X):\n        \"\"\"Passe avant.\"\"\"\n        # X: (input_dim, batch_size)\n        self.a1 = self.W1 @ X + self.b1\n        self.z1 = relu(self.a1)\n        self.a2 = self.W2 @ self.z1 + self.b2\n        return self.a2\n    \n    def backward(self, X, y, y_pred):\n        \"\"\"Passe arrière (rétropropagation).\"\"\"\n        batch_size = X.shape[1]\n        \n        # Gradient de la perte MSE\n        dL_da2 = (y_pred - y) / batch_size\n        \n        # Gradients de W2 et b2\n        dL_dW2 = dL_da2 @ self.z1.T\n        dL_db2 = np.sum(dL_da2, axis=1, keepdims=True)\n        \n        # Propagation vers la couche cachée\n        dL_dz1 = self.W2.T @ dL_da2\n        dL_da1 = dL_dz1 * relu_grad(self.a1)\n        \n        # Gradients de W1 et b1\n        dL_dW1 = dL_da1 @ X.T\n        dL_db1 = np.sum(dL_da1, axis=1, keepdims=True)\n        \n        return {'W1': dL_dW1, 'b1': dL_db1, 'W2': dL_dW2, 'b2': dL_db2}\n    \n    def train_step(self, X, y, lr=0.01):\n        \"\"\"Une étape d'entraînement.\"\"\"\n        y_pred = self.forward(X)\n        loss = mse_loss(y_pred, y)\n        grads = self.backward(X, y, y_pred)\n        \n        # Mise à jour des paramètres\n        self.W1 -= lr * grads['W1']\n        self.b1 -= lr * grads['b1']\n        self.W2 -= lr * grads['W2']\n        self.b2 -= lr * grads['b2']\n        \n        return loss\n\n# Exemple: apprendre XOR\nX = np.array([[0, 0, 1, 1],\n              [0, 1, 0, 1]])  # 2x4\ny = np.array([[0, 1, 1, 0]])  # 1x4\n\nmlp = MLP(input_dim=2, hidden_dim=4, output_dim=1)\n\nfor epoch in range(1000):\n    loss = mlp.train_step(X, y, lr=0.5)\n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")","type":"content","url":"/neural-networks#impl-mentation","position":51},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl2":"Graphes de calcul"},"type":"lvl2","url":"/neural-networks#graphes-de-calcul","position":52},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl2":"Graphes de calcul"},"content":"","type":"content","url":"/neural-networks#graphes-de-calcul","position":53},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Définition","lvl2":"Graphes de calcul"},"type":"lvl3","url":"/neural-networks#d-finition","position":54},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Définition","lvl2":"Graphes de calcul"},"content":"Les MLP ont une structure de chaîne simple. Les réseaux modernes ont des architectures plus complexes représentées par des graphes de calcul: des graphes orientés acycliques (DAG) où chaque nœud est une opération différentiable.","type":"content","url":"/neural-networks#d-finition","position":55},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Dérivée totale","lvl2":"Graphes de calcul"},"type":"lvl3","url":"/neural-networks#d-riv-e-totale","position":56},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Dérivée totale","lvl2":"Graphes de calcul"},"content":"Dans un DAG, un nœud peut influencer la sortie par plusieurs chemins. La règle de la chaîne devient:\\frac{\\partial o}{\\partial x_j} = \\sum_{k \\in \\text{enfants}(j)} \\frac{\\partial o}{\\partial x_k} \\frac{\\partial x_k}{\\partial x_j}\n\nLa rétropropagation parcourt le graphe dans l’ordre topologique inverse, accumulant les contributions de tous les chemins.","type":"content","url":"/neural-networks#d-riv-e-totale","position":57},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Différentiation automatique","lvl2":"Graphes de calcul"},"type":"lvl3","url":"/neural-networks#diff-rentiation-automatique","position":58},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl3":"Différentiation automatique","lvl2":"Graphes de calcul"},"content":"Les bibliothèques modernes (PyTorch, JAX, TensorFlow) implémentent la différentiation automatique: le graphe de calcul est construit lors de l’exécution, et les gradients sont calculés automatiquement.","type":"content","url":"/neural-networks#diff-rentiation-automatique","position":59},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl2":"Résumé"},"type":"lvl2","url":"/neural-networks#r-sum","position":60},{"hierarchy":{"lvl1":"Réseaux de neurones","lvl2":"Résumé"},"content":"Ce chapitre a introduit les réseaux de neurones:\n\nLe problème XOR motive les architectures multicouches\n\nUn MLP compose des transformations affines et des non-linéarités\n\nLes fonctions d’activation (ReLU, sigmoïde, etc.) apportent la non-linéarité essentielle\n\nLa rétropropagation calcule efficacement les gradients par la règle de la chaîne\n\nLe mode VJP (arrière) est optimal pour les fonctions à sortie scalaire\n\nLe gradient qui disparaît est un défi pour les réseaux profonds\n\nLes graphes de calcul généralisent les architectures en chaîne\n\nLes chapitres suivants présentent des architectures spécialisées: les réseaux convolutifs pour les images et les réseaux récurrents pour les séquences.","type":"content","url":"/neural-networks#r-sum","position":61},{"hierarchy":{"lvl1":"Analyse en composantes principales"},"type":"lvl1","url":"/pca","position":0},{"hierarchy":{"lvl1":"Analyse en composantes principales"},"content":"Objectifs d’apprentissage\n\nÀ la fin de ce chapitre, vous serez en mesure de:\n\nFormuler l’ACP comme minimisation de l’erreur de reconstruction\n\nFormuler l’ACP comme maximisation de la variance\n\nDériver l’ACP à partir de la décomposition en valeurs propres\n\nComprendre le lien entre l’ACP et l’analyse factorielle\n\nAppliquer l’ACP pour la réduction de dimensionnalité\n\nChoisir le nombre de composantes à retenir","type":"content","url":"/pca","position":1},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl2":"Motivation"},"type":"lvl2","url":"/pca#motivation","position":2},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl2":"Motivation"},"content":"L’analyse en composantes principales (ACP, ou PCA en anglais) est une technique fondamentale de réduction de dimensionnalité. Elle transforme des données de haute dimension en une représentation de plus basse dimension tout en préservant autant d’information que possible.","type":"content","url":"/pca#motivation","position":3},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Pourquoi réduire la dimensionnalité?","lvl2":"Motivation"},"type":"lvl3","url":"/pca#pourquoi-r-duire-la-dimensionnalit","position":4},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Pourquoi réduire la dimensionnalité?","lvl2":"Motivation"},"content":"Visualisation: projeter des données en 2D ou 3D pour les explorer visuellement\n\nCompression: représenter les données avec moins de variables\n\nDébruitage: éliminer les composantes de faible variance (souvent du bruit)\n\nPrétraitement: améliorer les performances d’algorithmes en aval\n\nDécouverte de structure: identifier les directions principales de variation","type":"content","url":"/pca#pourquoi-r-duire-la-dimensionnalit","position":5},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"L’idée de l’autoencodeur linéaire","lvl2":"Motivation"},"type":"lvl3","url":"/pca#lid-e-de-lautoencodeur-lin-aire","position":6},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"L’idée de l’autoencodeur linéaire","lvl2":"Motivation"},"content":"L’ACP peut être vue comme un autoencodeur linéaire: nous cherchons à transformer les données \\boldsymbol{x} \\in \\mathbb{R}^D en un sous-espace de dimension réduite \\boldsymbol{z} \\in \\mathbb{R}^L (où L < D), de sorte que la représentation soit une “bonne approximation” des données d’origine.\n\nLe processus se fait en deux étapes:\n\nEncodage: \\boldsymbol{z} = \\mathbf{W}^\\top \\boldsymbol{x} (projection vers l’espace latent)\n\nDécodage: \\hat{\\boldsymbol{x}} = \\mathbf{W} \\boldsymbol{z} (reconstruction)\n\nL’objectif est de trouver \\mathbf{W} tel que \\hat{\\boldsymbol{x}} soit proche de \\boldsymbol{x}.","type":"content","url":"/pca#lid-e-de-lautoencodeur-lin-aire","position":7},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl2":"Formulation du problème"},"type":"lvl2","url":"/pca#formulation-du-probl-me","position":8},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl2":"Formulation du problème"},"content":"","type":"content","url":"/pca#formulation-du-probl-me","position":9},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Notation","lvl2":"Formulation du problème"},"type":"lvl3","url":"/pca#notation","position":10},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Notation","lvl2":"Formulation du problème"},"content":"Soit un ensemble de données centrées \\mathcal{D} = \\{\\boldsymbol{x}_n : n = 1, \\ldots, N\\} où \\boldsymbol{x}_n \\in \\mathbb{R}^D et:\\bar{\\boldsymbol{x}} = \\frac{1}{N} \\sum_{n=1}^N \\boldsymbol{x}_n = \\mathbf{0}\n\nCes données sont organisées dans une matrice \\mathbf{X} de taille N \\times D (chaque ligne est un exemple).","type":"content","url":"/pca#notation","position":11},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Représentation en termes de fonctions de base","lvl2":"Formulation du problème"},"type":"lvl3","url":"/pca#repr-sentation-en-termes-de-fonctions-de-base","position":12},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Représentation en termes de fonctions de base","lvl2":"Formulation du problème"},"content":"Nous supposons que chaque \\boldsymbol{x}_n peut être “expliqué” comme une combinaison pondérée de L fonctions de base \\boldsymbol{w}_1, \\ldots, \\boldsymbol{w}_L où chaque \\boldsymbol{w}_k \\in \\mathbb{R}^D:\\boldsymbol{x}_n \\approx \\sum_{k=1}^L z_{nk} \\boldsymbol{w}_k\n\nLe vecteur \\boldsymbol{z}_n \\in \\mathbb{R}^L est la représentation de basse dimension, appelée vecteur latent ou embedding.","type":"content","url":"/pca#repr-sentation-en-termes-de-fonctions-de-base","position":13},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Erreur de reconstruction","lvl2":"Formulation du problème"},"type":"lvl3","url":"/pca#erreur-de-reconstruction","position":14},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Erreur de reconstruction","lvl2":"Formulation du problème"},"content":"L’erreur de reconstruction s’écrit:\\mathcal{L}(\\mathbf{W}, \\mathbf{Z}) = \\frac{1}{N} \\|\\mathbf{X} - \\mathbf{Z}\\mathbf{W}^\\top\\|_F^2 = \\frac{1}{N} \\sum_{n=1}^N \\|\\boldsymbol{x}_n - \\mathbf{W}\\boldsymbol{z}_n\\|^2\n\noù:\n\n\\mathbf{Z} est une matrice N \\times L dont les lignes sont les \\boldsymbol{z}_n\n\n\\mathbf{W} est une matrice D \\times L dont les colonnes sont les \\boldsymbol{w}_k\n\n\\|\\cdot\\|_F est la norme de Frobenius\n\nNous voulons minimiser cette erreur sous la contrainte que \\mathbf{W} soit orthonormale: \\mathbf{W}^\\top \\mathbf{W} = \\mathbf{I}_L.","type":"content","url":"/pca#erreur-de-reconstruction","position":15},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl2":"Dérivation de l’ACP"},"type":"lvl2","url":"/pca#d-rivation-de-lacp","position":16},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl2":"Dérivation de l’ACP"},"content":"","type":"content","url":"/pca#d-rivation-de-lacp","position":17},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Cas d’une seule composante (L = 1)","lvl2":"Dérivation de l’ACP"},"type":"lvl3","url":"/pca#cas-dune-seule-composante-l-1","position":18},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Cas d’une seule composante (L = 1)","lvl2":"Dérivation de l’ACP"},"content":"Commençons par trouver la meilleure représentation en 1D. L’erreur de reconstruction est:\\mathcal{L}(\\boldsymbol{w}_1, \\tilde{\\boldsymbol{z}}_1) = \\frac{1}{N} \\sum_{n=1}^N \\|\\boldsymbol{x}_n - z_{n1} \\boldsymbol{w}_1\\|^2\n\nEn développant:\\mathcal{L} = \\frac{1}{N} \\sum_{n=1}^N \\left[\\boldsymbol{x}_n^\\top \\boldsymbol{x}_n - 2z_{n1} \\boldsymbol{w}_1^\\top \\boldsymbol{x}_n + z_{n1}^2 \\boldsymbol{w}_1^\\top \\boldsymbol{w}_1\\right]\n\nPuisque \\boldsymbol{w}_1^\\top \\boldsymbol{w}_1 = 1 (contrainte d’orthonormalité):\\mathcal{L} = \\frac{1}{N} \\sum_{n=1}^N \\left[\\boldsymbol{x}_n^\\top \\boldsymbol{x}_n - 2z_{n1} \\boldsymbol{w}_1^\\top \\boldsymbol{x}_n + z_{n1}^2\\right]","type":"content","url":"/pca#cas-dune-seule-composante-l-1","position":19},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Optimisation par rapport à z_{n1}","lvl2":"Dérivation de l’ACP"},"type":"lvl3","url":"/pca#optimisation-par-rapport-z-n1","position":20},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Optimisation par rapport à z_{n1}","lvl2":"Dérivation de l’ACP"},"content":"En dérivant par rapport à z_{n1} et en égalant à zéro:\\frac{\\partial \\mathcal{L}}{\\partial z_{n1}} = \\frac{1}{N}\\left[-2\\boldsymbol{w}_1^\\top \\boldsymbol{x}_n + 2z_{n1}\\right] = 0 \\quad \\Rightarrow \\quad z_{n1}^* = \\boldsymbol{w}_1^\\top \\boldsymbol{x}_n\n\nLe coefficient optimal est la projection orthogonale de \\boldsymbol{x}_n sur \\boldsymbol{w}_1.","type":"content","url":"/pca#optimisation-par-rapport-z-n1","position":21},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Optimisation par rapport à \\boldsymbol{w}_1","lvl2":"Dérivation de l’ACP"},"type":"lvl3","url":"/pca#optimisation-par-rapport-boldsymbol-w-1","position":22},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Optimisation par rapport à \\boldsymbol{w}_1","lvl2":"Dérivation de l’ACP"},"content":"En réinsérant z_{n1}^* dans l’erreur:\\mathcal{L}(\\boldsymbol{w}_1) = \\frac{1}{N} \\sum_{n=1}^N \\left[\\boldsymbol{x}_n^\\top \\boldsymbol{x}_n - z_{n1}^2\\right] = \\text{const} - \\frac{1}{N} \\sum_{n=1}^N z_{n1}^2\n\nOr:\\frac{1}{N} \\sum_{n=1}^N z_{n1}^2 = \\frac{1}{N} \\sum_{n=1}^N \\boldsymbol{w}_1^\\top \\boldsymbol{x}_n \\boldsymbol{x}_n^\\top \\boldsymbol{w}_1 = \\boldsymbol{w}_1^\\top \\hat{\\boldsymbol{\\Sigma}} \\boldsymbol{w}_1\n\noù \\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{N} \\sum_{n=1}^N \\boldsymbol{x}_n \\boldsymbol{x}_n^\\top est la matrice de covariance empirique.","type":"content","url":"/pca#optimisation-par-rapport-boldsymbol-w-1","position":23},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Problème contraint","lvl2":"Dérivation de l’ACP"},"type":"lvl3","url":"/pca#probl-me-contraint","position":24},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Problème contraint","lvl2":"Dérivation de l’ACP"},"content":"Nous voulons maximiser \\boldsymbol{w}_1^\\top \\hat{\\boldsymbol{\\Sigma}} \\boldsymbol{w}_1 sous la contrainte \\|\\boldsymbol{w}_1\\| = 1.\n\nEn utilisant les multiplicateurs de Lagrange:\\tilde{\\mathcal{L}}(\\boldsymbol{w}_1) = \\boldsymbol{w}_1^\\top \\hat{\\boldsymbol{\\Sigma}} \\boldsymbol{w}_1 - \\lambda_1(\\boldsymbol{w}_1^\\top \\boldsymbol{w}_1 - 1)\n\nEn dérivant et égalant à zéro:\\hat{\\boldsymbol{\\Sigma}} \\boldsymbol{w}_1 = \\lambda_1 \\boldsymbol{w}_1\n\nC’est l’équation aux valeurs propres! Le vecteur \\boldsymbol{w}_1 doit être un vecteur propre de \\hat{\\boldsymbol{\\Sigma}}.","type":"content","url":"/pca#probl-me-contraint","position":25},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Choix de la valeur propre","lvl2":"Dérivation de l’ACP"},"type":"lvl3","url":"/pca#choix-de-la-valeur-propre","position":26},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Choix de la valeur propre","lvl2":"Dérivation de l’ACP"},"content":"En multipliant à gauche par \\boldsymbol{w}_1^\\top:\\boldsymbol{w}_1^\\top \\hat{\\boldsymbol{\\Sigma}} \\boldsymbol{w}_1 = \\lambda_1\n\nPuisque nous voulons maximiser cette quantité, nous choisissons le vecteur propre correspondant à la plus grande valeur propre.","type":"content","url":"/pca#choix-de-la-valeur-propre","position":27},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Extension à L composantes","lvl2":"Dérivation de l’ACP"},"type":"lvl3","url":"/pca#extension-l-composantes","position":28},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Extension à L composantes","lvl2":"Dérivation de l’ACP"},"content":"Par un raisonnement similaire (en utilisant l’orthogonalité avec les composantes précédentes), on montre que:\n\nLa k-ième composante principale est le k-ième vecteur propre de \\hat{\\boldsymbol{\\Sigma}}\n\nOrdonnés par valeurs propres décroissantes: \\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_D\n\nLa solution optimale est:\\mathbf{W} = \\mathbf{U}_L\n\noù \\mathbf{U}_L contient les L premiers vecteurs propres de \\hat{\\boldsymbol{\\Sigma}}.","type":"content","url":"/pca#extension-l-composantes","position":29},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl2":"Interprétation: maximisation de la variance"},"type":"lvl2","url":"/pca#interpr-tation-maximisation-de-la-variance","position":30},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl2":"Interprétation: maximisation de la variance"},"content":"","type":"content","url":"/pca#interpr-tation-maximisation-de-la-variance","position":31},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Variance des données projetées","lvl2":"Interprétation: maximisation de la variance"},"type":"lvl3","url":"/pca#variance-des-donn-es-projet-es","position":32},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Variance des données projetées","lvl2":"Interprétation: maximisation de la variance"},"content":"Puisque les données sont centrées (\\mathbb{E}[\\boldsymbol{x}_n] = \\mathbf{0}), la moyenne des projections est:\\mathbb{E}[z_{n1}] = \\mathbb{E}[\\boldsymbol{w}_1^\\top \\boldsymbol{x}_n] = \\boldsymbol{w}_1^\\top \\mathbb{E}[\\boldsymbol{x}_n] = 0\n\nLa variance des données projetées est:\\mathbb{V}[\\tilde{\\boldsymbol{z}}_1] = \\mathbb{E}[z_{n1}^2] - (\\mathbb{E}[z_{n1}])^2 = \\frac{1}{N} \\sum_{n=1}^N z_{n1}^2 = \\boldsymbol{w}_1^\\top \\hat{\\boldsymbol{\\Sigma}} \\boldsymbol{w}_1 = \\lambda_1","type":"content","url":"/pca#variance-des-donn-es-projet-es","position":33},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Équivalence des formulations","lvl2":"Interprétation: maximisation de la variance"},"type":"lvl3","url":"/pca#id-quivalence-des-formulations","position":34},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Équivalence des formulations","lvl2":"Interprétation: maximisation de la variance"},"content":"Minimiser l’erreur de reconstruction est équivalent à maximiser la variance des données projetées:\\arg\\min_{\\boldsymbol{w}_1} \\mathcal{L}(\\boldsymbol{w}_1) = \\arg\\max_{\\boldsymbol{w}_1} \\mathbb{V}[\\tilde{\\boldsymbol{z}}_1]\n\nC’est pourquoi on dit que l’ACP trouve les directions de variance maximale.","type":"content","url":"/pca#id-quivalence-des-formulations","position":35},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Interprétation géométrique","lvl2":"Interprétation: maximisation de la variance"},"type":"lvl3","url":"/pca#interpr-tation-g-om-trique","position":36},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Interprétation géométrique","lvl2":"Interprétation: maximisation de la variance"},"content":"La première composante principale \\boldsymbol{w}_1 est la direction le long de laquelle les données varient le plus\n\nLa deuxième composante \\boldsymbol{w}_2 est la direction de variance maximale parmi celles orthogonales à \\boldsymbol{w}_1\n\nEt ainsi de suite...","type":"content","url":"/pca#interpr-tation-g-om-trique","position":37},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl2":"Algorithme et implémentation"},"type":"lvl2","url":"/pca#algorithme-et-impl-mentation","position":38},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl2":"Algorithme et implémentation"},"content":"","type":"content","url":"/pca#algorithme-et-impl-mentation","position":39},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Calcul via la décomposition en valeurs propres","lvl2":"Algorithme et implémentation"},"type":"lvl3","url":"/pca#calcul-via-la-d-composition-en-valeurs-propres","position":40},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Calcul via la décomposition en valeurs propres","lvl2":"Algorithme et implémentation"},"content":"Entrée: Données X (N × D), dimension cible L\n1. Centrer les données: X_c = X - moyenne(X)\n2. Calculer la covariance: Σ = (1/N) X_c^T X_c\n3. Calculer les valeurs/vecteurs propres de Σ\n4. Trier par valeurs propres décroissantes\n5. W = les L premiers vecteurs propres\n6. Z = X_c W  (projections)\nSortie: W (D × L), Z (N × L)","type":"content","url":"/pca#calcul-via-la-d-composition-en-valeurs-propres","position":41},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Calcul via la SVD","lvl2":"Algorithme et implémentation"},"type":"lvl3","url":"/pca#calcul-via-la-svd","position":42},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Calcul via la SVD","lvl2":"Algorithme et implémentation"},"content":"La décomposition en valeurs singulières (SVD) de \\mathbf{X}_c donne:\\mathbf{X}_c = \\mathbf{U} \\boldsymbol{\\Sigma} \\mathbf{V}^\\top\n\nLes colonnes de \\mathbf{V} sont les vecteurs propres de \\mathbf{X}_c^\\top \\mathbf{X}_c, donc les composantes principales.\n\nCette approche est numériquement plus stable et plus efficace pour les grandes matrices.","type":"content","url":"/pca#calcul-via-la-svd","position":43},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl2":"Variance expliquée"},"type":"lvl2","url":"/pca#variance-expliqu-e","position":44},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl2":"Variance expliquée"},"content":"","type":"content","url":"/pca#variance-expliqu-e","position":45},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Proportion de variance","lvl2":"Variance expliquée"},"type":"lvl3","url":"/pca#proportion-de-variance","position":46},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Proportion de variance","lvl2":"Variance expliquée"},"content":"La variance totale des données est:\\text{Var}_{\\text{total}} = \\text{tr}(\\hat{\\boldsymbol{\\Sigma}}) = \\sum_{k=1}^D \\lambda_k\n\nLa variance expliquée par les L premières composantes est:\\text{Var}_L = \\sum_{k=1}^L \\lambda_k\n\nLa proportion de variance expliquée est:\\frac{\\text{Var}_L}{\\text{Var}_{\\text{total}}} = \\frac{\\sum_{k=1}^L \\lambda_k}{\\sum_{k=1}^D \\lambda_k}","type":"content","url":"/pca#proportion-de-variance","position":47},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Choix de L","lvl2":"Variance expliquée"},"type":"lvl3","url":"/pca#choix-de-l","position":48},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Choix de L","lvl2":"Variance expliquée"},"content":"Plusieurs critères pour choisir le nombre de composantes:\n\nSeuil de variance: garder assez de composantes pour expliquer (par exemple) 95% de la variance\n\nRègle du coude: tracer les valeurs propres et chercher un “coude” (changement de pente)\n\nCritère de Kaiser: garder les composantes avec \\lambda_k > 1 (après standardisation)\n\nValidation croisée: évaluer l’erreur de reconstruction sur des données de test","type":"content","url":"/pca#choix-de-l","position":49},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl2":"Analyse en composantes principales probabiliste (PPCA)"},"type":"lvl2","url":"/pca#analyse-en-composantes-principales-probabiliste-ppca","position":50},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl2":"Analyse en composantes principales probabiliste (PPCA)"},"content":"","type":"content","url":"/pca#analyse-en-composantes-principales-probabiliste-ppca","position":51},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Modèle génératif","lvl2":"Analyse en composantes principales probabiliste (PPCA)"},"type":"lvl3","url":"/pca#mod-le-g-n-ratif","position":52},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Modèle génératif","lvl2":"Analyse en composantes principales probabiliste (PPCA)"},"content":"L’ACP peut être formulée comme un modèle probabiliste. L’analyse factorielle suppose:\\begin{aligned}\np(\\boldsymbol{z}) &= \\mathcal{N}(\\boldsymbol{z} \\mid \\mathbf{0}, \\mathbf{I}) \\\\\np(\\boldsymbol{x} \\mid \\boldsymbol{z}, \\boldsymbol{\\theta}) &= \\mathcal{N}(\\boldsymbol{x} \\mid \\mathbf{W}\\boldsymbol{z} + \\boldsymbol{\\mu}, \\boldsymbol{\\Psi})\n\\end{aligned}\n\noù \\mathbf{W} est la matrice de chargement des facteurs et \\boldsymbol{\\Psi} est la covariance du bruit.","type":"content","url":"/pca#mod-le-g-n-ratif","position":53},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"PPCA comme cas particulier","lvl2":"Analyse en composantes principales probabiliste (PPCA)"},"type":"lvl3","url":"/pca#ppca-comme-cas-particulier","position":54},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"PPCA comme cas particulier","lvl2":"Analyse en composantes principales probabiliste (PPCA)"},"content":"L’ACP probabiliste (PPCA) est un cas particulier où:\n\n\\mathbf{W} a des colonnes orthonormales\n\n\\boldsymbol{\\Psi} = \\sigma^2 \\mathbf{I} (bruit isotrope)\n\n\\boldsymbol{\\mu} = \\mathbf{0}","type":"content","url":"/pca#ppca-comme-cas-particulier","position":55},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Lien avec l’ACP classique","lvl2":"Analyse en composantes principales probabiliste (PPCA)"},"type":"lvl3","url":"/pca#lien-avec-lacp-classique","position":56},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Lien avec l’ACP classique","lvl2":"Analyse en composantes principales probabiliste (PPCA)"},"content":"Dans la limite sans bruit (\\sigma^2 \\to 0), la solution MLE de PPCA converge vers la solution de l’ACP classique:\\mathbf{W}_{\\text{MLE}} = \\mathbf{U}_L \\mathbf{L}_L^{1/2}\n\noù \\mathbf{U}_L contient les L premiers vecteurs propres et \\mathbf{L}_L les valeurs propres correspondantes.","type":"content","url":"/pca#lien-avec-lacp-classique","position":57},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Avantages de la formulation probabiliste","lvl2":"Analyse en composantes principales probabiliste (PPCA)"},"type":"lvl3","url":"/pca#avantages-de-la-formulation-probabiliste","position":58},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl3":"Avantages de la formulation probabiliste","lvl2":"Analyse en composantes principales probabiliste (PPCA)"},"content":"Estimation du bruit \\sigma^2\n\nGestion des données manquantes\n\nCritère de sélection de modèle (vraisemblance)\n\nBase pour des extensions (mélange de PPCA, etc.)","type":"content","url":"/pca#avantages-de-la-formulation-probabiliste","position":59},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl2":"Résumé"},"type":"lvl2","url":"/pca#r-sum","position":60},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl2":"Résumé"},"content":"L’analyse en composantes principales est une technique fondamentale qui:\n\nRéduit la dimensionnalité en projetant sur un sous-espace de dimension L < D\n\nMinimise l’erreur de reconstruction (équivalent à maximiser la variance)\n\nSe calcule via la décomposition en valeurs propres de la matrice de covariance\n\nLes valeurs propres indiquent la variance expliquée par chaque composante\n\nAdmet une interprétation probabiliste (PPCA)\n\nL’ACP est un outil essentiel pour la visualisation, la compression et le prétraitement des données.","type":"content","url":"/pca#r-sum","position":61},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl2":"Exercices"},"type":"lvl2","url":"/pca#exercices","position":62},{"hierarchy":{"lvl1":"Analyse en composantes principales","lvl2":"Exercices"},"content":"Exercice 1: Calcul manuel\n\nConsidérez les données centrées suivantes en 2D:\n\\boldsymbol{x}_1 = (2, 1), \\boldsymbol{x}_2 = (-2, -1), \\boldsymbol{x}_3 = (1, 2), \\boldsymbol{x}_4 = (-1, -2)\n\nCalculez la matrice de covariance empirique \\hat{\\boldsymbol{\\Sigma}}.\n\nTrouvez les valeurs propres et vecteurs propres.\n\nQuelle proportion de variance est expliquée par la première composante?\n\nExercice 2: Reconstruction\n\nSoit \\mathbf{W} = \\begin{pmatrix} 1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{pmatrix} la première composante principale.\n\nPour le point \\boldsymbol{x} = (3, 1):\n\nCalculez le coefficient latent z = \\mathbf{W}^\\top \\boldsymbol{x}.\n\nCalculez la reconstruction \\hat{\\boldsymbol{x}} = \\mathbf{W} z.\n\nCalculez l’erreur de reconstruction \\|\\boldsymbol{x} - \\hat{\\boldsymbol{x}}\\|^2.\n\nExercice 3: Variance expliquée\n\nLes valeurs propres d’une matrice de covariance sont \\lambda_1 = 5, \\lambda_2 = 3, \\lambda_3 = 1.5, \\lambda_4 = 0.5.\n\nQuelle est la variance totale?\n\nQuelle proportion de variance est expliquée par les 2 premières composantes?\n\nCombien de composantes faut-il pour expliquer au moins 90% de la variance?\n\nExercice 4: Centrage\n\nExpliquez pourquoi il est important de centrer les données avant d’appliquer l’ACP.\n\nQue se passe-t-il si on n’effectue pas le centrage?","type":"content","url":"/pca#exercices","position":63},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents"},"type":"lvl1","url":"/rnn","position":0},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents"},"content":"Objectifs d’apprentissage\n\nÀ la fin de ce chapitre, vous serez en mesure de:\n\nExpliquer le fonctionnement des réseaux récurrents (RNN)\n\nFormuler le modèle génératif sous-jacent aux RNN\n\nComprendre la rétropropagation dans le temps (BPTT)\n\nDécrire le problème de la disparition et de l’explosion du gradient\n\nExpliquer les architectures GRU et LSTM\n\nComprendre les mécanismes d’attention","type":"content","url":"/rnn","position":1},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl2":"Motivation: traiter des séquences"},"type":"lvl2","url":"/rnn#motivation-traiter-des-s-quences","position":2},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl2":"Motivation: traiter des séquences"},"content":"Les réseaux de neurones que nous avons vus jusqu’à présent — MLP et CNN — traitent des entrées de taille fixe. Cependant, de nombreuses applications impliquent des séquences de longueur variable:\n\nTexte et langage naturel\n\nSéries temporelles financières ou météorologiques\n\nSignaux audio et parole\n\nVidéos (séquences d’images)\n\nLes réseaux de neurones récurrents (RNN, pour Recurrent Neural Networks) sont conçus spécifiquement pour traiter de telles données séquentielles. Leur caractéristique principale est de maintenir un état caché qui résume l’information des pas de temps précédents.","type":"content","url":"/rnn#motivation-traiter-des-s-quences","position":3},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl2":"Formulation probabiliste"},"type":"lvl2","url":"/rnn#formulation-probabiliste","position":4},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl2":"Formulation probabiliste"},"content":"","type":"content","url":"/rnn#formulation-probabiliste","position":5},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Le modèle génératif","lvl2":"Formulation probabiliste"},"type":"lvl3","url":"/rnn#le-mod-le-g-n-ratif","position":6},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Le modèle génératif","lvl2":"Formulation probabiliste"},"content":"Les RNN peuvent être vus comme un modèle génératif conditionnel. Étant donné une entrée optionnelle \\boldsymbol{x}, nous voulons modéliser la distribution d’une séquence de sortie \\boldsymbol{y}_{1:T} = (\\boldsymbol{y}_1, \\boldsymbol{y}_2, \\ldots, \\boldsymbol{y}_T):p(\\boldsymbol{y}_{1:T} \\mid \\boldsymbol{x}) = \\sum_{\\boldsymbol{h}_{1:T}} p(\\boldsymbol{y}_{1:T}, \\boldsymbol{h}_{1:T} \\mid \\boldsymbol{x}) = \\sum_{\\boldsymbol{h}_{1:T}} \\prod_{t=1}^T p(\\boldsymbol{y}_t \\mid \\boldsymbol{h}_t) \\, p(\\boldsymbol{h}_t \\mid \\boldsymbol{h}_{t-1}, \\boldsymbol{y}_{t-1}, \\boldsymbol{x})\n\noù \\boldsymbol{h}_t est l’état caché au temps t. Cette factorisation révèle la structure du RNN:\n\nL’état caché \\boldsymbol{h}_t dépend de l’état précédent \\boldsymbol{h}_{t-1}, de la sortie précédente \\boldsymbol{y}_{t-1} et de l’entrée \\boldsymbol{x}\n\nLa sortie \\boldsymbol{y}_t dépend uniquement de l’état caché \\boldsymbol{h}_t","type":"content","url":"/rnn#le-mod-le-g-n-ratif","position":7},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Distribution de sortie","lvl2":"Formulation probabiliste"},"type":"lvl3","url":"/rnn#distribution-de-sortie","position":8},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Distribution de sortie","lvl2":"Formulation probabiliste"},"content":"La distribution de probabilité des sorties dépend du type de données:\n\nPour la classification (par exemple, prédire le prochain caractère):p(\\boldsymbol{y}_t \\mid \\boldsymbol{h}_t) = \\text{Cat}(\\boldsymbol{y}_t \\mid \\text{softmax}(\\mathbf{W}_{hy} \\boldsymbol{h}_t + \\boldsymbol{b}_y))\n\noù \\mathbf{W}_{hy} sont les poids de la couche cachée vers la sortie et \\boldsymbol{b}_y est le biais.\n\nPour la régression (par exemple, prédire une valeur continue):p(\\boldsymbol{y}_t \\mid \\boldsymbol{h}_t) = \\mathcal{N}(\\boldsymbol{y}_t \\mid \\mathbf{W}_{hy} \\boldsymbol{h}_t + \\boldsymbol{b}_y, \\sigma^2 \\mathbf{I})","type":"content","url":"/rnn#distribution-de-sortie","position":9},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Fonction de transition","lvl2":"Formulation probabiliste"},"type":"lvl3","url":"/rnn#fonction-de-transition","position":10},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Fonction de transition","lvl2":"Formulation probabiliste"},"content":"L’état caché est généralement calculé de manière déterministe:p(\\boldsymbol{h}_t \\mid \\boldsymbol{h}_{t-1}, \\boldsymbol{y}_{t-1}, \\boldsymbol{x}) = \\mathbb{I}(\\boldsymbol{h}_t = f(\\boldsymbol{h}_{t-1}, \\boldsymbol{y}_{t-1}, \\boldsymbol{x}))\n\nLa fonction de mise à jour f est typiquement:\\boldsymbol{h}_t = \\varphi\\left(\\mathbf{W}_{xh} [\\boldsymbol{x}; \\boldsymbol{y}_{t-1}] + \\mathbf{W}_{hh} \\boldsymbol{h}_{t-1} + \\boldsymbol{b}_h\\right)\n\noù:\n\n\\mathbf{W}_{hh} sont les poids cachés-cachés (récurrence)\n\n\\mathbf{W}_{xh} sont les poids entrée-cachés\n\n\\boldsymbol{b}_h est le biais\n\n\\varphi est une fonction d’activation (tanh, ReLU, etc.)\n\nLa notation [\\boldsymbol{x}; \\boldsymbol{y}_{t-1}] désigne la concaténation des vecteurs.","type":"content","url":"/rnn#fonction-de-transition","position":11},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl2":"Types d’architectures RNN"},"type":"lvl2","url":"/rnn#types-darchitectures-rnn","position":12},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl2":"Types d’architectures RNN"},"content":"","type":"content","url":"/rnn#types-darchitectures-rnn","position":13},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Vec2Seq: génération de séquences","lvl2":"Types d’architectures RNN"},"type":"lvl3","url":"/rnn#vec2seq-g-n-ration-de-s-quences","position":14},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Vec2Seq: génération de séquences","lvl2":"Types d’architectures RNN"},"content":"Dans le cas vec2seq (vector to sequence), nous générons une séquence de sortie de longueur variable à partir d’un vecteur d’entrée fixe \\boldsymbol{x}.\n\nLes étapes de génération sont:\n\nInitialiser l’état caché à partir de \\boldsymbol{x}: \\boldsymbol{h}_1 = f(\\boldsymbol{x})\n\nÉchantillonner: \\tilde{\\boldsymbol{y}}_t \\sim p(\\boldsymbol{y}_t \\mid \\boldsymbol{h}_t)\n\nMettre à jour l’état: \\boldsymbol{h}_{t+1} = f(\\boldsymbol{h}_t, \\tilde{\\boldsymbol{y}}_t, \\boldsymbol{x})\n\nRépéter jusqu’à générer un token de fin ou atteindre une longueur maximale\n\nExemple: génération de texte caractère par caractère. Voici un exemple de sortie d’un RNN entraîné sur le livre La Machine à explorer le temps de H.G. Wells:\n\n“the githa some thong the time traveller held in his hand was a glittering metallic framework scarcely larger than a small clock...”\n\nCe texte, bien qu’imparfait, montre que le modèle a appris la structure du langage anglais sans supervision explicite.","type":"content","url":"/rnn#vec2seq-g-n-ration-de-s-quences","position":15},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Seq2Vec: classification de séquences","lvl2":"Types d’architectures RNN"},"type":"lvl3","url":"/rnn#seq2vec-classification-de-s-quences","position":16},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Seq2Vec: classification de séquences","lvl2":"Types d’architectures RNN"},"content":"Dans le cas seq2vec (sequence to vector), nous produisons un vecteur de sortie fixe à partir d’une séquence d’entrée de longueur variable.\n\nPour classifier une séquence, on peut:\n\nUtiliser l’état caché final \\boldsymbol{h}_T\n\nMoyenner tous les états cachés\n\nUtiliser un mécanisme d’attention (voir plus loin)","type":"content","url":"/rnn#seq2vec-classification-de-s-quences","position":17},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Seq2Seq: transformation de séquences","lvl2":"Types d’architectures RNN"},"type":"lvl3","url":"/rnn#seq2seq-transformation-de-s-quences","position":18},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Seq2Seq: transformation de séquences","lvl2":"Types d’architectures RNN"},"content":"L’architecture seq2seq (sequence to sequence) combine un encodeur et un décodeur:\n\nEncodeur: lit la séquence d’entrée et produit un vecteur de contexte\n\nDécodeur: génère la séquence de sortie conditionnée par ce contexte\n\nCette architecture est utilisée pour:\n\nLa traduction automatique\n\nLe résumé de texte\n\nLa génération de légendes d’images (avec un CNN comme encodeur)","type":"content","url":"/rnn#seq2seq-transformation-de-s-quences","position":19},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"RNN bidirectionnels","lvl2":"Types d’architectures RNN"},"type":"lvl3","url":"/rnn#rnn-bidirectionnels","position":20},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"RNN bidirectionnels","lvl2":"Types d’architectures RNN"},"content":"Dans un RNN standard, l’état caché \\boldsymbol{h}_t ne dépend que du contexte passé \\boldsymbol{x}_{1:t}. Pour certaines tâches (comme la reconnaissance d’entités nommées), le contexte futur est également informatif.\n\nUn RNN bidirectionnel combine deux RNN:\n\nUn qui traite la séquence de gauche à droite (état avant \\overrightarrow{\\boldsymbol{h}}_t)\n\nUn qui traite de droite à gauche (état arrière \\overleftarrow{\\boldsymbol{h}}_t)\n\nL’état combiné est \\boldsymbol{h}_t = [\\overrightarrow{\\boldsymbol{h}}_t; \\overleftarrow{\\boldsymbol{h}}_t].","type":"content","url":"/rnn#rnn-bidirectionnels","position":21},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl2":"Rétropropagation dans le temps (BPTT)"},"type":"lvl2","url":"/rnn#r-tropropagation-dans-le-temps-bptt","position":22},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl2":"Rétropropagation dans le temps (BPTT)"},"content":"","type":"content","url":"/rnn#r-tropropagation-dans-le-temps-bptt","position":23},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Calcul du gradient","lvl2":"Rétropropagation dans le temps (BPTT)"},"type":"lvl3","url":"/rnn#calcul-du-gradient","position":24},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Calcul du gradient","lvl2":"Rétropropagation dans le temps (BPTT)"},"content":"Pour entraîner un RNN, nous cherchons les paramètres qui maximisent la vraisemblance:\\boldsymbol{\\theta}^* = \\operatorname{argmax}_{\\boldsymbol{\\theta}} \\, p(\\boldsymbol{y}_{1:T} \\mid \\boldsymbol{x}_{1:T}, \\boldsymbol{\\theta})\n\nConsidérons le modèle simplifié:\\begin{aligned}\n\\boldsymbol{h}_t &= \\mathbf{W}_{hx} \\boldsymbol{x}_t + \\mathbf{W}_{hh} \\boldsymbol{h}_{t-1} \\\\\n\\boldsymbol{o}_t &= \\mathbf{W}_{ho} \\boldsymbol{h}_t\n\\end{aligned}\n\nLa fonction de perte moyenne est:L = \\frac{1}{T} \\sum_{t=1}^T \\ell(y_t, o_t)\n\nPour calculer le gradient, nous déroulons le graphe de calcul dans le temps et appliquons la rétropropagation. Cette technique s’appelle BPTT (Backpropagation Through Time).","type":"content","url":"/rnn#calcul-du-gradient","position":25},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Dérivation du gradient","lvl2":"Rétropropagation dans le temps (BPTT)"},"type":"lvl3","url":"/rnn#d-rivation-du-gradient","position":26},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Dérivation du gradient","lvl2":"Rétropropagation dans le temps (BPTT)"},"content":"De manière générale, avec \\boldsymbol{h}_t = f(\\boldsymbol{x}_t, \\boldsymbol{h}_{t-1}, \\boldsymbol{w}_h) et \\boldsymbol{o}_t = g(\\boldsymbol{h}_t, \\boldsymbol{w}_o), la règle de la chaîne donne:\\frac{\\partial L}{\\partial \\boldsymbol{w}_h} = \\frac{1}{T} \\sum_{t=1}^T \\frac{\\partial \\ell(y_t, \\boldsymbol{o}_t)}{\\partial \\boldsymbol{o}_t} \\frac{\\partial g(\\boldsymbol{h}_t, \\boldsymbol{w}_o)}{\\partial \\boldsymbol{h}_t} \\frac{\\partial \\boldsymbol{h}_t}{\\partial \\boldsymbol{w}_h}\n\nLe terme \\frac{\\partial \\boldsymbol{h}_t}{\\partial \\boldsymbol{w}_h} nécessite une récursion:\\frac{\\partial \\boldsymbol{h}_t}{\\partial \\boldsymbol{w}_h} = \\frac{\\partial f(\\boldsymbol{x}_t, \\boldsymbol{h}_{t-1}, \\boldsymbol{w}_h)}{\\partial \\boldsymbol{w}_h} + \\frac{\\partial f(\\boldsymbol{x}_t, \\boldsymbol{h}_{t-1}, \\boldsymbol{w}_h)}{\\partial \\boldsymbol{h}_{t-1}} \\frac{\\partial \\boldsymbol{h}_{t-1}}{\\partial \\boldsymbol{w}_h}\n\nEn déroulant cette récursion:\\frac{\\partial \\boldsymbol{h}_t}{\\partial \\boldsymbol{w}_h} = \\frac{\\partial f_t}{\\partial \\boldsymbol{w}_h} + \\sum_{i=1}^{t-1} \\left( \\prod_{j=i+1}^t \\frac{\\partial f_j}{\\partial \\boldsymbol{h}_{j-1}} \\right) \\frac{\\partial f_i}{\\partial \\boldsymbol{w}_h}","type":"content","url":"/rnn#d-rivation-du-gradient","position":27},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"BPTT tronquée","lvl2":"Rétropropagation dans le temps (BPTT)"},"type":"lvl3","url":"/rnn#bptt-tronqu-e","position":28},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"BPTT tronquée","lvl2":"Rétropropagation dans le temps (BPTT)"},"content":"Le produit imbriqué dans la somme rend le calcul O(T^2), ce qui est prohibitif pour les longues séquences. Une solution pratique est la BPTT tronquée: on limite la rétropropagation aux K pas de temps les plus récents.\n\nCette approximation:\n\nRéduit la complexité à O(TK)\n\nIntroduit un biais dans l’estimation du gradient\n\nEst généralement acceptable si K est suffisamment grand","type":"content","url":"/rnn#bptt-tronqu-e","position":29},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl2":"Le problème du gradient"},"type":"lvl2","url":"/rnn#le-probl-me-du-gradient","position":30},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl2":"Le problème du gradient"},"content":"","type":"content","url":"/rnn#le-probl-me-du-gradient","position":31},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Disparition et explosion du gradient","lvl2":"Le problème du gradient"},"type":"lvl3","url":"/rnn#disparition-et-explosion-du-gradient","position":32},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Disparition et explosion du gradient","lvl2":"Le problème du gradient"},"content":"Le produit de Jacobiennes \\prod_{j=i+1}^t \\frac{\\partial f_j}{\\partial \\boldsymbol{h}_{j-1}} pose un problème fondamental. À chaque pas de temps, nous multiplions approximativement par la matrice \\mathbf{W}_{hh}.\n\nSi les valeurs propres de \\mathbf{W}_{hh} ont:\n\nUn module < 1: le produit tend vers zéro → disparition du gradient\n\nUn module > 1: le produit explose → explosion du gradient\n\nCes problèmes sont analogues à ceux rencontrés dans les réseaux profonds, mais amplifiés car la même matrice est appliquée à chaque pas de temps.","type":"content","url":"/rnn#disparition-et-explosion-du-gradient","position":33},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Conséquences pratiques","lvl2":"Le problème du gradient"},"type":"lvl3","url":"/rnn#cons-quences-pratiques","position":34},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Conséquences pratiques","lvl2":"Le problème du gradient"},"content":"Disparition: le réseau ne peut pas apprendre les dépendances à long terme\n\nExplosion: l’entraînement devient instable","type":"content","url":"/rnn#cons-quences-pratiques","position":35},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Solutions","lvl2":"Le problème du gradient"},"type":"lvl3","url":"/rnn#solutions","position":36},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Solutions","lvl2":"Le problème du gradient"},"content":"Rognage de gradient (gradient clipping): si \\|\\nabla\\| > \\theta, on normalise: \\nabla \\leftarrow \\frac{\\theta}{\\|\\nabla\\|} \\nabla\n\nContrôle du rayon spectral: initialiser \\mathbf{W}_{hh} de sorte que son rayon spectral soit proche de 1\n\nArchitectures avec portes: GRU et LSTM (voir ci-dessous)","type":"content","url":"/rnn#solutions","position":37},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl2":"Unités récurrentes avec portes (GRU)"},"type":"lvl2","url":"/rnn#unit-s-r-currentes-avec-portes-gru","position":38},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl2":"Unités récurrentes avec portes (GRU)"},"content":"Les GRU (Gated Recurrent Units) résolvent le problème du gradient en introduisant des portes qui contrôlent le flux d’information.","type":"content","url":"/rnn#unit-s-r-currentes-avec-portes-gru","position":39},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Intuition","lvl2":"Unités récurrentes avec portes (GRU)"},"type":"lvl3","url":"/rnn#intuition","position":40},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Intuition","lvl2":"Unités récurrentes avec portes (GRU)"},"content":"L’idée clé est d’apprendre quand:\n\nMettre à jour l’état caché avec de nouvelles informations\n\nRéinitialiser l’état caché et oublier les informations obsolètes","type":"content","url":"/rnn#intuition","position":41},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Architecture","lvl2":"Unités récurrentes avec portes (GRU)"},"type":"lvl3","url":"/rnn#architecture","position":42},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Architecture","lvl2":"Unités récurrentes avec portes (GRU)"},"content":"Les GRU utilisent deux portes:\n\nPorte de réinitialisation \\mathbf{R}_t:\\mathbf{R}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xr} + \\mathbf{H}_{t-1} \\mathbf{W}_{hr} + \\boldsymbol{b}_r)\n\nPorte de mise à jour \\mathbf{Z}_t:\\mathbf{Z}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xz} + \\mathbf{H}_{t-1} \\mathbf{W}_{hz} + \\boldsymbol{b}_z)\n\nCes portes prennent des valeurs entre 0 et 1 grâce à la fonction sigmoïde \\sigma.\n\nÉtat candidat (utilise la porte de réinitialisation):\\tilde{\\mathbf{H}}_t = \\tanh(\\mathbf{X}_t \\mathbf{W}_{xh} + (\\mathbf{R}_t \\odot \\mathbf{H}_{t-1}) \\mathbf{W}_{hh} + \\boldsymbol{b}_h)\n\nLa porte de réinitialisation \\mathbf{R}_t contrôle combien de l’état précédent \\mathbf{H}_{t-1} influence le candidat.\n\nMise à jour de l’état (utilise la porte de mise à jour):\\mathbf{H}_t = \\mathbf{Z}_t \\odot \\mathbf{H}_{t-1} + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t\n\nQuand \\mathbf{Z}_t \\approx 1, l’état précédent est conservé; quand \\mathbf{Z}_t \\approx 0, le nouvel état candidat est adopté. Cette interpolation permet de créer des chemins de gradient qui ne passent pas par les multiplications matricielles.","type":"content","url":"/rnn#architecture","position":43},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl2":"Mémoire à court et long terme (LSTM)"},"type":"lvl2","url":"/rnn#m-moire-court-et-long-terme-lstm","position":44},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl2":"Mémoire à court et long terme (LSTM)"},"content":"Les LSTM (Long Short-Term Memory) sont une architecture plus élaborée qui augmente l’état caché avec une cellule de mémoire dédiée.","type":"content","url":"/rnn#m-moire-court-et-long-terme-lstm","position":45},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Architecture à trois portes","lvl2":"Mémoire à court et long terme (LSTM)"},"type":"lvl3","url":"/rnn#architecture-trois-portes","position":46},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Architecture à trois portes","lvl2":"Mémoire à court et long terme (LSTM)"},"content":"Les LSTM utilisent trois portes:\n\nPorte de sortie \\mathbf{O}_t — détermine ce qui est lu de la cellule:\\mathbf{O}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xo} + \\mathbf{H}_{t-1} \\mathbf{W}_{ho} + \\boldsymbol{b}_o)\n\nPorte d’entrée \\mathbf{I}_t — détermine ce qui est écrit dans la cellule:\\mathbf{I}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xi} + \\mathbf{H}_{t-1} \\mathbf{W}_{hi} + \\boldsymbol{b}_i)\n\nPorte d’oubli \\mathbf{F}_t — détermine ce qui est effacé de la cellule:\\mathbf{F}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{xf} + \\mathbf{H}_{t-1} \\mathbf{W}_{hf} + \\boldsymbol{b}_f)","type":"content","url":"/rnn#architecture-trois-portes","position":47},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Mise à jour de la cellule","lvl2":"Mémoire à court et long terme (LSTM)"},"type":"lvl3","url":"/rnn#mise-jour-de-la-cellule","position":48},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Mise à jour de la cellule","lvl2":"Mémoire à court et long terme (LSTM)"},"content":"Le contenu candidat pour la cellule:\\tilde{\\mathbf{C}}_t = \\tanh(\\mathbf{X}_t \\mathbf{W}_{xc} + \\mathbf{H}_{t-1} \\mathbf{W}_{hc} + \\boldsymbol{b}_c)\n\nLa cellule est mise à jour par:\\mathbf{C}_t = \\mathbf{F}_t \\odot \\mathbf{C}_{t-1} + \\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t\n\nCette équation montre clairement les deux flux:\n\n\\mathbf{F}_t \\odot \\mathbf{C}_{t-1}: conservation sélective de la mémoire précédente\n\n\\mathbf{I}_t \\odot \\tilde{\\mathbf{C}}_t: ajout sélectif de nouvelles informations","type":"content","url":"/rnn#mise-jour-de-la-cellule","position":49},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"État caché","lvl2":"Mémoire à court et long terme (LSTM)"},"type":"lvl3","url":"/rnn#id-tat-cach","position":50},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"État caché","lvl2":"Mémoire à court et long terme (LSTM)"},"content":"L’état caché est une version transformée de la cellule:\\mathbf{H}_t = \\mathbf{O}_t \\odot \\tanh(\\mathbf{C}_t)\n\nLa distinction entre \\mathbf{C}_t et \\mathbf{H}_t est importante:\n\n\\mathbf{C}_t est la mémoire à long terme — elle peut conserver l’information sur de nombreux pas de temps\n\n\\mathbf{H}_t est la mémoire à court terme — elle sert de sortie pour le pas de temps actuel","type":"content","url":"/rnn#id-tat-cach","position":51},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Pourquoi les LSTM fonctionnent","lvl2":"Mémoire à court et long terme (LSTM)"},"type":"lvl3","url":"/rnn#pourquoi-les-lstm-fonctionnent","position":52},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Pourquoi les LSTM fonctionnent","lvl2":"Mémoire à court et long terme (LSTM)"},"content":"Le flux de gradient à travers la cellule est:\\frac{\\partial \\mathbf{C}_t}{\\partial \\mathbf{C}_{t-1}} = \\mathbf{F}_t\n\nContrairement au RNN standard où le gradient doit passer par \\mathbf{W}_{hh} à chaque pas, ici le gradient peut circuler directement à travers la cellule, modulé uniquement par \\mathbf{F}_t. Si le réseau apprend \\mathbf{F}_t \\approx 1, l’information peut être préservée indéfiniment.","type":"content","url":"/rnn#pourquoi-les-lstm-fonctionnent","position":53},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl2":"Mécanismes d’attention"},"type":"lvl2","url":"/rnn#m-canismes-dattention","position":54},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl2":"Mécanismes d’attention"},"content":"","type":"content","url":"/rnn#m-canismes-dattention","position":55},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Motivation","lvl2":"Mécanismes d’attention"},"type":"lvl3","url":"/rnn#motivation","position":56},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Motivation","lvl2":"Mécanismes d’attention"},"content":"Les architectures seq2seq classiques encodent toute la séquence d’entrée dans un seul vecteur de contexte. Cette approche a deux limitations:\n\nGoulot d’étranglement: toute l’information doit passer par un vecteur de taille fixe\n\nPas de focus: le décodeur n’a pas accès direct aux parties pertinentes de l’entrée\n\nLes mécanismes d’attention permettent au décodeur de “regarder” dynamiquement différentes parties de la séquence d’entrée.","type":"content","url":"/rnn#motivation","position":57},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Formulation","lvl2":"Mécanismes d’attention"},"type":"lvl3","url":"/rnn#formulation","position":58},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Formulation","lvl2":"Mécanismes d’attention"},"content":"L’attention peut être vue comme une recherche souple dans un dictionnaire:\n\nClés \\mathbf{K} \\in \\mathbb{R}^{m \\times k}: représentations des m éléments de la séquence d’entrée\n\nValeurs \\mathbf{V} \\in \\mathbb{R}^{m \\times v}: informations associées à chaque élément\n\nRequête \\boldsymbol{q} \\in \\mathbb{R}^q: ce que le décodeur cherche à un instant donné\n\nLa sortie de l’attention est une combinaison convexe des valeurs:\\text{Attn}(\\boldsymbol{q}, \\mathbf{K}, \\mathbf{V}) = \\sum_{i=1}^m \\alpha_i(\\boldsymbol{q}, \\boldsymbol{k}_{1:m}) \\, \\boldsymbol{v}_i\n\nLes poids d’attention \\alpha_i satisfont \\alpha_i \\geq 0 et \\sum_i \\alpha_i = 1.","type":"content","url":"/rnn#formulation","position":59},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Calcul des poids d’attention","lvl2":"Mécanismes d’attention"},"type":"lvl3","url":"/rnn#calcul-des-poids-dattention","position":60},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Calcul des poids d’attention","lvl2":"Mécanismes d’attention"},"content":"Les poids sont calculés à partir d’un score d’attention a(\\boldsymbol{q}, \\boldsymbol{k}_i) qui mesure la similarité entre la requête et chaque clé:\\alpha_i(\\boldsymbol{q}, \\boldsymbol{k}_{1:m}) = \\text{softmax}_i\\left([a(\\boldsymbol{q}, \\boldsymbol{k}_1), \\ldots, a(\\boldsymbol{q}, \\boldsymbol{k}_m)]\\right) = \\frac{\\exp(a(\\boldsymbol{q}, \\boldsymbol{k}_i))}{\\sum_{j=1}^m \\exp(a(\\boldsymbol{q}, \\boldsymbol{k}_j))}\n\nExemples de fonctions de score:\n\nProduit scalaire: a(\\boldsymbol{q}, \\boldsymbol{k}) = \\boldsymbol{q}^\\top \\boldsymbol{k}\n\nProduit scalaire normalisé: a(\\boldsymbol{q}, \\boldsymbol{k}) = \\frac{\\boldsymbol{q}^\\top \\boldsymbol{k}}{\\sqrt{d}}\n\nAdditif: a(\\boldsymbol{q}, \\boldsymbol{k}) = \\boldsymbol{w}^\\top \\tanh(\\mathbf{W}_q \\boldsymbol{q} + \\mathbf{W}_k \\boldsymbol{k})","type":"content","url":"/rnn#calcul-des-poids-dattention","position":61},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Application à seq2seq","lvl2":"Mécanismes d’attention"},"type":"lvl3","url":"/rnn#application-seq2seq","position":62},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Application à seq2seq","lvl2":"Mécanismes d’attention"},"content":"Dans un modèle seq2seq avec attention:\n\nL’encodeur produit une séquence d’états cachés (\\boldsymbol{h}_1^{enc}, \\ldots, \\boldsymbol{h}_T^{enc})\n\nCes états servent de clés et de valeurs\n\nL’état caché du décodeur \\boldsymbol{h}_t^{dec} sert de requête\n\nLe contexte attentionnel \\boldsymbol{c}_t = \\text{Attn}(\\boldsymbol{h}_t^{dec}, \\mathbf{H}^{enc}, \\mathbf{H}^{enc}) est utilisé pour la prédiction\n\nCette architecture permet au modèle d’aligner dynamiquement les sorties avec les parties pertinentes de l’entrée, ce qui est particulièrement utile pour la traduction automatique.","type":"content","url":"/rnn#application-seq2seq","position":63},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl2":"Applications"},"type":"lvl2","url":"/rnn#applications","position":64},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl2":"Applications"},"content":"","type":"content","url":"/rnn#applications","position":65},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Génération de légendes d’images","lvl2":"Applications"},"type":"lvl3","url":"/rnn#g-n-ration-de-l-gendes-dimages","position":66},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Génération de légendes d’images","lvl2":"Applications"},"content":"En combinant un CNN (encodeur d’image) avec un RNN (décodeur de texte), on peut générer automatiquement des descriptions d’images:\n\nLe CNN extrait un vecteur de caractéristiques de l’image\n\nCe vecteur initialise l’état caché du RNN\n\nLe RNN génère une séquence de mots décrivant l’image","type":"content","url":"/rnn#g-n-ration-de-l-gendes-dimages","position":67},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Traduction automatique","lvl2":"Applications"},"type":"lvl3","url":"/rnn#traduction-automatique","position":68},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Traduction automatique","lvl2":"Applications"},"content":"Les modèles seq2seq avec attention ont révolutionné la traduction:\n\nL’encodeur lit la phrase source\n\nLe décodeur génère la phrase cible, en utilisant l’attention pour aligner les mots","type":"content","url":"/rnn#traduction-automatique","position":69},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Modèles de langage","lvl2":"Applications"},"type":"lvl3","url":"/rnn#mod-les-de-langage","position":70},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl3":"Modèles de langage","lvl2":"Applications"},"content":"Les RNN peuvent modéliser p(w_1, w_2, \\ldots, w_T) en factorisant:p(w_1, \\ldots, w_T) = \\prod_{t=1}^T p(w_t \\mid w_1, \\ldots, w_{t-1})\n\nCes modèles sont utilisés pour:\n\nLa complétion de texte\n\nLa correction orthographique\n\nLa génération de texte","type":"content","url":"/rnn#mod-les-de-langage","position":71},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl2":"Résumé"},"type":"lvl2","url":"/rnn#r-sum","position":72},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl2":"Résumé"},"content":"Les réseaux de neurones récurrents permettent de traiter des séquences de longueur variable en maintenant un état caché qui résume l’historique. Points clés:\n\nRNN de base: simple mais souffre de la disparition/explosion du gradient\n\nGRU: deux portes (réinitialisation, mise à jour) pour contrôler le flux d’information\n\nLSTM: trois portes et une cellule de mémoire dédiée pour les dépendances à long terme\n\nAttention: permet au décodeur de se concentrer sur les parties pertinentes de l’entrée\n\nCes architectures ont été dominantes en NLP jusqu’à l’avènement des Transformers, qui généralisent l’attention à des architectures entièrement basées sur ce mécanisme.","type":"content","url":"/rnn#r-sum","position":73},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl2":"Exercices"},"type":"lvl2","url":"/rnn#exercices","position":74},{"hierarchy":{"lvl1":"Réseaux de neurones récurrents","lvl2":"Exercices"},"content":"Exercice 1: Déroulement d’un RNN\n\nConsidérez un RNN simple avec \\boldsymbol{h}_t = \\tanh(\\mathbf{W}_{xh} \\boldsymbol{x}_t + \\mathbf{W}_{hh} \\boldsymbol{h}_{t-1}) où:\n\n\\mathbf{W}_{xh} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n\n\\mathbf{W}_{hh} = \\begin{pmatrix} 0.5 & 0 \\\\ 0 & 0.5 \\end{pmatrix}\n\n\\boldsymbol{h}_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n\nCalculez \\boldsymbol{h}_1, \\boldsymbol{h}_2, \\boldsymbol{h}_3 pour la séquence d’entrée x_1 = 1, x_2 = -1, x_3 = 0.5.\n\nExercice 2: Explosion du gradient\n\nSoit \\mathbf{W}_{hh} = \\begin{pmatrix} 1.5 & 0 \\\\ 0 & 1.5 \\end{pmatrix}.\n\nCalculez le rayon spectral de \\mathbf{W}_{hh}.\n\nCalculez \\mathbf{W}_{hh}^{10} et \\mathbf{W}_{hh}^{100}.\n\nExpliquez pourquoi cela pose problème pour la rétropropagation dans le temps.\n\nExercice 3: Porte d’oubli LSTM\n\nDans un LSTM, supposons que la porte d’oubli \\mathbf{F}_t = \\mathbf{1} (tout à 1) et la porte d’entrée \\mathbf{I}_t = \\mathbf{0} (tout à 0) pour tous les pas de temps.\n\nQue devient l’équation de mise à jour de la cellule?\n\nComment évolue \\mathbf{C}_t au cours du temps?\n\nQuel est l’effet sur le gradient lors de la rétropropagation?\n\nExercice 4: Attention\n\nConsidérez une séquence de 3 clés \\boldsymbol{k}_1, \\boldsymbol{k}_2, \\boldsymbol{k}_3 avec les valeurs associées \\boldsymbol{v}_1 = [1, 0], \\boldsymbol{v}_2 = [0, 1], \\boldsymbol{v}_3 = [0.5, 0.5].\n\nSi les scores d’attention (avant softmax) sont a_1 = 2, a_2 = 1, a_3 = 1:\n\nCalculez les poids d’attention \\alpha_1, \\alpha_2, \\alpha_3.\n\nCalculez la sortie de l’attention.\n\nComment changerait la sortie si a_1 = 10?","type":"content","url":"/rnn#exercices","position":75},{"hierarchy":{"lvl1":"Machines à vecteurs de support"},"type":"lvl1","url":"/svm","position":0},{"hierarchy":{"lvl1":"Machines à vecteurs de support"},"content":"Objectifs d’apprentissage\n\nÀ la fin de ce chapitre, vous serez en mesure de:\n\nFormuler le problème de classification à marge maximale\n\nDériver les formulations primale et duale des SVM\n\nExpliquer le concept de marge souple et les variables de jeu\n\nAppliquer l’astuce du noyau pour des frontières non linéaires\n\nDéfinir les noyaux de Mercer et la matrice de Gram\n\nÉnoncer le théorème du représentant\n\nImplémenter un SVM simple avec descente de sous-gradient","type":"content","url":"/svm","position":1},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl2":"Introduction"},"type":"lvl2","url":"/svm#introduction","position":2},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl2":"Introduction"},"content":"Les machines à vecteurs de support (SVM, de l’anglais support vector machines) sont des classifieurs qui cherchent l’hyperplan séparateur maximisant la marge entre les classes. Cette approche géométrique conduit à des propriétés de généralisation remarquables et à l’utilisation de noyaux pour des frontières non linéaires.","type":"content","url":"/svm#introduction","position":3},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl2":"Classifieurs à marge maximale"},"type":"lvl2","url":"/svm#classifieurs-marge-maximale","position":4},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl2":"Classifieurs à marge maximale"},"content":"","type":"content","url":"/svm#classifieurs-marge-maximale","position":5},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Motivation","lvl2":"Classifieurs à marge maximale"},"type":"lvl3","url":"/svm#motivation","position":6},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Motivation","lvl2":"Classifieurs à marge maximale"},"content":"Considérons un problème de classification binaire avec des données linéairement séparables. Il existe alors une infinité d’hyperplans séparateurs possibles. Lequel choisir?\n\nL’intuition géométrique suggère de choisir l’hyperplan qui maximise la marge, c’est-à-dire la distance au point le plus proche de chaque classe. Un classifieur à grande marge est plus robuste aux perturbations et devrait mieux généraliser.","type":"content","url":"/svm#motivation","position":7},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Formulation géométrique","lvl2":"Classifieurs à marge maximale"},"type":"lvl3","url":"/svm#formulation-g-om-trique","position":8},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Formulation géométrique","lvl2":"Classifieurs à marge maximale"},"content":"Considérons un classifieur linéaire:h(x) = \\text{sign}(f(x)), \\quad f(x) = w^\\top x + w_0\n\nLa frontière de décision est l’hyperplan \\{x : w^\\top x + w_0 = 0\\}. Le vecteur w est normal à cet hyperplan.\n\nPour un point x, nous pouvons l’écrire comme:x = x_\\perp + r \\frac{w}{\\|w\\|}\n\noù x_\\perp est la projection orthogonale de x sur l’hyperplan et r est la distance signée à l’hyperplan.\n\nEn substituant dans f(x):f(x) = w^\\top x_\\perp + w_0 + r\\|w\\| = r\\|w\\|\n\ncar w^\\top x_\\perp + w_0 = 0 par définition de l’hyperplan. La distance signée est donc:r = \\frac{f(x)}{\\|w\\|}","type":"content","url":"/svm#formulation-g-om-trique","position":9},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Contraintes de marge","lvl2":"Classifieurs à marge maximale"},"type":"lvl3","url":"/svm#contraintes-de-marge","position":10},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Contraintes de marge","lvl2":"Classifieurs à marge maximale"},"content":"Pour que les exemples soient correctement classifiés avec une marge, nous exigeons:\\tilde{y}_n f(x_n) \\geq 1 \\quad \\text{pour tout } n\n\noù \\tilde{y}_n \\in \\{-1, +1\\} est l’étiquette (avec cette convention plutôt que \\{0, 1\\}). La marge géométrique est alors:\\gamma = \\min_n \\frac{\\tilde{y}_n f(x_n)}{\\|w\\|} = \\frac{1}{\\|w\\|}\n\ncar nous avons imposé \\tilde{y}_n f(x_n) \\geq 1.","type":"content","url":"/svm#contraintes-de-marge","position":11},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Problème d’optimisation primal","lvl2":"Classifieurs à marge maximale"},"type":"lvl3","url":"/svm#probl-me-doptimisation-primal","position":12},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Problème d’optimisation primal","lvl2":"Classifieurs à marge maximale"},"content":"Maximiser la marge 1/\\|w\\| équivaut à minimiser \\|w\\|^2. Le problème de classification à marge dure (hard margin) est:\\min_{w, w_0} \\frac{1}{2}\\|w\\|^2 \\quad \\text{tel que} \\quad \\tilde{y}_n(w^\\top x_n + w_0) \\geq 1, \\quad n = 1, \\ldots, N\n\nC’est un problème d’optimisation quadratique convexe avec d + 1 variables et N contraintes linéaires.","type":"content","url":"/svm#probl-me-doptimisation-primal","position":13},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl2":"Marge souple"},"type":"lvl2","url":"/svm#marge-souple","position":14},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl2":"Marge souple"},"content":"","type":"content","url":"/svm#marge-souple","position":15},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Motivation","lvl2":"Marge souple"},"type":"lvl3","url":"/svm#motivation-1","position":16},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Motivation","lvl2":"Marge souple"},"content":"Si les données ne sont pas linéairement séparables, les contraintes \\tilde{y}_n f(x_n) \\geq 1 ne peuvent pas être satisfaites simultanément. Nous introduisons des variables de jeu (slack variables) \\xi_n \\geq 0 qui permettent de violer les contraintes.","type":"content","url":"/svm#motivation-1","position":17},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Formulation","lvl2":"Marge souple"},"type":"lvl3","url":"/svm#formulation","position":18},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Formulation","lvl2":"Marge souple"},"content":"Le problème de classification à marge souple (soft margin) est:\\min_{w, w_0, \\xi} \\frac{1}{2}\\|w\\|^2 + C \\sum_{n=1}^{N} \\xi_n\n\nsous les contraintes:\\xi_n \\geq 0, \\quad \\tilde{y}_n(w^\\top x_n + w_0) \\geq 1 - \\xi_n\n\nLe paramètre C > 0 contrôle le compromis entre maximiser la marge et minimiser les violations.","type":"content","url":"/svm#formulation","position":19},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Interprétation des variables de jeu","lvl2":"Marge souple"},"type":"lvl3","url":"/svm#interpr-tation-des-variables-de-jeu","position":20},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Interprétation des variables de jeu","lvl2":"Marge souple"},"content":"\\xi_n = 0: le point est correctement classifié avec une marge suffisante\n\n0 < \\xi_n < 1: le point est dans la marge mais du bon côté de l’hyperplan\n\n\\xi_n \\geq 1: le point est mal classifié\n\nLa somme \\sum_n \\xi_n est une borne supérieure sur le nombre d’erreurs.","type":"content","url":"/svm#interpr-tation-des-variables-de-jeu","position":21},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl2":"Formulation duale"},"type":"lvl2","url":"/svm#formulation-duale","position":22},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl2":"Formulation duale"},"content":"","type":"content","url":"/svm#formulation-duale","position":23},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Lagrangien","lvl2":"Formulation duale"},"type":"lvl3","url":"/svm#lagrangien","position":24},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Lagrangien","lvl2":"Formulation duale"},"content":"Le lagrangien du problème à marge souple est:\\mathcal{L}(w, w_0, \\xi, \\alpha, \\mu) = \\frac{1}{2}w^\\top w + C\\sum_{n=1}^N \\xi_n - \\sum_{n=1}^N \\alpha_n\\left(\\tilde{y}_n(w^\\top x_n + w_0) - 1 + \\xi_n\\right) - \\sum_{n=1}^N \\mu_n \\xi_n\n\noù \\alpha_n \\geq 0 et \\mu_n \\geq 0 sont les multiplicateurs de Lagrange.","type":"content","url":"/svm#lagrangien","position":25},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Conditions d’optimalité","lvl2":"Formulation duale"},"type":"lvl3","url":"/svm#conditions-doptimalit","position":26},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Conditions d’optimalité","lvl2":"Formulation duale"},"content":"En annulant les dérivées par rapport aux variables primales:\\frac{\\partial \\mathcal{L}}{\\partial w} = w - \\sum_{n=1}^N \\alpha_n \\tilde{y}_n x_n = 0 \\implies w = \\sum_{n=1}^N \\alpha_n \\tilde{y}_n x_n\\frac{\\partial \\mathcal{L}}{\\partial w_0} = -\\sum_{n=1}^N \\alpha_n \\tilde{y}_n = 0\\frac{\\partial \\mathcal{L}}{\\partial \\xi_n} = C - \\alpha_n - \\mu_n = 0 \\implies \\alpha_n \\leq C","type":"content","url":"/svm#conditions-doptimalit","position":27},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Problème dual","lvl2":"Formulation duale"},"type":"lvl3","url":"/svm#probl-me-dual","position":28},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Problème dual","lvl2":"Formulation duale"},"content":"En substituant ces conditions dans le lagrangien, nous obtenons le problème dual:\\max_\\alpha \\sum_{n=1}^N \\alpha_n - \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j \\tilde{y}_i \\tilde{y}_j x_i^\\top x_j\n\nsous les contraintes:0 \\leq \\alpha_n \\leq C, \\quad \\sum_{n=1}^N \\alpha_n \\tilde{y}_n = 0","type":"content","url":"/svm#probl-me-dual","position":29},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Vecteurs de support","lvl2":"Formulation duale"},"type":"lvl3","url":"/svm#vecteurs-de-support","position":30},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Vecteurs de support","lvl2":"Formulation duale"},"content":"Les conditions de Karush-Kuhn-Tucker (KKT) impliquent que \\alpha_n > 0 seulement si la contrainte correspondante est active:\\alpha_n(\\tilde{y}_n f(x_n) - 1 + \\xi_n) = 0\n\nLes exemples avec \\alpha_n > 0 sont les vecteurs de support. Ils se trouvent sur la frontière de la marge (\\xi_n = 0) ou à l’intérieur de la marge (\\xi_n > 0).\n\nLa solution optimale ne dépend que des vecteurs de support, d’où le nom de la méthode. En pratique, seule une fraction des exemples d’entraînement sont des vecteurs de support.","type":"content","url":"/svm#vecteurs-de-support","position":31},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Prédiction","lvl2":"Formulation duale"},"type":"lvl3","url":"/svm#pr-diction","position":32},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Prédiction","lvl2":"Formulation duale"},"content":"La fonction de prédiction s’exprime en fonction des vecteurs de support:f(x) = \\sum_{n \\in \\mathcal{S}} \\alpha_n \\tilde{y}_n x_n^\\top x + w_0\n\noù \\mathcal{S} est l’ensemble des indices des vecteurs de support.\n\nLe biais w_0 se calcule à partir des vecteurs de support sur la marge (0 < \\alpha_n < C):w_0 = \\frac{1}{|\\mathcal{M}|} \\sum_{n \\in \\mathcal{M}} \\left(\\tilde{y}_n - \\sum_{m \\in \\mathcal{S}} \\alpha_m \\tilde{y}_m x_m^\\top x_n\\right)","type":"content","url":"/svm#pr-diction","position":33},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl2":"Lien avec la perte à charnière"},"type":"lvl2","url":"/svm#lien-avec-la-perte-charni-re","position":34},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl2":"Lien avec la perte à charnière"},"content":"Le problème SVM à marge souple peut se reformuler comme:\\min_w \\sum_{n=1}^N \\ell_{\\text{hinge}}(\\tilde{y}_n, f(x_n)) + \\lambda\\|w\\|^2\n\noù \\lambda = 1/(2C) et la perte à charnière est:\\ell_{\\text{hinge}}(\\tilde{y}, \\eta) = \\max(0, 1 - \\tilde{y}\\eta) = [1 - \\tilde{y}\\eta]_+\n\nCette formulation est analogue à la régression logistique régularisée, avec la perte à charnière remplaçant la perte logistique.","type":"content","url":"/svm#lien-avec-la-perte-charni-re","position":35},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl2":"L’astuce du noyau"},"type":"lvl2","url":"/svm#lastuce-du-noyau","position":36},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl2":"L’astuce du noyau"},"content":"","type":"content","url":"/svm#lastuce-du-noyau","position":37},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Motivation","lvl2":"L’astuce du noyau"},"type":"lvl3","url":"/svm#motivation-2","position":38},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Motivation","lvl2":"L’astuce du noyau"},"content":"La formulation duale ne dépend des données qu’à travers les produits scalaires x_i^\\top x_j. Nous pouvons remplacer ces produits par une fonction de noyau:\\mathcal{K}(x_i, x_j) = \\phi(x_i)^\\top \\phi(x_j)\n\noù \\phi: \\mathbb{R}^d \\to \\mathbb{R}^m est une transformation (potentiellement vers un espace de dimension infinie).","type":"content","url":"/svm#motivation-2","position":39},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Problème dual avec noyau","lvl2":"L’astuce du noyau"},"type":"lvl3","url":"/svm#probl-me-dual-avec-noyau","position":40},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Problème dual avec noyau","lvl2":"L’astuce du noyau"},"content":"\\max_\\alpha \\sum_{n=1}^N \\alpha_n - \\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j \\tilde{y}_i \\tilde{y}_j \\mathcal{K}(x_i, x_j)\n\nLa fonction de prédiction devient:f(x) = \\sum_{n \\in \\mathcal{S}} \\alpha_n \\tilde{y}_n \\mathcal{K}(x_n, x) + w_0\n\nL’astuce est que nous n’avons jamais besoin de calculer \\phi(x) explicitement. Nous évaluons seulement le noyau.","type":"content","url":"/svm#probl-me-dual-avec-noyau","position":41},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl2":"Noyaux de Mercer"},"type":"lvl2","url":"/svm#noyaux-de-mercer","position":42},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl2":"Noyaux de Mercer"},"content":"","type":"content","url":"/svm#noyaux-de-mercer","position":43},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Définition","lvl2":"Noyaux de Mercer"},"type":"lvl3","url":"/svm#d-finition","position":44},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Définition","lvl2":"Noyaux de Mercer"},"content":"Un noyau \\mathcal{K}: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R} est un noyau de Mercer (ou noyau défini positif) si pour tout ensemble de points \\{x_1, \\ldots, x_N\\} et tout vecteur c \\in \\mathbb{R}^N:\\sum_{i=1}^N \\sum_{j=1}^N c_i c_j \\mathcal{K}(x_i, x_j) \\geq 0\n\nDe manière équivalente, la matrice de Gram:K_{ij} = \\mathcal{K}(x_i, x_j)\n\ndoit être semi-définie positive pour tout ensemble de points.","type":"content","url":"/svm#d-finition","position":45},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Théorème de Mercer","lvl2":"Noyaux de Mercer"},"type":"lvl3","url":"/svm#th-or-me-de-mercer","position":46},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Théorème de Mercer","lvl2":"Noyaux de Mercer"},"content":"Si \\mathcal{K} est un noyau de Mercer, alors il existe un espace de Hilbert \\mathcal{H} et une fonction \\phi: \\mathcal{X} \\to \\mathcal{H} tels que:\\mathcal{K}(x, x') = \\langle\\phi(x), \\phi(x')\\rangle_{\\mathcal{H}}\n\nLe noyau est donc bien un produit scalaire dans un certain espace de caractéristiques.","type":"content","url":"/svm#th-or-me-de-mercer","position":47},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Exemples de noyaux","lvl2":"Noyaux de Mercer"},"type":"lvl3","url":"/svm#exemples-de-noyaux","position":48},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Exemples de noyaux","lvl2":"Noyaux de Mercer"},"content":"Noyau linéaire:\\mathcal{K}(x, x') = x^\\top x'\n\nNoyau polynomial:\\mathcal{K}(x, x') = (x^\\top x' + c)^d\n\noù d est le degré et c \\geq 0.\n\nNoyau gaussien (RBF):\\mathcal{K}(x, x') = \\exp\\left(-\\frac{\\|x - x'\\|^2}{2\\ell^2}\\right)\n\noù \\ell est le paramètre de bande passante. Ce noyau correspond à un espace de caractéristiques de dimension infinie.\n\nNoyau périodique:\\mathcal{K}(r) = \\exp\\left(-\\frac{2}{\\ell^2}\\sin^2\\left(\\pi\\frac{r}{p}\\right)\\right)\n\noù r = |x - x'| et p est la période.","type":"content","url":"/svm#exemples-de-noyaux","position":49},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Composition de noyaux","lvl2":"Noyaux de Mercer"},"type":"lvl3","url":"/svm#composition-de-noyaux","position":50},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Composition de noyaux","lvl2":"Noyaux de Mercer"},"content":"De nouveaux noyaux valides peuvent être construits par:\n\nMultiplication par un scalaire: c \\cdot \\mathcal{K}(x, x') pour c > 0\n\nSomme: \\mathcal{K}_1(x, x') + \\mathcal{K}_2(x, x')\n\nProduit: \\mathcal{K}_1(x, x') \\cdot \\mathcal{K}_2(x, x')\n\nExponentielle: \\exp(\\mathcal{K}(x, x'))\n\nPolynôme: q(\\mathcal{K}(x, x')) pour tout polynôme q à coefficients non négatifs","type":"content","url":"/svm#composition-de-noyaux","position":51},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Exemple: le noyau quadratique","lvl2":"Noyaux de Mercer"},"type":"lvl3","url":"/svm#exemple-le-noyau-quadratique","position":52},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Exemple: le noyau quadratique","lvl2":"Noyaux de Mercer"},"content":"Pour x = (x_1, x_2) \\in \\mathbb{R}^2, le noyau quadratique \\mathcal{K}(x, x') = (x^\\top x')^2 se développe:\\mathcal{K}(x, x') = (x_1 x_1' + x_2 x_2')^2 = x_1^2 (x_1')^2 + 2x_1 x_2 x_1' x_2' + x_2^2 (x_2')^2\n\nCeci correspond à \\phi(x) = [x_1^2, \\sqrt{2}x_1 x_2, x_2^2]^\\top \\in \\mathbb{R}^3.\n\nLe noyau quadratique en 2D équivaut à un produit scalaire dans un espace de caractéristiques en 3D.","type":"content","url":"/svm#exemple-le-noyau-quadratique","position":53},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl2":"Le théorème du représentant"},"type":"lvl2","url":"/svm#le-th-or-me-du-repr-sentant","position":54},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl2":"Le théorème du représentant"},"content":"","type":"content","url":"/svm#le-th-or-me-du-repr-sentant","position":55},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Énoncé","lvl2":"Le théorème du représentant"},"type":"lvl3","url":"/svm#id-nonc","position":56},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Énoncé","lvl2":"Le théorème du représentant"},"content":"Soit \\mathcal{H}_\\mathcal{K} un espace de Hilbert à noyau reproduisant (RKHS) associé au noyau \\mathcal{K}. Pour tout problème de la forme:\\min_{f \\in \\mathcal{H}_\\mathcal{K}} \\sum_{n=1}^N L(y_n, f(x_n)) + \\lambda\\|f\\|_{\\mathcal{H}_\\mathcal{K}}^2\n\nla solution optimale s’écrit:f(x) = \\sum_{n=1}^N \\alpha_n \\mathcal{K}(x, x_n)","type":"content","url":"/svm#id-nonc","position":57},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Implications","lvl2":"Le théorème du représentant"},"type":"lvl3","url":"/svm#implications","position":58},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Implications","lvl2":"Le théorème du représentant"},"content":"Ce théorème est fondamental car:\n\nIl réduit un problème de dimension infinie (optimiser sur toutes les fonctions de \\mathcal{H}_\\mathcal{K}) à un problème de dimension finie (N coefficients \\alpha_n)\n\nIl justifie l’utilisation de méthodes à noyaux dans de nombreux contextes au-delà des SVM","type":"content","url":"/svm#implications","position":59},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Application: régression ridge à noyau","lvl2":"Le théorème du représentant"},"type":"lvl3","url":"/svm#application-r-gression-ridge-noyau","position":60},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Application: régression ridge à noyau","lvl2":"Le théorème du représentant"},"content":"La régression ridge:\\min_w \\|y - Xw\\|^2 + \\lambda\\|w\\|^2\n\na pour forme duale:\\min_\\alpha (y - K\\alpha)^\\top(y - K\\alpha) + \\lambda \\alpha^\\top K \\alpha\n\nLa solution est \\hat{\\alpha} = (K + \\lambda I)^{-1}y et les prédictions sont \\hat{f} = K\\hat{\\alpha}.","type":"content","url":"/svm#application-r-gression-ridge-noyau","position":61},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl2":"Optimisation par sous-gradient"},"type":"lvl2","url":"/svm#optimisation-par-sous-gradient","position":62},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl2":"Optimisation par sous-gradient"},"content":"La perte à charnière n’est pas différentiable en \\tilde{y}\\eta = 1. Nous utilisons la méthode du sous-gradient.","type":"content","url":"/svm#optimisation-par-sous-gradient","position":63},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Sous-gradient","lvl2":"Optimisation par sous-gradient"},"type":"lvl3","url":"/svm#sous-gradient","position":64},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Sous-gradient","lvl2":"Optimisation par sous-gradient"},"content":"Pour une fonction convexe f, un vecteur g est un sous-gradient de f en x si:f(z) \\geq f(x) + g^\\top(z - x) \\quad \\text{pour tout } z\n\nL’ensemble des sous-gradients est le sous-différentiel \\partial f(x).\n\nPour la perte à charnière \\ell(\\tilde{y}, \\eta) = [1 - \\tilde{y}\\eta]_+:\\partial_\\eta \\ell(\\tilde{y}, \\eta) = \\begin{cases}\n0 & \\text{si } \\tilde{y}\\eta > 1 \\\\\n[-\\tilde{y}, 0] & \\text{si } \\tilde{y}\\eta = 1 \\\\\n-\\tilde{y} & \\text{si } \\tilde{y}\\eta < 1\n\\end{cases}","type":"content","url":"/svm#sous-gradient","position":65},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Descente de sous-gradient","lvl2":"Optimisation par sous-gradient"},"type":"lvl3","url":"/svm#descente-de-sous-gradient","position":66},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl3":"Descente de sous-gradient","lvl2":"Optimisation par sous-gradient"},"content":"La mise à jour pour le SVM linéaire avec perte à charnière est:w_{t+1} = w_t - \\eta_t \\left(\\lambda w_t + \\frac{1}{N}\\sum_{n: \\tilde{y}_n f(x_n) < 1} -\\tilde{y}_n x_n\\right)\n\nLes exemples contribuant au gradient sont ceux dont la marge est insuffisante.","type":"content","url":"/svm#descente-de-sous-gradient","position":67},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl2":"Implémentation"},"type":"lvl2","url":"/svm#impl-mentation","position":68},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl2":"Implémentation"},"content":"import numpy as np\n\ndef hinge_loss(y, scores):\n    \"\"\"Perte à charnière.\"\"\"\n    return np.maximum(0, 1 - y * scores)\n\ndef svm_sgd(X, y, C=1.0, lr=0.01, n_epochs=100):\n    \"\"\"SVM linéaire par descente de sous-gradient.\n    \n    Args:\n        X: Matrice de design (N x d)\n        y: Étiquettes dans {-1, +1} (N,)\n        C: Paramètre de régularisation\n        lr: Taux d'apprentissage initial\n        n_epochs: Nombre d'époques\n        \n    Returns:\n        Vecteur de poids (d,), biais\n    \"\"\"\n    N, d = X.shape\n    w = np.zeros(d)\n    b = 0.0\n    \n    for epoch in range(n_epochs):\n        for i in np.random.permutation(N):\n            eta = lr / (1 + epoch)  # Décroissance du taux\n            \n            score = np.dot(X[i], w) + b\n            \n            if y[i] * score < 1:\n                # Exemple avec marge insuffisante\n                w = (1 - eta / C) * w + eta * y[i] * X[i]\n                b = b + eta * y[i]\n            else:\n                # Exemple bien classifié avec marge\n                w = (1 - eta / C) * w\n                \n    return w, b\n\ndef predict(X, w, b):\n    \"\"\"Prédictions SVM.\"\"\"\n    return np.sign(X @ w + b)","type":"content","url":"/svm#impl-mentation","position":69},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl2":"Résumé"},"type":"lvl2","url":"/svm#r-sum","position":70},{"hierarchy":{"lvl1":"Machines à vecteurs de support","lvl2":"Résumé"},"content":"Ce chapitre a présenté les machines à vecteurs de support:\n\nLes SVM maximisent la marge géométrique entre les classes\n\nLe problème primal minimise \\|w\\|^2 sous contraintes de marge\n\nLes variables de jeu permettent de traiter les données non séparables\n\nLe problème dual ne dépend que des produits scalaires, permettant l’astuce du noyau\n\nLes noyaux de Mercer définissent des produits scalaires implicites dans des espaces de grande dimension\n\nLe théorème du représentant justifie que la solution est une combinaison linéaire de noyaux\n\nLa perte à charnière relie les SVM à la régularisation\n\nLe chapitre suivant introduit les réseaux de neurones, qui étendent les modèles linéaires par composition de transformations non linéaires.","type":"content","url":"/svm#r-sum","position":71},{"hierarchy":{"lvl1":"Arbres de décision"},"type":"lvl1","url":"/trees","position":0},{"hierarchy":{"lvl1":"Arbres de décision"},"content":"Objectifs d’apprentissage\n\nÀ la fin de ce chapitre, vous serez en mesure de:\n\nDécrire la structure des arbres de décision et leur fonctionnement\n\nExpliquer les critères de division (impureté de Gini, entropie)\n\nImplémenter un algorithme glouton d’apprentissage d’arbre\n\nDiscuter de l’élagage et de la régularisation\n\nAppliquer les arbres à la classification et à la régression\n\nComprendre pourquoi les arbres sont des modèles de base idéaux pour les ensembles","type":"content","url":"/trees","position":1},{"hierarchy":{"lvl1":"Arbres de décision","lvl2":"Motivation"},"type":"lvl2","url":"/trees#motivation","position":2},{"hierarchy":{"lvl1":"Arbres de décision","lvl2":"Motivation"},"content":"Les arbres de décision sont parmi les modèles d’apprentissage automatique les plus intuitifs et interprétables. Ils partitionnent l’espace des caractéristiques en régions avec des prédictions simples, imitant ainsi le processus de décision humain sous forme d’une série de questions.\n\nPar exemple, pour prédire si un objet est un fruit comestible, un arbre pourrait poser successivement les questions:\n\n“La couleur est-elle rouge ou jaune?”\n\n“La forme est-elle ronde?”\n\n“La taille est-elle supérieure à 5 cm?”\n\nChaque question divise l’espace en deux régions, et les réponses successives mènent à une prédiction finale.","type":"content","url":"/trees#motivation","position":3},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Avantages des arbres de décision","lvl2":"Motivation"},"type":"lvl3","url":"/trees#avantages-des-arbres-de-d-cision","position":4},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Avantages des arbres de décision","lvl2":"Motivation"},"content":"Les arbres de décision sont populaires pour plusieurs raisons:\n\nInterprétabilité: on peut visualiser et expliquer facilement les règles de décision\n\nGestion des types mixtes: ils traitent naturellement les caractéristiques discrètes et continues\n\nInvariance aux transformations monotones: les seuils sont basés sur le rang des valeurs, pas leur magnitude\n\nSélection automatique de variables: les caractéristiques non informatives ne sont pas utilisées\n\nRobustesse aux valeurs aberrantes: les seuils ne sont pas affectés par les valeurs extrêmes\n\nScalabilité: ils sont rapides à entraîner et s’adaptent bien aux grands ensembles de données\n\nGestion des valeurs manquantes: des stratégies existent pour les entrées incomplètes","type":"content","url":"/trees#avantages-des-arbres-de-d-cision","position":5},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Inconvénients","lvl2":"Motivation"},"type":"lvl3","url":"/trees#inconv-nients","position":6},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Inconvénients","lvl2":"Motivation"},"content":"Cependant, les arbres ont aussi des limitations:\n\nSurapprentissage facile: sans régularisation, ils mémorisent les données\n\nInstabilité: de petits changements dans les données peuvent modifier drastiquement l’arbre\n\nFrontières axiales: les divisions sont parallèles aux axes, limitant l’expressivité\n\nCette instabilité est précisément ce qui rend les arbres excellents pour les méthodes d’ensemble, que nous verrons au chapitre suivant.","type":"content","url":"/trees#inconv-nients","position":7},{"hierarchy":{"lvl1":"Arbres de décision","lvl2":"Structure d’un arbre de décision"},"type":"lvl2","url":"/trees#structure-dun-arbre-de-d-cision","position":8},{"hierarchy":{"lvl1":"Arbres de décision","lvl2":"Structure d’un arbre de décision"},"content":"","type":"content","url":"/trees#structure-dun-arbre-de-d-cision","position":9},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Composants","lvl2":"Structure d’un arbre de décision"},"type":"lvl3","url":"/trees#composants","position":10},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Composants","lvl2":"Structure d’un arbre de décision"},"content":"Un arbre de décision est composé de:\n\nNœuds internes: posent des questions sur les caractéristiques\n\nBranches: représentent les réponses possibles\n\nFeuilles: contiennent les prédictions finales","type":"content","url":"/trees#composants","position":11},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Arbres de régression","lvl2":"Structure d’un arbre de décision"},"type":"lvl3","url":"/trees#arbres-de-r-gression","position":12},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Arbres de régression","lvl2":"Structure d’un arbre de décision"},"content":"Pour la régression, un arbre partitionne l’espace d’entrée en J régions disjointes R_1, R_2, \\ldots, R_J et associe une valeur constante w_j à chaque région:f(\\boldsymbol{x}; \\boldsymbol{\\theta}) = \\sum_{j=1}^J w_j \\, \\mathbb{I}(\\boldsymbol{x} \\in R_j)\n\noù \\boldsymbol{\\theta} = \\{(R_j, w_j) : j = 1, \\ldots, J\\} représente les paramètres de l’arbre.\n\nChaque région R_j est définie par une conjonction de conditions. Par exemple:R_1 = \\{\\boldsymbol{x} : x_1 \\leq t_1 \\text{ et } x_2 \\leq t_2\\}\n\nLa prédiction pour cette région est la moyenne des réponses des exemples qui y tombent:w_j = \\frac{\\sum_{n=1}^N y_n \\, \\mathbb{I}(\\boldsymbol{x}_n \\in R_j)}{\\sum_{n=1}^N \\mathbb{I}(\\boldsymbol{x}_n \\in R_j)}\n\nLe résultat est une surface constante par morceaux.","type":"content","url":"/trees#arbres-de-r-gression","position":13},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Arbres de classification","lvl2":"Structure d’un arbre de décision"},"type":"lvl3","url":"/trees#arbres-de-classification","position":14},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Arbres de classification","lvl2":"Structure d’un arbre de décision"},"content":"Pour la classification, chaque feuille contient une distribution sur les étiquettes de classe plutôt qu’une valeur unique. La distribution empirique au nœud i est:\\hat{\\pi}_{ic} = \\frac{1}{|\\mathcal{D}_i|} \\sum_{n \\in \\mathcal{D}_i} \\mathbb{I}(y_n = c)\n\noù \\mathcal{D}_i est l’ensemble des exemples atteignant le nœud i.\n\nLa prédiction peut être:\n\nLa classe majoritaire: \\hat{y} = \\arg\\max_c \\hat{\\pi}_{ic}\n\nLa distribution complète \\hat{\\boldsymbol{\\pi}}_i pour une estimation probabiliste","type":"content","url":"/trees#arbres-de-classification","position":15},{"hierarchy":{"lvl1":"Arbres de décision","lvl2":"Apprentissage d’un arbre"},"type":"lvl2","url":"/trees#apprentissage-dun-arbre","position":16},{"hierarchy":{"lvl1":"Arbres de décision","lvl2":"Apprentissage d’un arbre"},"content":"","type":"content","url":"/trees#apprentissage-dun-arbre","position":17},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Le problème d’optimisation","lvl2":"Apprentissage d’un arbre"},"type":"lvl3","url":"/trees#le-probl-me-doptimisation","position":18},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Le problème d’optimisation","lvl2":"Apprentissage d’un arbre"},"content":"Idéalement, nous voudrions minimiser:\\mathcal{L}(\\boldsymbol{\\theta}) = \\sum_{n=1}^N \\ell(y_n, f(\\boldsymbol{x}_n; \\boldsymbol{\\theta})) = \\sum_{j=1}^J \\sum_{\\boldsymbol{x}_n \\in R_j} \\ell(y_n, w_j)\n\nCependant, cette fonction n’est pas différentiable car nous devons chercher dans l’espace discret des structures d’arbres. Trouver la partition optimale est un problème NP-complet.","type":"content","url":"/trees#le-probl-me-doptimisation","position":19},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Algorithme glouton","lvl2":"Apprentissage d’un arbre"},"type":"lvl3","url":"/trees#algorithme-glouton","position":20},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Algorithme glouton","lvl2":"Apprentissage d’un arbre"},"content":"En pratique, nous utilisons une procédure gloutonne (greedy) qui développe l’arbre itérativement, un nœud à la fois. Les algorithmes classiques (CART, C4.5, ID3) suivent cette approche.\n\nÀ chaque nœud i, soit \\mathcal{D}_i = \\{(\\boldsymbol{x}_n, y_n) : n \\in N_i\\} l’ensemble des exemples qui atteignent ce nœud. Nous cherchons la meilleure division pour minimiser l’erreur dans les sous-arbres enfants.","type":"content","url":"/trees#algorithme-glouton","position":21},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Types de divisions","lvl2":"Apprentissage d’un arbre"},"type":"lvl3","url":"/trees#types-de-divisions","position":22},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Types de divisions","lvl2":"Apprentissage d’un arbre"},"content":"Pour une caractéristique continue j: on compare avec un seuil t\n\nEnfant gauche: \\mathcal{D}_i^L(j, t) = \\{(\\boldsymbol{x}_n, y_n) \\in \\mathcal{D}_i : x_{nj} \\leq t\\}\n\nEnfant droit: \\mathcal{D}_i^R(j, t) = \\{(\\boldsymbol{x}_n, y_n) \\in \\mathcal{D}_i : x_{nj} > t\\}\n\nL’ensemble des seuils candidats \\mathcal{T}_j est obtenu en triant les valeurs uniques de \\{x_{nj}\\}.\n\nPour une caractéristique catégorielle avec K_j valeurs: on teste chaque valeur possible, créant K_j divisions binaires.","type":"content","url":"/trees#types-de-divisions","position":23},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Critère de division","lvl2":"Apprentissage d’un arbre"},"type":"lvl3","url":"/trees#crit-re-de-division","position":24},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Critère de division","lvl2":"Apprentissage d’un arbre"},"content":"Nous choisissons la meilleure caractéristique j_i et le meilleur seuil t_i en minimisant:(j_i, t_i) = \\arg\\min_{j \\in \\{1, \\ldots, D\\}} \\min_{t \\in \\mathcal{T}_j} \\left[ \\frac{|\\mathcal{D}_i^L(j, t)|}{|\\mathcal{D}_i|} c(\\mathcal{D}_i^L(j, t)) + \\frac{|\\mathcal{D}_i^R(j, t)|}{|\\mathcal{D}_i|} c(\\mathcal{D}_i^R(j, t)) \\right]\n\noù c(\\cdot) est une fonction de coût mesurant l’impureté d’un nœud.","type":"content","url":"/trees#crit-re-de-division","position":25},{"hierarchy":{"lvl1":"Arbres de décision","lvl2":"Critères d’impureté"},"type":"lvl2","url":"/trees#crit-res-dimpuret","position":26},{"hierarchy":{"lvl1":"Arbres de décision","lvl2":"Critères d’impureté"},"content":"","type":"content","url":"/trees#crit-res-dimpuret","position":27},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Pour la régression: erreur quadratique","lvl2":"Critères d’impureté"},"type":"lvl3","url":"/trees#pour-la-r-gression-erreur-quadratique","position":28},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Pour la régression: erreur quadratique","lvl2":"Critères d’impureté"},"content":"En régression, nous utilisons l’erreur quadratique moyenne:\\text{cost}(\\mathcal{D}_i) = \\frac{1}{|\\mathcal{D}_i|} \\sum_{n \\in \\mathcal{D}_i} (y_n - \\bar{y}_i)^2\n\noù \\bar{y}_i = \\frac{1}{|\\mathcal{D}_i|} \\sum_{n \\in \\mathcal{D}_i} y_n est la moyenne des réponses au nœud i.","type":"content","url":"/trees#pour-la-r-gression-erreur-quadratique","position":29},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Pour la classification: indice de Gini","lvl2":"Critères d’impureté"},"type":"lvl3","url":"/trees#pour-la-classification-indice-de-gini","position":30},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Pour la classification: indice de Gini","lvl2":"Critères d’impureté"},"content":"L’indice de Gini mesure l’impureté d’un nœud:G_i = \\sum_{c=1}^C \\hat{\\pi}_{ic}(1 - \\hat{\\pi}_{ic}) = 1 - \\sum_{c=1}^C \\hat{\\pi}_{ic}^2\n\nPropriétés:\n\nG_i = 0 si le nœud est pur (tous les exemples de la même classe)\n\nG_i est maximal quand les classes sont équiprobables\n\nG_i \\in [0, 1 - 1/C] pour C classes\n\nL’indice de Gini peut être interprété comme la probabilité de mal classifier un exemple choisi au hasard si on lui assigne une classe selon la distribution \\hat{\\boldsymbol{\\pi}}_i.","type":"content","url":"/trees#pour-la-classification-indice-de-gini","position":31},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Pour la classification: entropie","lvl2":"Critères d’impureté"},"type":"lvl3","url":"/trees#pour-la-classification-entropie","position":32},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Pour la classification: entropie","lvl2":"Critères d’impureté"},"content":"Alternativement, nous pouvons utiliser l’entropie (aussi appelée déviance):H_i = \\mathbb{H}(\\hat{\\boldsymbol{\\pi}}_i) = -\\sum_{c=1}^C \\hat{\\pi}_{ic} \\log \\hat{\\pi}_{ic}\n\nPropriétés similaires:\n\nH_i = 0 pour un nœud pur\n\nH_i est maximal pour une distribution uniforme\n\nH_i \\in [0, \\log C]\n\nLe gain d’information est la réduction d’entropie obtenue par une division:\\text{IG}(j, t) = H_i - \\left[ \\frac{|\\mathcal{D}_i^L|}{|\\mathcal{D}_i|} H_L + \\frac{|\\mathcal{D}_i^R|}{|\\mathcal{D}_i|} H_R \\right]","type":"content","url":"/trees#pour-la-classification-entropie","position":33},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Comparaison des critères","lvl2":"Critères d’impureté"},"type":"lvl3","url":"/trees#comparaison-des-crit-res","position":34},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Comparaison des critères","lvl2":"Critères d’impureté"},"content":"En pratique, l’indice de Gini et l’entropie donnent des résultats similaires. Le Gini est légèrement plus rapide à calculer car il évite le logarithme. L’entropie a l’avantage d’être liée à la théorie de l’information.","type":"content","url":"/trees#comparaison-des-crit-res","position":35},{"hierarchy":{"lvl1":"Arbres de décision","lvl2":"Régularisation et élagage"},"type":"lvl2","url":"/trees#r-gularisation-et-lagage","position":36},{"hierarchy":{"lvl1":"Arbres de décision","lvl2":"Régularisation et élagage"},"content":"","type":"content","url":"/trees#r-gularisation-et-lagage","position":37},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Le problème du surapprentissage","lvl2":"Régularisation et élagage"},"type":"lvl3","url":"/trees#le-probl-me-du-surapprentissage","position":38},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Le problème du surapprentissage","lvl2":"Régularisation et élagage"},"content":"Si nous laissons l’arbre croître jusqu’à ce que chaque feuille contienne un seul exemple, nous obtenons une erreur d’entraînement de 0 mais une très mauvaise généralisation.","type":"content","url":"/trees#le-probl-me-du-surapprentissage","position":39},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Stratégies de régularisation","lvl2":"Régularisation et élagage"},"type":"lvl3","url":"/trees#strat-gies-de-r-gularisation","position":40},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Stratégies de régularisation","lvl2":"Régularisation et élagage"},"content":"Arrêt précoce (pre-pruning): on stoppe la croissance selon des heuristiques:\n\nProfondeur maximale atteinte\n\nNombre minimal d’exemples par feuille\n\nGain d’information insuffisant\n\nÉlagage (post-pruning): on fait croître l’arbre complet, puis on fusionne les sous-arbres avec leur parent:\n\nÉlagage par coût-complexité (CART)\n\nÉlagage par erreur réduite\n\nÉlagage pessimiste (C4.5)\n\nL’élagage post-construction est généralement préféré car il évite de stopper prématurément une division qui pourrait être bénéfique plus tard.","type":"content","url":"/trees#strat-gies-de-r-gularisation","position":41},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Régularisation par coût-complexité","lvl2":"Régularisation et élagage"},"type":"lvl3","url":"/trees#r-gularisation-par-co-t-complexit","position":42},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Régularisation par coût-complexité","lvl2":"Régularisation et élagage"},"content":"CART utilise un critère de coût-complexité:C_\\alpha(T) = \\sum_{j=1}^{|T|} \\sum_{\\boldsymbol{x}_n \\in R_j} \\ell(y_n, w_j) + \\alpha |T|\n\noù |T| est le nombre de feuilles et \\alpha \\geq 0 est un hyperparamètre de complexité. Plus \\alpha est grand, plus l’arbre sera petit.","type":"content","url":"/trees#r-gularisation-par-co-t-complexit","position":43},{"hierarchy":{"lvl1":"Arbres de décision","lvl2":"L’instabilité des arbres"},"type":"lvl2","url":"/trees#linstabilit-des-arbres","position":44},{"hierarchy":{"lvl1":"Arbres de décision","lvl2":"L’instabilité des arbres"},"content":"","type":"content","url":"/trees#linstabilit-des-arbres","position":45},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Sensibilité aux données","lvl2":"L’instabilité des arbres"},"type":"lvl3","url":"/trees#sensibilit-aux-donn-es","position":46},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Sensibilité aux données","lvl2":"L’instabilité des arbres"},"content":"Les arbres de décision sont instables: de petits changements dans les données d’entraînement peuvent produire des arbres très différents.\n\nCela vient du processus de construction glouton: si une division précoce change, toute la structure en aval est affectée. Cette propriété est parfois appelée haute variance.","type":"content","url":"/trees#sensibilit-aux-donn-es","position":47},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Illustration","lvl2":"L’instabilité des arbres"},"type":"lvl3","url":"/trees#illustration","position":48},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Illustration","lvl2":"L’instabilité des arbres"},"content":"Considérons les données Iris. En entraînant un arbre sur l’ensemble complet, nous obtenons une certaine frontière de décision. Si nous omettons un seul point (proche de la frontière), l’arbre résultant peut avoir une structure complètement différente.\n\nCette instabilité est un inconvénient pour un modèle unique, mais un avantage pour les méthodes d’ensemble: des arbres divers peuvent être combinés pour obtenir des prédictions plus robustes.","type":"content","url":"/trees#illustration","position":49},{"hierarchy":{"lvl1":"Arbres de décision","lvl2":"Arbres vs autres modèles"},"type":"lvl2","url":"/trees#arbres-vs-autres-mod-les","position":50},{"hierarchy":{"lvl1":"Arbres de décision","lvl2":"Arbres vs autres modèles"},"content":"","type":"content","url":"/trees#arbres-vs-autres-mod-les","position":51},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Comparaison avec les modèles linéaires","lvl2":"Arbres vs autres modèles"},"type":"lvl3","url":"/trees#comparaison-avec-les-mod-les-lin-aires","position":52},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Comparaison avec les modèles linéaires","lvl2":"Arbres vs autres modèles"},"content":"Aspect\n\nArbres de décision\n\nModèles linéaires\n\nFrontières\n\nParallèles aux axes\n\nHyperplans obliques\n\nInteractions\n\nCapturées naturellement\n\nNécessitent des termes explicites\n\nInterprétabilité\n\nRègles si-alors\n\nCoefficients\n\nExtrapolation\n\nConstante (dernière région)\n\nLinéaire","type":"content","url":"/trees#comparaison-avec-les-mod-les-lin-aires","position":53},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Comparaison avec k-NN","lvl2":"Arbres vs autres modèles"},"type":"lvl3","url":"/trees#comparaison-avec-k-nn","position":54},{"hierarchy":{"lvl1":"Arbres de décision","lvl3":"Comparaison avec k-NN","lvl2":"Arbres vs autres modèles"},"content":"Aspect\n\nArbres de décision\n\nk-NN\n\nEntraînement\n\nLent (construction de l’arbre)\n\nAucun\n\nPrédiction\n\nRapide (parcours de l’arbre)\n\nLent (calcul des distances)\n\nMémoire\n\nModèle compact\n\nStocke toutes les données\n\nInterprétabilité\n\nRègles explicites\n\nExemples similaires","type":"content","url":"/trees#comparaison-avec-k-nn","position":55},{"hierarchy":{"lvl1":"Arbres de décision","lvl2":"Résumé"},"type":"lvl2","url":"/trees#r-sum","position":56},{"hierarchy":{"lvl1":"Arbres de décision","lvl2":"Résumé"},"content":"Les arbres de décision sont des modèles puissants et interprétables qui:\n\nPartitionnent l’espace d’entrée en régions avec des prédictions simples\n\nUtilisent des critères d’impureté (Gini, entropie, MSE) pour guider la construction\n\nSont appris par un algorithme glouton car l’optimisation exacte est NP-complète\n\nNécessitent une régularisation (élagage, profondeur maximale) pour éviter le surapprentissage\n\nSont instables mais cette propriété les rend idéaux pour les méthodes d’ensemble\n\nAu chapitre suivant, nous verrons comment combiner plusieurs arbres pour créer des modèles plus robustes et performants: le bagging et les forêts aléatoires.","type":"content","url":"/trees#r-sum","position":57},{"hierarchy":{"lvl1":"Arbres de décision","lvl2":"Exercices"},"type":"lvl2","url":"/trees#exercices","position":58},{"hierarchy":{"lvl1":"Arbres de décision","lvl2":"Exercices"},"content":"Exercice 1: Construction manuelle\n\nConsidérez les données suivantes pour un problème de classification binaire:\n\nx_1\n\nx_2\n\ny\n\n1\n\n2\n\n+\n\n2\n\n1\n\n+\n\n3\n\n3\n\n-\n\n4\n\n2\n\n-\n\nCalculez l’entropie initiale du nœud racine.\n\nPour la caractéristique x_1 avec seuil t = 2.5, calculez l’entropie des deux enfants et le gain d’information.\n\nFaites de même pour x_2 avec t = 2.5.\n\nQuelle division est préférable?\n\nExercice 2: Indice de Gini\n\nUn nœud contient 60 exemples de classe A et 40 exemples de classe B.\n\nCalculez l’indice de Gini de ce nœud.\n\nSi une division produit un enfant gauche avec (50 A, 10 B) et un enfant droit avec (10 A, 30 B), calculez l’indice de Gini pondéré après division.\n\nCette division réduit-elle l’impureté? De combien?\n\nExercice 3: Régression\n\nConsidérez un arbre de régression avec les données \\{(1, 2), (2, 4), (3, 3), (4, 8)\\} où la première coordonnée est x et la seconde y.\n\nCalculez l’erreur quadratique du nœud racine (avant toute division).\n\nÉvaluez l’erreur après division à x = 2.5.\n\nÉvaluez l’erreur après division à x = 3.5.\n\nQuelle division est meilleure?\n\nExercice 4: Profondeur et complexité\n\nUn arbre binaire complet de profondeur d a combien de:\n\nFeuilles?\n\nNœuds internes?\n\nNœuds au total?\n\nSi chaque nœud interne teste une caractéristique parmi D avec un seuil parmi N valeurs, combien de paramètres faut-il stocker pour représenter l’arbre?","type":"content","url":"/trees#exercices","position":59}]}