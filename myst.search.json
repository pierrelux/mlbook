{"version":"1","records":[{"hierarchy":{"lvl1":"Introduction"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Introduction"},"content":"","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Introduction","lvl2":"Qu’est-ce qu’un modèle?"},"type":"lvl2","url":"/#quest-ce-quun-mod-le","position":2},{"hierarchy":{"lvl1":"Introduction","lvl2":"Qu’est-ce qu’un modèle?"},"content":"Le mot modèle est omniprésent en apprentissage machine, mais sa signification précise est rarement explicitée. Un modèle est une représentation simplifiée d’un phénomène — une abstraction qui capture certains aspects de la réalité tout en en ignorant d’autres. Cette idée n’est pas propre à l’informatique: les physiciens utilisent des modèles (la mécanique newtonienne ignore les effets relativistes), les économistes aussi (l’homo economicus ignore l’irrationalité humaine), et les statisticiens depuis toujours.\n\nEn apprentissage machine, un modèle est typiquement une fonction paramétrée f_\\theta: \\mathcal{X} \\to \\mathcal{Y} qui associe des entrées x à des sorties y. Les paramètres \\theta déterminent le comportement de cette fonction. Apprendre, c’est trouver les valeurs de \\theta qui font que f_\\theta se comporte de façon utile — par exemple, qu’elle prédise correctement des étiquettes sur de nouvelles données.\n\nPrenons un exemple concret. Supposons qu’on veuille prédire le prix d’une maison à partir de sa superficie. Un modèle linéaire simple serait:f_\\theta(x) = \\theta_0 + \\theta_1 x\n\noù x est la superficie, \\theta_0 est l’ordonnée à l’origine, et \\theta_1 est la pente. Ce modèle fait une hypothèse forte: la relation entre superficie et prix est linéaire. Cette hypothèse est probablement fausse — les très grandes maisons ne suivent pas la même dynamique que les petites — mais elle peut être utile si elle capture l’essentiel de la variation pour les maisons qui nous intéressent.\n\nC’est là une tension fondamentale: un modèle trop simple ignore des régularités importantes (sous-apprentissage), mais un modèle trop complexe risque de capturer du bruit plutôt que du signal (surapprentissage). Tout le livre tourne autour de cette tension.","type":"content","url":"/#quest-ce-quun-mod-le","position":3},{"hierarchy":{"lvl1":"Introduction","lvl3":"Modèle vs algorithme d’apprentissage","lvl2":"Qu’est-ce qu’un modèle?"},"type":"lvl3","url":"/#mod-le-vs-algorithme-dapprentissage","position":4},{"hierarchy":{"lvl1":"Introduction","lvl3":"Modèle vs algorithme d’apprentissage","lvl2":"Qu’est-ce qu’un modèle?"},"content":"Il est important de distinguer le modèle (la famille de fonctions \\{f_\\theta : \\theta \\in \\Theta\\}) de l’algorithme d’apprentissage (la procédure qui, étant donné des données, choisit un \\theta particulier). Le même modèle peut être entraîné par différents algorithmes; le même algorithme peut être appliqué à différents modèles.\n\nPar exemple, pour un modèle linéaire:\n\nL’algorithme des moindres carrés trouve le \\theta qui minimise la somme des erreurs au carré\n\nLa descente de gradient trouve (approximativement) le même \\theta par itérations successives\n\nL’inférence bayésienne produit une distribution sur les \\theta possibles plutôt qu’un point unique\n\nCes algorithmes ont des propriétés différentes (rapidité, stabilité, interprétabilité), mais ils opèrent tous sur le même espace de modèles.","type":"content","url":"/#mod-le-vs-algorithme-dapprentissage","position":5},{"hierarchy":{"lvl1":"Introduction","lvl2":"Pourquoi apprendre l’apprentissage machine à l’ère des LLMs?"},"type":"lvl2","url":"/#pourquoi-apprendre-lapprentissage-machine-l-re-des-llms","position":6},{"hierarchy":{"lvl1":"Introduction","lvl2":"Pourquoi apprendre l’apprentissage machine à l’ère des LLMs?"},"content":"Aujourd’hui, un assistant de programmation peut écrire un pipeline d’apprentissage machine complet en quelques secondes. Il peut charger des données, définir un modèle, l’entraîner, afficher des courbes d’apprentissage, et rapporter des métriques de performance. Pourquoi, alors, passer un semestre à étudier les fondements théoriques et algorithmiques de l’AM?\n\nLa réponse courte: parce que les LLMs sont eux-mêmes des systèmes d’apprentissage machine, et ils utilisent constamment l’AM comme sous-routine.","type":"content","url":"/#pourquoi-apprendre-lapprentissage-machine-l-re-des-llms","position":7},{"hierarchy":{"lvl1":"Introduction","lvl3":"Les LLMs sont construits sur ces fondements","lvl2":"Pourquoi apprendre l’apprentissage machine à l’ère des LLMs?"},"type":"lvl3","url":"/#les-llms-sont-construits-sur-ces-fondements","position":8},{"hierarchy":{"lvl1":"Introduction","lvl3":"Les LLMs sont construits sur ces fondements","lvl2":"Pourquoi apprendre l’apprentissage machine à l’ère des LLMs?"},"content":"Un grand modèle de langage comme GPT ou Claude est, fondamentalement:\n\nUne architecture de réseau de neurones (le transformer, couvert au chapitre sur les réseaux récurrents et l’attention)\n\nEntraînée par descente de gradient stochastique sur une fonction de perte (la cross-entropie sur la prédiction du prochain token)\n\nAvec des techniques de régularisation pour éviter le surapprentissage (dropout, weight decay)\n\nEt des considérations de généralisation pour que le modèle fonctionne sur des textes jamais vus\n\nComprendre les LLMs, c’est comprendre l’AM. Un praticien qui ne connaît pas la descente de gradient ne peut pas diagnostiquer pourquoi un fine-tuning diverge. Un praticien qui ne comprend pas le surapprentissage ne saura pas interpréter les courbes de validation. Un praticien qui ignore la notion de distribution des données ne comprendra pas les échecs de généralisation hors domaine.","type":"content","url":"/#les-llms-sont-construits-sur-ces-fondements","position":9},{"hierarchy":{"lvl1":"Introduction","lvl3":"Les LLMs utilisent l’AM comme sous-routine","lvl2":"Pourquoi apprendre l’apprentissage machine à l’ère des LLMs?"},"type":"lvl3","url":"/#les-llms-utilisent-lam-comme-sous-routine","position":10},{"hierarchy":{"lvl1":"Introduction","lvl3":"Les LLMs utilisent l’AM comme sous-routine","lvl2":"Pourquoi apprendre l’apprentissage machine à l’ère des LLMs?"},"content":"Quand vous demandez à un assistant de programmation d’analyser des données ou de construire un modèle prédictif, il génère du code qui appelle des algorithmes d’AM classiques: régression logistique, forêts aléatoires, réseaux de neurones, validation croisée. Le LLM ne fait pas l’apprentissage — il écrit du code qui le fait.\n\nEt ce code peut être faux.\n\nUn LLM peut écrire un pipeline où:\n\nLes données de test sont utilisées pour choisir les hyperparamètres (fuite d’information)\n\nLes caractéristiques incluent des variables qui ne seront pas disponibles en production (variables privilégiées)\n\nLe modèle mémorise les exemples d’entraînement plutôt que d’apprendre des régularités (surapprentissage)\n\nLa métrique optimisée ne correspond pas à l’objectif métier réel (erreur de spécification)\n\nDans tous ces cas, les métriques rapportées par le pipeline seront excellentes, mais le modèle échouera en déploiement. Savoir détecter ces erreurs requiert une compréhension des fondements.","type":"content","url":"/#les-llms-utilisent-lam-comme-sous-routine","position":11},{"hierarchy":{"lvl1":"Introduction","lvl3":"La compétence centrale: auditer un pipeline","lvl2":"Pourquoi apprendre l’apprentissage machine à l’ère des LLMs?"},"type":"lvl3","url":"/#la-comp-tence-centrale-auditer-un-pipeline","position":12},{"hierarchy":{"lvl1":"Introduction","lvl3":"La compétence centrale: auditer un pipeline","lvl2":"Pourquoi apprendre l’apprentissage machine à l’ère des LLMs?"},"content":"À l’ère où le code s’écrit facilement, la compétence rare n’est plus d’écrire du code — c’est de savoir si le code fait ce qu’il prétend faire. Un praticien compétent doit pouvoir:\n\nLire un pipeline d’AM et identifier sa structure (quel modèle? quelle perte? quel algorithme d’optimisation?)\n\nÉvaluer si le protocole expérimental est valide (les données de test sont-elles vraiment indépendantes? la métrique est-elle pertinente?)\n\nDiagnostiquer les modes d’échec (surapprentissage? sous-apprentissage? fuite d’information?)\n\nCorriger en utilisant les bons outils (régularisation, validation croisée, augmentation de données)\n\nCe livre vise à développer ces compétences. Chaque chapitre introduit des concepts qui permettent de poser des questions précises sur un système d’apprentissage: Quelle est la classe d’hypothèses? Quel est le risque que nous minimisons? Comment savons-nous que le modèle généralisera?\n\nCompétence centrale\n\nSavoir entraîner un modèle ne suffit plus. Il faut savoir inspecter, évaluer et critiquer les artefacts produits par du code — qu’il soit écrit par un humain ou généré par un LLM.","type":"content","url":"/#la-comp-tence-centrale-auditer-un-pipeline","position":13},{"hierarchy":{"lvl1":"Introduction","lvl2":"Types d’apprentissage"},"type":"lvl2","url":"/#types-dapprentissage","position":14},{"hierarchy":{"lvl1":"Introduction","lvl2":"Types d’apprentissage"},"content":"Les problèmes d’apprentissage machine se divisent en plusieurs catégories selon la nature des données disponibles et l’objectif visé.","type":"content","url":"/#types-dapprentissage","position":15},{"hierarchy":{"lvl1":"Introduction","lvl3":"Apprentissage supervisé","lvl2":"Types d’apprentissage"},"type":"lvl3","url":"/#apprentissage-supervis","position":16},{"hierarchy":{"lvl1":"Introduction","lvl3":"Apprentissage supervisé","lvl2":"Types d’apprentissage"},"content":"Dans l’apprentissage supervisé, nous disposons de paires d’entrées et de sorties: un ensemble \\{(x_1, y_1), \\ldots, (x_n, y_n)\\} où chaque x_i est une entrée et y_i est la sortie correspondante (l’étiquette ou la cible). L’objectif est d’apprendre une fonction f telle que f(x) \\approx y pour de nouvelles paires (x, y) jamais vues.\n\nSelon la nature de la sortie:\n\nClassification: y \\in \\{0, 1, \\ldots, K-1\\} (un nombre fini de classes). Exemple: déterminer si un courriel est un spam.\n\nRégression: y \\in \\mathbb{R} (une valeur continue). Exemple: prédire le prix d’une maison.","type":"content","url":"/#apprentissage-supervis","position":17},{"hierarchy":{"lvl1":"Introduction","lvl3":"Apprentissage non supervisé","lvl2":"Types d’apprentissage"},"type":"lvl3","url":"/#apprentissage-non-supervis","position":18},{"hierarchy":{"lvl1":"Introduction","lvl3":"Apprentissage non supervisé","lvl2":"Types d’apprentissage"},"content":"Dans l’apprentissage non supervisé, nous n’avons que des entrées \\{x_1, \\ldots, x_n\\} sans étiquettes associées. L’objectif est de découvrir une structure cachée dans les données:\n\nPartitionnement (clustering): regrouper les données en clusters similaires\n\nRéduction de dimensionnalité: trouver une représentation compacte des données\n\nEstimation de densité: modéliser la distribution p(x) des données","type":"content","url":"/#apprentissage-non-supervis","position":19},{"hierarchy":{"lvl1":"Introduction","lvl3":"Apprentissage par renforcement","lvl2":"Types d’apprentissage"},"type":"lvl3","url":"/#apprentissage-par-renforcement","position":20},{"hierarchy":{"lvl1":"Introduction","lvl3":"Apprentissage par renforcement","lvl2":"Types d’apprentissage"},"content":"Dans l’apprentissage par renforcement, un agent interagit avec un environnement et apprend à prendre des actions qui maximisent une récompense cumulative. Ce paradigme s’applique aux jeux (AlphaGo), à la robotique, et aux systèmes de recommandation. Ce livre ne couvre pas l’apprentissage par renforcement en détail.","type":"content","url":"/#apprentissage-par-renforcement","position":21},{"hierarchy":{"lvl1":"Introduction","lvl2":"Prérequis et ressources"},"type":"lvl2","url":"/#pr-requis-et-ressources","position":22},{"hierarchy":{"lvl1":"Introduction","lvl2":"Prérequis et ressources"},"content":"Ce livre suppose une familiarité avec:\n\nAlgèbre linéaire: vecteurs, matrices, produits, valeurs propres\n\nProbabilités: distributions, espérance, variance, théorème de Bayes\n\nCalcul différentiel: dérivées, gradients, règle de la chaîne\n\nProgrammation: Python, NumPy, matplotlib\n\nLes annexes fournissent des révisions de ce matériel.","type":"content","url":"/#pr-requis-et-ressources","position":23},{"hierarchy":{"lvl1":"Introduction","lvl3":"Ouvrages de référence","lvl2":"Prérequis et ressources"},"type":"lvl3","url":"/#ouvrages-de-r-f-rence","position":24},{"hierarchy":{"lvl1":"Introduction","lvl3":"Ouvrages de référence","lvl2":"Prérequis et ressources"},"content":"Murphy, K. Probabilistic Machine Learning: An Introduction (2022)\n\nHastie, Tibshirani, Friedman. The Elements of Statistical Learning (2009)\n\nBishop, C. Pattern Recognition and Machine Learning (2006)\n\nGoodfellow, Bengio, Courville. Deep Learning (2016)","type":"content","url":"/#ouvrages-de-r-f-rence","position":25},{"hierarchy":{"lvl1":"Introduction","lvl2":"Notation"},"type":"lvl2","url":"/#notation","position":26},{"hierarchy":{"lvl1":"Introduction","lvl2":"Notation"},"content":"Symbole\n\nSignification\n\nx, \\boldsymbol{x}\n\nScalaire, vecteur\n\n\\boldsymbol{X}\n\nMatrice\n\n\\theta, \\boldsymbol{\\theta}\n\nParamètres du modèle\n\n\\mathcal{D} = \\{(x_i, y_i)\\}\n\nEnsemble de données\n\n\\mathcal{H}\n\nClasse d’hypothèses\n\n\\ell(y, \\hat{y})\n\nFonction de perte\n\n\\mathcal{R}(f)\n\nRisque (vrai)\n\n\\hat{\\mathcal{R}}(f)\n\nRisque empirique\n\n\\mathbb{E}[\\cdot]\n\nEspérance\n\n\\mathbb{P}(\\cdot)\n\nProbabilité\n\n\\mathbb{1}_A\n\nIndicatrice de l’événement A","type":"content","url":"/#notation","position":27},{"hierarchy":{"lvl1":"K plus proches voisins"},"type":"lvl1","url":"/knn","position":0},{"hierarchy":{"lvl1":"K plus proches voisins"},"content":"Objectifs d’apprentissage\n\nÀ la fin de ce chapitre, vous serez en mesure de:\n\nExpliquer le fonctionnement de l’algorithme des k plus proches voisins\n\nDéfinir et appliquer différentes fonctions de distance\n\nAnalyser l’effet du paramètre k sur le compromis biais-variance\n\nExpliquer le fléau de la dimensionnalité et ses conséquences\n\nImplémenter l’algorithme k-ppv pour la classification et la régression","type":"content","url":"/knn","position":1},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"L’idée de base"},"type":"lvl2","url":"/knn#lid-e-de-base","position":2},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"L’idée de base"},"content":"Soit \\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N un ensemble d’entraînement avec x_i \\in \\mathbb{R}^d et y_i \\in \\{1, \\ldots, C\\}. Nous voulons prédire l’étiquette d’un nouveau point x. L’approche la plus simple consiste à regarder les exemples connus qui ressemblent à x et à prédire la même chose.\n\nLes k plus proches voisins (k-ppv) formalisent cette intuition. Pour classifier x, nous identifions les k points de \\mathcal{D} les plus proches de x et prenons un vote majoritaire sur leurs étiquettes. La méthode ne fait aucune hypothèse sur la forme de la relation entre x et y. Elle se contente de consulter les données au moment de la prédiction.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate simple 2D data\nnp.random.seed(42)\nn_per_class = 15\n\n# Class 0: cluster around (-1, -1)\nX0 = np.random.randn(n_per_class, 2) * 0.6 + np.array([-1, -1])\n# Class 1: cluster around (1, 1)\nX1 = np.random.randn(n_per_class, 2) * 0.6 + np.array([1, 1])\n\nX_train = np.vstack([X0, X1])\ny_train = np.array([0] * n_per_class + [1] * n_per_class)\n\n# Query point\nx_query = np.array([0.3, 0.2])\nk = 5\n\n# Compute distances and find k nearest\ndistances = np.sqrt(np.sum((X_train - x_query)**2, axis=1))\nk_nearest_idx = np.argsort(distances)[:k]\n\nfig, ax = plt.subplots(figsize=(6, 5))\n\n# Plot training points\nax.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], \n           c='C0', s=60, label='Classe 0', zorder=2)\nax.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], \n           c='C1', s=60, label='Classe 1', zorder=2)\n\n# Highlight k nearest neighbors\nfor idx in k_nearest_idx:\n    ax.plot([x_query[0], X_train[idx, 0]], [x_query[1], X_train[idx, 1]], \n            'k--', alpha=0.4, linewidth=1, zorder=1)\n    ax.scatter(X_train[idx, 0], X_train[idx, 1], \n               s=150, facecolors='none', edgecolors='black', linewidths=2, zorder=3)\n\n# Plot query point\nax.scatter(x_query[0], x_query[1], c='red', s=120, marker='*', \n           label='Point requête', zorder=4)\n\n# Count votes\nvotes = y_train[k_nearest_idx]\nn_class0 = np.sum(votes == 0)\nn_class1 = np.sum(votes == 1)\nprediction = 0 if n_class0 > n_class1 else 1\n\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.legend(loc='upper left')\nax.set_title(f'$k = {k}$: votes = [{n_class0} classe 0, {n_class1} classe 1] → prédiction: classe {prediction}')\nax.set_aspect('equal')\nplt.tight_layout()\n\n\n\nSoit \\mathcal{N}_k(x) l’ensemble des indices des k plus proches voisins de x. La prédiction est:\\hat{y} = \\arg\\max_{c} \\sum_{i \\in \\mathcal{N}_k(x)} \\mathbb{1}_{y_i = c}\n\nLa somme compte combien de voisins appartiennent à chaque classe c, et nous retenons la classe la plus fréquente.\n\nCette formulation admet une interprétation probabiliste. La proportion de voisins appartenant à la classe c estime la probabilité conditionnelle:p(y = c \\mid x, \\mathcal{D}) = \\frac{1}{k} \\sum_{i \\in \\mathcal{N}_k(x)} \\mathbb{1}_{y_i = c}\n\nLa prédiction déterministe correspond au mode de cette distribution empirique.","type":"content","url":"/knn#lid-e-de-base","position":3},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Fonctions de distance"},"type":"lvl2","url":"/knn#fonctions-de-distance","position":4},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Fonctions de distance"},"content":"L’algorithme repose sur la capacité à mesurer la proximité entre points. Une fonction de distance d: \\mathcal{X} \\times \\mathcal{X} \\to [0, \\infty) doit satisfaire trois axiomes: d(x, y) = 0 si et seulement si x = y (identité), d(x, y) = d(y, x) (symétrie), et d(x, z) \\leq d(x, y) + d(y, z) (inégalité triangulaire).\n\nLa distance euclidienne est le choix le plus courant:d_2(x, y) = \\sqrt{\\sum_{j=1}^{d} (x_j - y_j)^2} = \\|x - y\\|_2\n\nLa distance de Manhattan suit les axes plutôt que la ligne droite:d_1(x, y) = \\sum_{j=1}^{d} |x_j - y_j| = \\|x - y\\|_1\n\nCes deux distances appartiennent à la famille des normes \\ell_p, définies par \\|x\\|_p = \\left(\\sum_j |x_j|^p\\right)^{1/p}. Le cas limite p \\to \\infty donne la norme \\ell_\\infty:\\|x\\|_\\infty = \\max_j |x_j|","type":"content","url":"/knn#fonctions-de-distance","position":5},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Boules unité: visualiser ce que “équidistant” signifie","lvl2":"Fonctions de distance"},"type":"lvl3","url":"/knn#boules-unit-visualiser-ce-que-quidistant-signifie","position":6},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Boules unité: visualiser ce que “équidistant” signifie","lvl2":"Fonctions de distance"},"content":"Pour comprendre comment une norme mesure les distances, on trace sa boule unité. Formellement, la boule unité d’une norme \\|\\cdot\\| est l’ensemble:B = \\{x \\in \\mathbb{R}^d : \\|x\\| \\leq 1\\}\n\net sa frontière, la sphère unité, est S = \\{x : \\|x\\| = 1\\}. Tous les points sur cette sphère sont à distance exactement 1 de l’origine. La forme de la boule révèle ce que la norme considère comme “équidistant”.\n\nNorme \\ell_2 (cercle): Le point (1, 0) et le point (0.71, 0.71) sont à la même distance de l’origine. Se déplacer en diagonale coûte autant que suivre un axe. C’est notre intuition géométrique habituelle.\n\nNorme \\ell_1 (losange): Le point (1, 0) est à distance 1, mais (0.71, 0.71) est à distance 0.71 + 0.71 = 1.42. Se déplacer en diagonale coûte plus cher, comme un taxi qui ne peut tourner qu’aux intersections.\n\nNorme \\ell_\\infty (carré): Seule la plus grande coordonnée compte. Les points (1, 0), (1, 0.5) et (1, 1) sont tous à distance 1. C’est la distance du joueur d’échecs (le roi peut se déplacer d’une case dans n’importe quelle direction).\n\nPour les k-ppv, la forme de la boule détermine quels points sont considérés voisins. Avec \\ell_1, les voisins forment un losange autour de la requête; avec \\ell_\\infty, un carré.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 3, figsize=(11, 4))\n\n# Unit balls for different norms\ntheta = np.linspace(0, 2*np.pi, 1000)\n\n# Key points to highlight\np1 = (1, 0)\np2 = (1/np.sqrt(2), 1/np.sqrt(2))  # ≈ (0.71, 0.71)\n\n# L1 norm (diamond)\nax = axes[0]\nt = np.linspace(0, 1, 250)\nx_l1 = np.concatenate([t, 1-t, -t, -1+t])\ny_l1 = np.concatenate([1-t, -t, -1+t, t])\nax.fill(x_l1, y_l1, alpha=0.3, color='C0')\nax.plot(x_l1, y_l1, 'C0-', linewidth=2)\n\n# Show points - (1,0) is on boundary, (0.71, 0.71) is OUTSIDE\nax.scatter([p1[0]], [p1[1]], s=80, c='black', zorder=5)\nax.scatter([p2[0]], [p2[1]], s=80, c='red', zorder=5)\nax.annotate(f'$(1, 0)$\\n$d=1$', p1, textcoords='offset points', \n            xytext=(5, 10), fontsize=9)\nax.annotate(f'$(0.71, 0.71)$\\n$d=1.42$', p2, textcoords='offset points', \n            xytext=(5, 10), fontsize=9, color='red')\n\nax.set_title(r'Norme $\\ell_1$ (Manhattan)')\nax.set_xlim(-1.5, 1.5)\nax.set_ylim(-1.5, 1.5)\nax.set_aspect('equal')\nax.axhline(0, color='gray', linewidth=0.5)\nax.axvline(0, color='gray', linewidth=0.5)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\n\n# L2 norm (circle)\nax = axes[1]\nx_l2 = np.cos(theta)\ny_l2 = np.sin(theta)\nax.fill(x_l2, y_l2, alpha=0.3, color='C1')\nax.plot(x_l2, y_l2, 'C1-', linewidth=2)\n\n# Both points are on the boundary for L2\nax.scatter([p1[0]], [p1[1]], s=80, c='black', zorder=5)\nax.scatter([p2[0]], [p2[1]], s=80, c='black', zorder=5)\nax.annotate(f'$(1, 0)$\\n$d=1$', p1, textcoords='offset points', \n            xytext=(5, 10), fontsize=9)\nax.annotate(f'$(0.71, 0.71)$\\n$d=1$', p2, textcoords='offset points', \n            xytext=(5, 10), fontsize=9)\n\nax.set_title(r'Norme $\\ell_2$ (Euclidienne)')\nax.set_xlim(-1.5, 1.5)\nax.set_ylim(-1.5, 1.5)\nax.set_aspect('equal')\nax.axhline(0, color='gray', linewidth=0.5)\nax.axvline(0, color='gray', linewidth=0.5)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\n\n# L-infinity norm (square)\nax = axes[2]\nx_linf = np.array([1, 1, -1, -1, 1])\ny_linf = np.array([1, -1, -1, 1, 1])\nax.fill(x_linf, y_linf, alpha=0.3, color='C2')\nax.plot(x_linf, y_linf, 'C2-', linewidth=2)\n\n# For L-inf: (1,0), (1,0.5), (1,1) all have distance 1\np_inf = [(1, 0), (1, 0.5), (1, 1)]\nfor i, p in enumerate(p_inf):\n    ax.scatter([p[0]], [p[1]], s=80, c='black', zorder=5)\nax.annotate('$(1, 0)$\\n$d=1$', p_inf[0], textcoords='offset points', \n            xytext=(-45, -5), fontsize=9)\nax.annotate('$(1, 0.5)$\\n$d=1$', p_inf[1], textcoords='offset points', \n            xytext=(5, -5), fontsize=9)\nax.annotate('$(1, 1)$\\n$d=1$', p_inf[2], textcoords='offset points', \n            xytext=(5, 5), fontsize=9)\n\nax.set_title(r'Norme $\\ell_\\infty$')\nax.set_xlim(-1.5, 1.5)\nax.set_ylim(-1.5, 1.5)\nax.set_aspect('equal')\nax.axhline(0, color='gray', linewidth=0.5)\nax.axvline(0, color='gray', linewidth=0.5)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\n\nplt.tight_layout()\n\n\n\n","type":"content","url":"/knn#boules-unit-visualiser-ce-que-quidistant-signifie","position":7},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Données numériques: normalisation et corrélation","lvl2":"Fonctions de distance"},"type":"lvl3","url":"/knn#donn-es-num-riques-normalisation-et-corr-lation","position":8},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Données numériques: normalisation et corrélation","lvl2":"Fonctions de distance"},"content":"La distance euclidienne traite toutes les dimensions de manière égale. Si les variables ont des échelles différentes, certaines domineront le calcul. Imaginons un problème où x_1 est l’âge (0-100) et x_2 est le revenu annuel (0-500000). Sans normalisation, la différence de revenu écrasera la différence d’âge.\n\nSolution pratique: normaliser les variables (soustraire la moyenne, diviser par l’écart-type) avant d’appliquer les k-ppv. C’est presque toujours nécessaire pour des données tabulaires.\n\nLa distance de Mahalanobis va plus loin en tenant compte des corrélations:d_M(x, y) = \\sqrt{(x - y)^\\top \\Sigma^{-1} (x - y)}\n\noù \\Sigma est la matrice de covariance des données. Pour comprendre cette formule, décomposons-la.\n\nLa matrice de covariance \\Sigma. Cette matrice d \\times d capture deux informations: sur la diagonale, les variances de chaque variable; hors diagonale, les covariances (corrélations) entre variables. Si \\Sigma = \\begin{pmatrix} 4 & 0 \\\\ 0 & 1 \\end{pmatrix}, la première variable a une variance 4 fois plus grande que la seconde, et elles sont indépendantes.\n\nCalcul concret. Soit X \\in \\mathbb{R}^{N \\times d} la matrice des données (chaque ligne est un exemple). On centre d’abord les données en soustrayant la moyenne de chaque colonne:\\bar{x}_j = \\frac{1}{N} \\sum_{i=1}^{N} x_{ij}, \\quad \\tilde{X} = X - \\mathbf{1} \\bar{x}^\\top\n\nLa covariance empirique est alors:\\Sigma = \\frac{1}{N-1} \\tilde{X}^\\top \\tilde{X}\n\nL’élément (j, k) de cette matrice est \\Sigma_{jk} = \\frac{1}{N-1} \\sum_{i=1}^{N} (x_{ij} - \\bar{x}_j)(x_{ik} - \\bar{x}_k). En Python:X_centered = X - X.mean(axis=0)\nSigma = (X_centered.T @ X_centered) / (len(X) - 1)\n# ou directement: Sigma = np.cov(X.T)\n\nL’inverse \\Sigma^{-1}. Multiplier par l’inverse de la covariance “blanchit” les données: les directions de forte variance sont comprimées, les corrélations sont supprimées. Après cette transformation, les données ressemblent à un nuage sphérique de variance unitaire.\n\nInterprétation géométrique. La distance de Mahalanobis mesure “à combien d’écarts-types” un point se trouve d’un autre, en tenant compte de la forme du nuage de données. Deux points éloignés dans une direction de forte variance sont considérés plus proches que deux points également éloignés dans une direction de faible variance.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Ellipse\n\nnp.random.seed(42)\n\n# Generate correlated 2D data\nn = 200\nmean = [0, 0]\ncov = [[2, 1.5], [1.5, 1.5]]  # Correlated, different variances\nX = np.random.multivariate_normal(mean, cov, n)\n\n# Compute sample covariance\nSigma = np.cov(X.T)\nSigma_inv = np.linalg.inv(Sigma)\n\n# A reference point and a query point\nref = np.array([0, 0])\nquery1 = np.array([2, 0])    # Along high-variance direction\nquery2 = np.array([-0.5, 1]) # Along low-variance direction\n\n# Compute distances\ndef euclidean(a, b):\n    return np.sqrt(np.sum((a - b)**2))\n\ndef mahalanobis(a, b, Sigma_inv):\n    diff = a - b\n    return np.sqrt(diff @ Sigma_inv @ diff)\n\nd_euc1 = euclidean(ref, query1)\nd_euc2 = euclidean(ref, query2)\nd_mah1 = mahalanobis(ref, query1, Sigma_inv)\nd_mah2 = mahalanobis(ref, query2, Sigma_inv)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Left: Euclidean view\nax = axes[0]\nax.scatter(X[:, 0], X[:, 1], alpha=0.3, s=20, c='gray')\nax.scatter([ref[0]], [ref[1]], s=100, c='black', zorder=5, label='Référence')\nax.scatter([query1[0]], [query1[1]], s=100, c='C0', zorder=5, marker='s')\nax.scatter([query2[0]], [query2[1]], s=100, c='C1', zorder=5, marker='^')\n\n# Draw circles for Euclidean distance\ncircle1 = plt.Circle(ref, d_euc1, fill=False, color='C0', linestyle='--', linewidth=2)\ncircle2 = plt.Circle(ref, d_euc2, fill=False, color='C1', linestyle='--', linewidth=2)\nax.add_patch(circle1)\nax.add_patch(circle2)\n\nax.annotate(f'$d_{{euc}} = {d_euc1:.2f}$', query1, textcoords='offset points', \n            xytext=(10, 10), fontsize=10, color='C0')\nax.annotate(f'$d_{{euc}} = {d_euc2:.2f}$', query2, textcoords='offset points', \n            xytext=(10, 10), fontsize=10, color='C1')\n\nax.set_xlim(-4, 4)\nax.set_ylim(-4, 4)\nax.set_aspect('equal')\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_title('Distance Euclidienne\\n(ignore la structure des données)')\n\n# Right: Mahalanobis view\nax = axes[1]\nax.scatter(X[:, 0], X[:, 1], alpha=0.3, s=20, c='gray')\nax.scatter([ref[0]], [ref[1]], s=100, c='black', zorder=5, label='Référence')\nax.scatter([query1[0]], [query1[1]], s=100, c='C0', zorder=5, marker='s')\nax.scatter([query2[0]], [query2[1]], s=100, c='C1', zorder=5, marker='^')\n\n# Draw ellipses for Mahalanobis distance (iso-distance contours)\n# Eigendecomposition for ellipse orientation\neigenvalues, eigenvectors = np.linalg.eigh(Sigma)\nangle = np.degrees(np.arctan2(eigenvectors[1, 1], eigenvectors[0, 1]))\n\nfor d_mah, color in [(d_mah1, 'C0'), (d_mah2, 'C1')]:\n    width = 2 * d_mah * np.sqrt(eigenvalues[1])\n    height = 2 * d_mah * np.sqrt(eigenvalues[0])\n    ellipse = Ellipse(ref, width, height, angle=angle, fill=False, \n                      color=color, linestyle='--', linewidth=2)\n    ax.add_patch(ellipse)\n\nax.annotate(f'$d_{{mah}} = {d_mah1:.2f}$', query1, textcoords='offset points', \n            xytext=(10, 10), fontsize=10, color='C0')\nax.annotate(f'$d_{{mah}} = {d_mah2:.2f}$', query2, textcoords='offset points', \n            xytext=(10, 10), fontsize=10, color='C1')\n\nax.set_xlim(-4, 4)\nax.set_ylim(-4, 4)\nax.set_aspect('equal')\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_title('Distance de Mahalanobis\\n(tient compte de la covariance)')\n\nplt.tight_layout()\n\n\n\nDans cet exemple, le point bleu (carré) est plus loin en distance euclidienne, mais plus proche en distance de Mahalanobis. Cela s’explique par le fait qu’il se trouve dans la direction où les données varient naturellement. Le point orange (triangle), bien que plus proche en euclidien, est “surprenant” par rapport à la distribution et donc plus loin en Mahalanobis.\n\nEn pratique, on utilise rarement Mahalanobis directement pour les k-ppv. La normalisation standard (centrer et réduire chaque variable) capture l’essentiel. Mahalanobis devient utile quand les corrélations entre variables sont fortes et informatives.","type":"content","url":"/knn#donn-es-num-riques-normalisation-et-corr-lation","position":9},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Au-delà des vecteurs numériques","lvl2":"Fonctions de distance"},"type":"lvl3","url":"/knn#au-del-des-vecteurs-num-riques","position":10},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Au-delà des vecteurs numériques","lvl2":"Fonctions de distance"},"content":"Les k-ppv ne se limitent pas aux vecteurs dans \\mathbb{R}^d. Toute fonction de distance valide permet d’appliquer l’algorithme.\n\nChaînes de caractères et ADN. La distance d’édition (ou distance de Levenshtein) compte le nombre minimum d’opérations (insertion, suppression, substitution) pour transformer une chaîne en une autre:d_{\\text{edit}}(\\texttt{\"chat\"}, \\texttt{\"chien\"}) = 3\n\nCette distance est utilisée pour la correction orthographique, l’alignement de séquences ADN, et la détection de plagiat. Pour comparer des séquences génétiques, on peut aussi utiliser des distances spécialisées qui tiennent compte de la biologie des mutations.\n\nDocuments et texte. Pour comparer des documents, on les représente souvent comme des vecteurs de fréquences de mots (bag-of-words). La similarité cosinus mesure l’angle entre ces vecteurs:\\text{sim}_{\\cos}(x, y) = \\frac{x \\cdot y}{\\|x\\| \\|y\\|}\n\nOn convertit en distance par d = 1 - \\text{sim}_{\\cos}. Cette mesure ignore la longueur des documents et se concentre sur leur contenu thématique. C’est le choix standard pour la recherche d’information et la classification de texte.\n\nEnsembles et données binaires. Pour des données représentées comme des ensembles (mots-clés, tags, gènes exprimés), la distance de Jaccard mesure le chevauchement:d_{\\text{Jaccard}}(A, B) = 1 - \\frac{|A \\cap B|}{|A \\cup B|}\n\nDeux documents partageant 80% de leurs mots-clés ont une distance de 0.2. Pour des vecteurs binaires (présence/absence), la distance de Hamming compte les positions différentes.\n\nImages. Les pixels bruts donnent des distances peu informatives. Par exemple, deux images du même objet décalé d’un pixel seraient très “différentes”. En pratique, on extrait des représentations (embeddings) via des réseaux de neurones pré-entraînés, puis on applique la distance euclidienne ou cosinus dans cet espace de représentation.","type":"content","url":"/knn#au-del-des-vecteurs-num-riques","position":11},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Guide pratique: quelle distance choisir?","lvl2":"Fonctions de distance"},"type":"lvl3","url":"/knn#guide-pratique-quelle-distance-choisir","position":12},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Guide pratique: quelle distance choisir?","lvl2":"Fonctions de distance"},"content":"Type de données\n\nDistance recommandée\n\nPourquoi\n\nVecteurs numériques\n\nEuclidienne (après normalisation)\n\nSimple, efficace, interprétable\n\nDonnées avec corrélations fortes\n\nMahalanobis ou PCA + Euclidienne\n\nTient compte de la structure\n\nTexte / documents\n\nCosinus sur TF-IDF ou embeddings\n\nInvariant à la longueur\n\nSéquences (ADN, protéines)\n\nDistance d’édition ou alignement\n\nCapture les mutations/insertions\n\nEnsembles, tags\n\nJaccard\n\nMesure le chevauchement\n\nVecteurs binaires\n\nHamming\n\nCompte les différences\n\nImages\n\nCosinus sur embeddings CNN\n\nLes pixels bruts sont peu informatifs\n\nLe choix de la distance encode vos hypothèses. Si deux clients ayant acheté les mêmes produits sont “similaires”, utilisez Jaccard sur les paniers. Si deux clients ayant dépensé des montants similaires sont “proches”, utilisez la distance euclidienne sur les dépenses. La distance définit ce que “voisin” signifie: c’est une décision de modélisation, pas un détail technique.","type":"content","url":"/knn#guide-pratique-quelle-distance-choisir","position":13},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"L’effet du paramètre k"},"type":"lvl2","url":"/knn#leffet-du-param-tre-k","position":14},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"L’effet du paramètre k"},"content":"Le paramètre k contrôle la complexité du modèle. Avec k = 1, chaque point est classifié selon son plus proche voisin. La frontière de décision est très irrégulière et s’adapte étroitement aux données. L’erreur d’entraînement est exactement zéro: chaque point est son propre plus proche voisin. Mais cette adaptation excessive aux données d’entraînement nuit à la généralisation.\n\nAvec un grand k, la prédiction moyenne sur plus de voisins et la frontière devient plus lisse. Le cas extrême k = N prédit toujours la classe majoritaire globale, ignorant complètement l’entrée.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n# Generate 2D classification data\nnp.random.seed(0)\nn_samples = 100\n\n# Two interleaved half circles\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=n_samples, noise=0.25, random_state=42)\n\ndef knn_predict(X_train, y_train, X_test, k):\n    predictions = []\n    for x in X_test:\n        distances = np.sqrt(np.sum((X_train - x)**2, axis=1))\n        k_nearest_idx = np.argsort(distances)[:k]\n        k_nearest_labels = y_train[k_nearest_idx]\n        predictions.append(np.round(np.mean(k_nearest_labels)))\n    return np.array(predictions)\n\n# Create mesh for decision boundary\nx_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\ny_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 150),\n                     np.linspace(y_min, y_max, 150))\nX_mesh = np.c_[xx.ravel(), yy.ravel()]\n\nfig, axes = plt.subplots(1, 3, figsize=(12, 4))\nk_values = [1, 5, 50]\ncmap_light = ListedColormap(['#FFAAAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#0000FF'])\n\nfor ax, k in zip(axes, k_values):\n    Z = knn_predict(X, y, X_mesh, k)\n    Z = Z.reshape(xx.shape)\n    \n    ax.contourf(xx, yy, Z, alpha=0.4, cmap=cmap_light)\n    ax.scatter(X[y == 0, 0], X[y == 0, 1], c='C0', s=30, edgecolors='k', linewidths=0.5)\n    ax.scatter(X[y == 1, 0], X[y == 1, 1], c='C1', s=30, edgecolors='k', linewidths=0.5)\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_title(f'$k = {k}$')\n    ax.set_xlabel('$x_1$')\n    ax.set_ylabel('$x_2$')\n\nplt.tight_layout()\n\n\n\nEntre ces deux extrêmes se trouve le compromis biais-variance. Un petit k donne un modèle à faible biais mais haute variance: les prédictions sont sensibles aux fluctuations des données. Un grand k donne un modèle à haute biais mais faible variance: les prédictions sont stables mais peuvent manquer des structures locales.\n\nLe choix de k se fait par validation. On trace l’erreur sur un ensemble de validation en fonction de k et on retient la valeur qui minimise cette erreur. Des valeurs impaires évitent les égalités dans les votes.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\n# Generate a larger dataset for smoother curves\nn_samples = 500\nX = np.random.randn(n_samples, 2)\n# Create a non-linear boundary\ny = ((X[:, 0]**2 + X[:, 1]**2) > 1.5).astype(int)\n# Add label noise\nnoise_idx = np.random.choice(n_samples, size=int(0.1 * n_samples), replace=False)\ny[noise_idx] = 1 - y[noise_idx]\n\n# Split into train and test\nn_train = 350\nX_train, X_test = X[:n_train], X[n_train:]\ny_train, y_test = y[:n_train], y[n_train:]\n\ndef knn_predict(X_train, y_train, X_test, k):\n    predictions = []\n    for x in X_test:\n        distances = np.sqrt(np.sum((X_train - x)**2, axis=1))\n        k_nearest_idx = np.argsort(distances)[:k]\n        k_nearest_labels = y_train[k_nearest_idx]\n        predictions.append(np.round(np.mean(k_nearest_labels)))\n    return np.array(predictions)\n\n# Compute train and test error for different k\n# Use 1/k on x-axis (model complexity decreases as k increases)\nk_values = list(range(1, 100, 2))  # Odd values to avoid ties\ntrain_errors = []\ntest_errors = []\n\nfor k in k_values:\n    y_pred_train = knn_predict(X_train, y_train, X_train, k)\n    y_pred_test = knn_predict(X_train, y_train, X_test, k)\n    train_errors.append(np.mean(y_pred_train != y_train))\n    test_errors.append(np.mean(y_pred_test != y_test))\n\nfig, ax = plt.subplots(figsize=(8, 4.5))\n\nax.plot(k_values, train_errors, 'C0-', linewidth=2, label='Erreur entraînement')\nax.plot(k_values, test_errors, 'C1-', linewidth=2, label='Erreur test')\n\n# Mark optimal k\nbest_idx = np.argmin(test_errors)\nbest_k = k_values[best_idx]\nax.scatter([best_k], [test_errors[best_idx]], s=100, c='C1', zorder=5, edgecolors='black')\nax.axvline(best_k, color='gray', linestyle='--', alpha=0.5)\n\n# Add annotations for the regions - positioned to avoid overlapping curves\nax.text(10, 0.45, 'Surapprentissage\\n(haute variance)', fontsize=10, \n        ha='center', va='top', color='gray', alpha=0.8)\nax.text(85, 0.1, 'Sous-apprentissage\\n(haut biais)', fontsize=10, \n        ha='center', va='bottom', color='gray', alpha=0.8)\n\n# Mark optimal k with an arrow from above\nax.annotate(f'Meilleur $k = {best_k}$', xy=(best_k, test_errors[best_idx]), \n            xytext=(best_k, test_errors[best_idx] + 0.1),\n            fontsize=10, ha='center',\n            arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0', color='black'))\n\nax.set_xlabel('$k$ (nombre de voisins)')\nax.set_ylabel('Taux d\\'erreur')\nax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2, frameon=False)\nax.set_xlim(0, 100)\nax.set_ylim(0, 0.5)\n\n# Add complexity arrow at the bottom\nax.text(0.5, -0.15, r'$\\longleftarrow$ complexité du modèle', transform=ax.transAxes, \n        ha='center', va='top', fontsize=9, color='gray')\n\nplt.tight_layout()\n\n\n\n","type":"content","url":"/knn#leffet-du-param-tre-k","position":15},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Diagramme de Voronoï"},"type":"lvl2","url":"/knn#diagramme-de-vorono","position":16},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Diagramme de Voronoï"},"content":"Le cas k = 1 induit une partition de l’espace en cellules. La cellule V_i associée au point x_i contient tous les points plus proches de x_i que de tout autre point d’entraînement:V_i = \\{x \\in \\mathbb{R}^d : d(x, x_i) \\leq d(x, x_j) \\text{ pour tout } j \\neq i\\}\n\nCette partition s’appelle le diagramme de Voronoï. Les frontières entre cellules sont des hyperplans en dimension d. Avec le 1-ppv, la frontière de décision suit exactement ce diagramme.","type":"content","url":"/knn#diagramme-de-vorono","position":17},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Pourquoi s’intéresser au diagramme de Voronoï?","lvl2":"Diagramme de Voronoï"},"type":"lvl3","url":"/knn#pourquoi-sint-resser-au-diagramme-de-vorono","position":18},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Pourquoi s’intéresser au diagramme de Voronoï?","lvl2":"Diagramme de Voronoï"},"content":"Cette représentation géométrique révèle des propriétés importantes du classifieur 1-ppv.\n\nSensibilité au bruit. Un point mal étiqueté dans l’ensemble d’entraînement crée une cellule entière où les prédictions seront incorrectes. La taille de cette cellule dépend de la densité locale des données: dans une région peu dense, un seul point erroné peut contaminer une grande partie de l’espace.\n\nBorne sur l’erreur. Le théorème de Cover et Hart (1967) établit un résultat remarquable: lorsque N \\to \\infty, l’erreur du classifieur 1-ppv est bornée parR^* \\leq R_{1\\text{-ppv}} \\leq 2R^*\\left(1 - \\frac{R^*}{C}\\right)\n\noù R^* est l’erreur de Bayes (le minimum atteignable par tout classifieur) et C le nombre de classes. Pour un problème binaire, l’erreur du 1-ppv est au plus le double de l’erreur optimale. Sans aucune hypothèse sur la forme de la frontière de décision, le 1-ppv atteint une performance raisonnable. Ce résultat a d’ailleurs surpris la communauté statistique à l’époque.\n\nApplications au-delà de la classification. Les diagrammes de Voronoï apparaissent naturellement dans de nombreux domaines: modélisation des zones de chalandise en géographie (quel est le magasin le plus proche?), segmentation cellulaire en biologie, génération de maillages en simulation numérique. L’algorithme k-means, que nous verrons plus tard, converge vers une partition de Voronoï autour des centroïdes.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import Voronoi, voronoi_plot_2d\n\n# Generate points\nnp.random.seed(123)\nn_points = 20\nX = np.random.randn(n_points, 2) * 1.5\ny = (X[:, 0] + X[:, 1] + np.random.randn(n_points) * 0.5 > 0).astype(int)\n\n# Compute Voronoi diagram\nvor = Voronoi(X)\n\nfig, ax = plt.subplots(figsize=(7, 6))\n\n# Plot Voronoi regions with colors based on class\nfrom matplotlib.patches import Polygon\nfrom matplotlib.collections import PatchCollection\n\n# Create colored regions\nfor region_idx, point_idx in enumerate(vor.point_region):\n    region = vor.regions[point_idx]\n    if -1 not in region and len(region) > 0:\n        polygon = [vor.vertices[i] for i in region]\n        poly = Polygon(polygon, alpha=0.3, \n                      facecolor='C0' if y[region_idx] == 0 else 'C1',\n                      edgecolor='gray', linewidth=0.5)\n        ax.add_patch(poly)\n\n# Plot Voronoi edges\nvoronoi_plot_2d(vor, ax=ax, show_vertices=False, show_points=False, \n                line_colors='gray', line_width=1, line_alpha=0.6)\n\n# Plot points\nax.scatter(X[y == 0, 0], X[y == 0, 1], c='C0', s=80, edgecolors='k', \n           linewidths=1, zorder=5, label='Classe 0')\nax.scatter(X[y == 1, 0], X[y == 1, 1], c='C1', s=80, edgecolors='k', \n           linewidths=1, zorder=5, label='Classe 1')\n\nax.set_xlim(-4, 4)\nax.set_ylim(-4, 4)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.legend(loc='upper left')\nax.set_aspect('equal')\nplt.tight_layout()\n\n\n\n","type":"content","url":"/knn#pourquoi-sint-resser-au-diagramme-de-vorono","position":19},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Le fléau de la dimensionnalité"},"type":"lvl2","url":"/knn#le-fl-au-de-la-dimensionnalit","position":20},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Le fléau de la dimensionnalité"},"content":"Les k-ppv fonctionnent bien en basse dimension mais souffrent en haute dimension. Ce phénomène, le fléau de la dimensionnalité, affecte toutes les méthodes basées sur la localité. Le problème n’est pas principalement calculatoire: le coût O(Nd) croît linéairement en d, mais il est fondamentalement statistique.","type":"content","url":"/knn#le-fl-au-de-la-dimensionnalit","position":21},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"La notion de voisinage s’effondre","lvl2":"Le fléau de la dimensionnalité"},"type":"lvl3","url":"/knn#la-notion-de-voisinage-seffondre","position":22},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"La notion de voisinage s’effondre","lvl2":"Le fléau de la dimensionnalité"},"content":"Considérons des points uniformément distribués dans [0, 1]^d. Pour capturer une fraction p des points dans un hypercube, le côté doit être r = p^{1/d}. En dimension 1, capturer 10% des points requiert un intervalle de longueur 0.1. En dimension 100, il faut un hypercube de côté 0.1^{1/100} \\approx 0.98, couvrant presque tout l’espace.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\np = 0.1  # fraction of points to capture\ndimensions = np.arange(1, 101)\nside_length = p ** (1 / dimensions)\n\nfig, ax = plt.subplots(figsize=(7, 4))\nax.plot(dimensions, side_length, 'C0-', linewidth=2)\nax.axhline(1, color='gray', linestyle='--', alpha=0.5)\n\n# Mark specific points\nfor d in [1, 2, 10, 50, 100]:\n    r = p ** (1/d)\n    ax.plot(d, r, 'ko', markersize=6)\n    if d == 1:\n        ax.annotate(f'd={d}\\nr={r:.2f}', (d, r), textcoords='offset points', \n                   xytext=(10, -15), fontsize=9)\n    elif d == 100:\n        ax.annotate(f'd={d}\\nr={r:.2f}', (d, r), textcoords='offset points', \n                   xytext=(-40, -20), fontsize=9)\n\nax.set_xlabel('Dimension $d$')\nax.set_ylabel('Côté de l\\'hypercube $r$')\nax.set_title(f'Côté nécessaire pour capturer {int(p*100)}% des points: $r = p^{{1/d}}$')\nax.set_xlim(0, 105)\nax.set_ylim(0, 1.1)\nplt.tight_layout()\n\n\n\n","type":"content","url":"/knn#la-notion-de-voisinage-seffondre","position":23},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Concentration des distances","lvl2":"Le fléau de la dimensionnalité"},"type":"lvl3","url":"/knn#concentration-des-distances","position":24},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Concentration des distances","lvl2":"Le fléau de la dimensionnalité"},"content":"En haute dimension, un phénomène contre-intuitif se produit: les distances entre points se concentrent autour d’une même valeur. Le ratio entre la distance au plus proche voisin et au plus éloigné tend vers 1:\\frac{d_{\\min}}{d_{\\max}} \\xrightarrow{d \\to \\infty} 1\n\nTous les points deviennent approximativement équidistants. La notion même de “plus proche voisin” perd son sens: si tous les points sont à la même distance, lequel choisir?","type":"content","url":"/knn#concentration-des-distances","position":25},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Conséquences sur le biais","lvl2":"Le fléau de la dimensionnalité"},"type":"lvl3","url":"/knn#cons-quences-sur-le-biais","position":26},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Conséquences sur le biais","lvl2":"Le fléau de la dimensionnalité"},"content":"Rappelons que les k-ppv estiment \\mathbb{E}[Y \\mid X = x] par une moyenne locale. En haute dimension, cette moyenne n’est plus locale car elle inclut des points qui, bien que “voisins” au sens de la distance, peuvent être très différents de x dans l’espace d’entrée. Le biais augmente: nous moyennons sur des régions trop vastes pour capturer les variations locales de la fonction cible.\n\nPour maintenir une densité constante de voisins dans une boule de rayon fixe, le nombre d’exemples requis croît exponentiellement avec la dimension: N \\propto r^{-d}. Avec 1000 points en dimension 10, la densité locale est déjà très faible.","type":"content","url":"/knn#cons-quences-sur-le-biais","position":27},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Que faire?","lvl2":"Le fléau de la dimensionnalité"},"type":"lvl3","url":"/knn#que-faire","position":28},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Que faire?","lvl2":"Le fléau de la dimensionnalité"},"content":"La réduction de dimension (PCA, autoencodeurs) projette les données dans un espace de plus basse dimension avant d’appliquer les k-ppv. Les distances adaptatives comme Mahalanobis peuvent aider si certaines dimensions sont plus informatives. Mais fondamentalement, les méthodes de voisinage ne sont pas adaptées aux problèmes en très haute dimension, ce qui motive l’étude des méthodes paramétriques dans les chapitres suivants.","type":"content","url":"/knn#que-faire","position":29},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Régression"},"type":"lvl2","url":"/knn#r-gression","position":30},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Régression"},"content":"En classification, nous avons combiné les étiquettes des voisins par vote majoritaire. Pour la régression, où y_i \\in \\mathbb{R}, la combinaison naturelle est une moyenne:\\hat{y} = \\frac{1}{k} \\sum_{i \\in \\mathcal{N}_k(x)} y_i\n\nCette moyenne locale estime l’espérance conditionnelle \\mathbb{E}[Y \\mid X = x]. L’intuition est simple: si nous voulons prédire la température demain et que nous avons des données historiques, regarder les jours passés qui ressemblaient à aujourd’hui et moyenner leurs températures du lendemain semble raisonnable.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate 1D regression data\nnp.random.seed(42)\nn = 50\nX_train = np.sort(np.random.uniform(0, 10, n))\ny_train = np.sin(X_train) + np.random.randn(n) * 0.3\n\ndef knn_regression(X_train, y_train, X_test, k):\n    predictions = []\n    for x in X_test:\n        distances = np.abs(X_train - x)\n        k_nearest_idx = np.argsort(distances)[:k]\n        predictions.append(np.mean(y_train[k_nearest_idx]))\n    return np.array(predictions)\n\nX_test = np.linspace(0, 10, 200)\n\nfig, axes = plt.subplots(1, 3, figsize=(12, 3.5))\nk_values = [1, 5, 15]\n\nfor ax, k in zip(axes, k_values):\n    y_pred = knn_regression(X_train, y_train, X_test, k)\n    \n    ax.scatter(X_train, y_train, c='C0', s=30, alpha=0.6, label='Données')\n    ax.plot(X_test, y_pred, 'C1-', linewidth=2, label=f'k-ppv ($k={k}$)')\n    ax.plot(X_test, np.sin(X_test), 'k--', alpha=0.5, label=r'$\\sin(x)$')\n    ax.set_xlabel('$x$')\n    ax.set_ylabel('$y$')\n    ax.set_title(f'$k = {k}$')\n    ax.legend(loc='upper right', fontsize=8)\n    ax.set_xlim(0, 10)\n\nplt.tight_layout()\n\n\n\nAvec k = 1, la prédiction saute d’un point à l’autre, créant une fonction en escalier. Augmenter k lisse la prédiction, mais un k trop grand écrase les variations locales.","type":"content","url":"/knn#r-gression","position":31},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Estimation de densité par noyaux"},"type":"lvl2","url":"/knn#estimation-de-densit-par-noyaux","position":32},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Estimation de densité par noyaux"},"content":"Avant d’aller plus loin dans la régression, introduisons un outil fondamental: l’estimation de densité par noyaux (kernel density estimation, ou fenêtres de Parzen).\n\nSupposons que nous observons des points x_1, \\ldots, x_N tirés d’une densité inconnue p(x). Comment estimer cette densité? Une approche naïve serait de construire un histogramme, mais les histogrammes dépendent du choix arbitraire des intervalles et produisent des estimations discontinues.\n\nL’idée des fenêtres de Parzen est de placer un petit “noyau” sur chaque observation et de sommer ces contributions:\\hat{p}(x) = \\frac{1}{N} \\sum_{i=1}^{N} K_\\lambda(x - x_i)\n\nLe noyau K_\\lambda est une fonction qui satisfait \\int K_\\lambda(u) \\, du = 1 et K_\\lambda(u) = K_\\lambda(-u). Le paramètre \\lambda contrôle la largeur de bande (bandwidth): plus \\lambda est grand, plus le noyau est étalé, plus l’estimation est lisse.\n\nLe noyau le plus courant est le noyau gaussien:K_\\lambda(u) = \\frac{1}{\\lambda \\sqrt{2\\pi}} \\exp\\left(-\\frac{u^2}{2\\lambda^2}\\right)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate samples from a mixture of Gaussians\nnp.random.seed(42)\nn = 100\nsamples = np.concatenate([\n    np.random.randn(n//2) * 0.5 + 2,\n    np.random.randn(n//2) * 0.8 + 5\n])\n\ndef gaussian_kernel(u, bandwidth):\n    return np.exp(-u**2 / (2 * bandwidth**2)) / (bandwidth * np.sqrt(2 * np.pi))\n\ndef kde(x_query, samples, bandwidth):\n    density = np.zeros_like(x_query)\n    for xi in samples:\n        density += gaussian_kernel(x_query - xi, bandwidth)\n    return density / len(samples)\n\nx = np.linspace(-1, 9, 500)\n\nfig, axes = plt.subplots(1, 3, figsize=(12, 3.5))\nbandwidths = [0.2, 0.5, 1.5]\n\nfor ax, bw in zip(axes, bandwidths):\n    density = kde(x, samples, bw)\n    \n    ax.fill_between(x, density, alpha=0.3, color='C0')\n    ax.plot(x, density, 'C0-', linewidth=2, label=f'KDE ($\\\\lambda={bw}$)')\n    ax.scatter(samples, np.zeros_like(samples) - 0.02, c='k', s=10, alpha=0.5, marker='|')\n    ax.set_xlabel('$x$')\n    ax.set_ylabel(r'$\\hat{p}(x)$')\n    ax.set_title(f'Largeur de bande $\\\\lambda = {bw}$')\n    ax.set_xlim(-1, 9)\n    ax.set_ylim(-0.05, 0.6)\n\nplt.tight_layout()\n\n\n\nAvec une petite largeur de bande (\\lambda = 0.2), chaque observation crée un pic distinct et l’estimation est très variable. Avec une grande largeur de bande (\\lambda = 1.5), les détails sont perdus et la structure bimodale des données est masquée. Le choix de \\lambda incarne encore une fois le compromis biais-variance.","type":"content","url":"/knn#estimation-de-densit-par-noyaux","position":33},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Régression de Nadaraya-Watson"},"type":"lvl2","url":"/knn#r-gression-de-nadaraya-watson","position":34},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Régression de Nadaraya-Watson"},"content":"L’estimation de densité par noyaux mène naturellement à une forme de régression plus souple que les k-ppv. Plutôt que de sélectionner exactement k voisins et de les traiter également, nous pouvons pondérer tous les points selon leur proximité à la requête.\n\nL’estimateur de Nadaraya-Watson définit la prédiction comme une moyenne pondérée:\\hat{y}(x) = \\frac{\\sum_{i=1}^{N} K_\\lambda(x - x_i) \\, y_i}{\\sum_{i=1}^{N} K_\\lambda(x - x_i)} = \\sum_{i=1}^{N} w_i(x) \\, y_i\n\noù les poids sont normalisés:w_i(x) = \\frac{K_\\lambda(x - x_i)}{\\sum_{j=1}^{N} K_\\lambda(x - x_j)}\n\nChaque point d’entraînement contribue à la prédiction, mais les points éloignés ont un poids négligeable. Le noyau agit comme une fenêtre qui détermine l’influence locale.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate 1D regression data\nnp.random.seed(42)\nn = 50\nX_train = np.sort(np.random.uniform(0, 10, n))\ny_train = np.sin(X_train) + np.random.randn(n) * 0.3\n\ndef gaussian_kernel(u, bandwidth):\n    return np.exp(-u**2 / (2 * bandwidth**2))\n\ndef nadaraya_watson(X_train, y_train, X_test, bandwidth):\n    predictions = []\n    for x in X_test:\n        weights = gaussian_kernel(X_train - x, bandwidth)\n        if np.sum(weights) > 1e-10:\n            predictions.append(np.sum(weights * y_train) / np.sum(weights))\n        else:\n            predictions.append(0)\n    return np.array(predictions)\n\nX_test = np.linspace(0, 10, 200)\n\nfig, axes = plt.subplots(1, 3, figsize=(12, 3.5))\nbandwidths = [0.2, 0.5, 1.5]\n\nfor ax, bw in zip(axes, bandwidths):\n    y_pred = nadaraya_watson(X_train, y_train, X_test, bw)\n    \n    ax.scatter(X_train, y_train, c='C0', s=30, alpha=0.6, label='Données')\n    ax.plot(X_test, y_pred, 'C1-', linewidth=2, label=f'Nadaraya-Watson')\n    ax.plot(X_test, np.sin(X_test), 'k--', alpha=0.5, label=r'$\\sin(x)$')\n    ax.set_xlabel('$x$')\n    ax.set_ylabel('$y$')\n    ax.set_title(f'$\\\\lambda = {bw}$')\n    ax.legend(loc='upper right', fontsize=8)\n    ax.set_xlim(0, 10)\n\nplt.tight_layout()\n\n\n\nComparé aux k-ppv, Nadaraya-Watson produit des prédictions plus lisses car la transition entre voisins est graduelle plutôt qu’abrupte. Le paramètre \\lambda joue un rôle analogue à k: une petite largeur de bande donne une courbe qui suit de près les données (haute variance), une grande largeur de bande lisse excessivement (haut biais).\n\nLes deux approches, k-ppv et Nadaraya-Watson, sont des méthodes à moyennes locales. Elles estiment \\mathbb{E}[Y \\mid X = x] en faisant une moyenne pondérée des y_i pour les points x_i proches de x. La différence réside dans la définition de “proche”: les k-ppv utilisent une frontière nette (les k plus proches), tandis que Nadaraya-Watson utilise une pondération douce (le noyau).","type":"content","url":"/knn#r-gression-de-nadaraya-watson","position":35},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Complexité"},"type":"lvl2","url":"/knn#complexit","position":36},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Complexité"},"content":"L’entraînement consiste à stocker les données: O(N). L’inférence requiert de calculer la distance à tous les points et d’identifier les k plus proches: O(Nd) par requête. Pour de grands ensembles, des structures comme les arbres k-d ou le hachage sensible à la localité (LSH) réduisent ce coût.","type":"content","url":"/knn#complexit","position":37},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Passage à l’échelle"},"type":"lvl2","url":"/knn#passage-l-chelle","position":38},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Passage à l’échelle"},"content":"L’implémentation naïve des k-ppv calcule la distance entre la requête et chaque point d’entraînement: O(Nd) par requête. Pour un million de points en dimension 100, chaque prédiction requiert 100 millions d’opérations. Cette complexité linéaire en N rend la méthode impraticable pour de grands ensembles de données.","type":"content","url":"/knn#passage-l-chelle","position":39},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Arbres k-d","lvl2":"Passage à l’échelle"},"type":"lvl3","url":"/knn#arbres-k-d","position":40},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Arbres k-d","lvl2":"Passage à l’échelle"},"content":"Les arbres k-d (k-dimensional trees) partitionnent récursivement l’espace en régions. À chaque nœud, l’espace est divisé selon une dimension, alternant entre les dimensions à chaque niveau.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\n\nnp.random.seed(42)\n\n# Generate points\npoints = np.random.rand(15, 2) * 10\n\n# Build k-d tree splits manually for visualization\n# We'll store splits as (dimension, value, depth, bounds)\nsplits = []\n\ndef build_kdtree_splits(points_idx, depth, x_min, x_max, y_min, y_max):\n    if len(points_idx) <= 1:\n        return\n    \n    dim = depth % 2\n    pts = points[points_idx]\n    \n    # Split at median\n    sorted_idx = np.argsort(pts[:, dim])\n    median_pos = len(sorted_idx) // 2\n    median_val = pts[sorted_idx[median_pos], dim]\n    \n    splits.append((dim, median_val, depth, x_min, x_max, y_min, y_max))\n    \n    left_idx = points_idx[sorted_idx[:median_pos]]\n    right_idx = points_idx[sorted_idx[median_pos:]]\n    \n    if dim == 0:  # vertical split\n        build_kdtree_splits(left_idx, depth + 1, x_min, median_val, y_min, y_max)\n        build_kdtree_splits(right_idx, depth + 1, median_val, x_max, y_min, y_max)\n    else:  # horizontal split\n        build_kdtree_splits(left_idx, depth + 1, x_min, x_max, y_min, median_val)\n        build_kdtree_splits(right_idx, depth + 1, x_min, x_max, median_val, y_max)\n\nbuild_kdtree_splits(np.arange(len(points)), 0, 0, 10, 0, 10)\n\nfig, ax = plt.subplots(figsize=(7, 6))\n\n# Draw splits\ncolors = ['C0', 'C1', 'C2', 'C3']\nfor dim, val, depth, x_min, x_max, y_min, y_max in splits:\n    color = colors[min(depth, len(colors)-1)]\n    alpha = 0.8 - depth * 0.15\n    if dim == 0:  # vertical line\n        ax.plot([val, val], [y_min, y_max], color=color, linewidth=2, alpha=alpha)\n    else:  # horizontal line\n        ax.plot([x_min, x_max], [val, val], color=color, linewidth=2, alpha=alpha)\n\n# Draw points\nax.scatter(points[:, 0], points[:, 1], s=80, c='black', zorder=5)\n\nax.set_xlim(-0.5, 10.5)\nax.set_ylim(-0.5, 10.5)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_aspect('equal')\n\n# Legend for depth\nfrom matplotlib.lines import Line2D\nlegend_elements = [Line2D([0], [0], color=colors[i], linewidth=2, label=f'Profondeur {i}') \n                   for i in range(4)]\nax.legend(handles=legend_elements, loc='upper right')\n\n\n\nLa recherche du plus proche voisin exploite cette structure: si la requête est loin de la frontière de division, des sous-arbres entiers peuvent être élagués sans examiner leurs points.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle, Rectangle\n\nnp.random.seed(42)\npoints = np.random.rand(15, 2) * 10\n\n# Query point\nquery = np.array([7.5, 3.0])\n\n# Find true nearest neighbor\ndistances = np.sqrt(np.sum((points - query)**2, axis=1))\nnn_idx = np.argmin(distances)\nnn_dist = distances[nn_idx]\n\n# Rebuild splits for this visualization\nsplits = []\ndef build_kdtree_splits(points_idx, depth, x_min, x_max, y_min, y_max):\n    if len(points_idx) <= 1:\n        return\n    dim = depth % 2\n    pts = points[points_idx]\n    sorted_idx = np.argsort(pts[:, dim])\n    median_pos = len(sorted_idx) // 2\n    median_val = pts[sorted_idx[median_pos], dim]\n    splits.append((dim, median_val, depth, x_min, x_max, y_min, y_max))\n    left_idx = points_idx[sorted_idx[:median_pos]]\n    right_idx = points_idx[sorted_idx[median_pos:]]\n    if dim == 0:\n        build_kdtree_splits(left_idx, depth + 1, x_min, median_val, y_min, y_max)\n        build_kdtree_splits(right_idx, depth + 1, median_val, x_max, y_min, y_max)\n    else:\n        build_kdtree_splits(left_idx, depth + 1, x_min, x_max, y_min, median_val)\n        build_kdtree_splits(right_idx, depth + 1, x_min, x_max, median_val, y_max)\n\nbuild_kdtree_splits(np.arange(len(points)), 0, 0, 10, 0, 10)\n\nfig, ax = plt.subplots(figsize=(7, 6))\n\n# Shade pruned regions (regions that don't intersect the search circle)\n# For simplicity, we'll manually identify some pruned regions\npruned_regions = [\n    (0, 4.5, 0, 10),      # Left side of first split\n]\n\nfor x_min, x_max, y_min, y_max in pruned_regions:\n    rect = Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, \n                     facecolor='gray', alpha=0.3, edgecolor='none')\n    ax.add_patch(rect)\n\n# Draw splits\nfor dim, val, depth, x_min, x_max, y_min, y_max in splits:\n    alpha = 0.6\n    if dim == 0:\n        ax.plot([val, val], [y_min, y_max], color='gray', linewidth=1.5, alpha=alpha)\n    else:\n        ax.plot([x_min, x_max], [val, val], color='gray', linewidth=1.5, alpha=alpha)\n\n# Draw search radius\ncircle = Circle(query, nn_dist, fill=False, color='C1', linewidth=2, linestyle='--')\nax.add_patch(circle)\n\n# Draw points\nax.scatter(points[:, 0], points[:, 1], s=80, c='C0', zorder=5, label='Points')\nax.scatter(points[nn_idx, 0], points[nn_idx, 1], s=120, c='C2', zorder=6, \n           edgecolors='black', linewidths=2, label='Plus proche voisin')\n\n# Draw query\nax.scatter(query[0], query[1], s=150, c='red', marker='*', zorder=7, label='Requête')\n\n# Draw line to nearest neighbor\nax.plot([query[0], points[nn_idx, 0]], [query[1], points[nn_idx, 1]], \n        'C2--', linewidth=1.5, alpha=0.7)\n\nax.set_xlim(-0.5, 10.5)\nax.set_ylim(-0.5, 10.5)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_aspect('equal')\nax.legend(loc='upper left')\n\n# Add annotation for pruned region\nax.annotate('Région élaguée\\n(pas explorée)', xy=(2, 5), fontsize=10, \n            ha='center', color='gray')\n\n\n\nEn basse dimension, les arbres k-d réduisent la complexité moyenne à O(\\log N) par requête. Mais leur efficacité se dégrade rapidement avec la dimension. Au-delà de d \\approx 20, la structure arborescente n’offre plus d’avantage significatif sur la recherche exhaustive. C’est là une autre manifestation du fléau de la dimensionnalité.","type":"content","url":"/knn#arbres-k-d","position":41},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Recherche approximative","lvl2":"Passage à l’échelle"},"type":"lvl3","url":"/knn#recherche-approximative","position":42},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Recherche approximative","lvl2":"Passage à l’échelle"},"content":"Quand la recherche exacte est trop coûteuse, on peut accepter des voisins approximatifs. Les méthodes de recherche approximative des plus proches voisins (approximate nearest neighbors, ANN) garantissent de trouver des points qui sont proches, sans garantir qu’ils soient les plus proches.\n\nLe hachage sensible à la localité (locality-sensitive hashing, LSH) projette les points dans un espace de hachage où les points proches ont une forte probabilité de collision. Plusieurs tables de hachage avec des fonctions différentes permettent d’atteindre un bon rappel. La complexité devient sous-linéaire en N, au prix d’une approximation.\n\nDes bibliothèques comme FAISS (Facebook AI Similarity Search) et Annoy (Approximate Nearest Neighbors Oh Yeah) implémentent ces algorithmes et permettent de chercher parmi des milliards de vecteurs. Ces outils sont essentiels pour les systèmes de recommandation et la recherche sémantique à grande échelle, où les représentations vectorielles (embeddings) de documents, images ou produits sont comparées par similarité.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(123)\n\n# Generate points in 2D\nn_points = 40\npoints = np.random.randn(n_points, 2)\n\n# Query point\nquery = np.array([0.5, 0.3])\n\n# Find true 5 nearest neighbors\nk = 5\ndistances = np.sqrt(np.sum((points - query)**2, axis=1))\ntrue_nn_idx = np.argsort(distances)[:k]\n\n# Simulate approximate nearest neighbors (miss one, include one wrong)\napprox_nn_idx = np.array([true_nn_idx[0], true_nn_idx[1], true_nn_idx[2], \n                          true_nn_idx[4], np.argsort(distances)[k+1]])\n\n# Points that are in both\ncommon_idx = np.intersect1d(true_nn_idx, approx_nn_idx)\n# Only in true\nonly_true = np.setdiff1d(true_nn_idx, approx_nn_idx)\n# Only in approx\nonly_approx = np.setdiff1d(approx_nn_idx, true_nn_idx)\n\nfig, axes = plt.subplots(1, 2, figsize=(11, 5))\n\n# Left: Exact search\nax = axes[0]\nax.scatter(points[:, 0], points[:, 1], s=50, c='lightgray', alpha=0.7)\nax.scatter(points[true_nn_idx, 0], points[true_nn_idx, 1], s=100, c='C2', \n           edgecolors='black', linewidths=1.5, label=f'{k} plus proches (vrais)')\nax.scatter(query[0], query[1], s=200, c='red', marker='*', zorder=10, label='Requête')\n\n# Draw circle for k-th distance\nkth_dist = distances[true_nn_idx[-1]]\ncircle = plt.Circle(query, kth_dist, fill=False, color='C2', linestyle='--', linewidth=1.5)\nax.add_patch(circle)\n\nax.set_xlim(-3, 3)\nax.set_ylim(-3, 3)\nax.set_aspect('equal')\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_title('Recherche exacte')\nax.legend(loc='upper left', fontsize=9)\n\n# Right: Approximate search\nax = axes[1]\nax.scatter(points[:, 0], points[:, 1], s=50, c='lightgray', alpha=0.7)\n\n# Common points (found by both)\nax.scatter(points[common_idx, 0], points[common_idx, 1], s=100, c='C2', \n           edgecolors='black', linewidths=1.5, label='Trouvés (corrects)')\n\n# Missed by approximate\nax.scatter(points[only_true, 0], points[only_true, 1], s=100, c='C3', \n           edgecolors='black', linewidths=1.5, marker='s', label='Manqués')\n\n# False positives from approximate\nax.scatter(points[only_approx, 0], points[only_approx, 1], s=100, c='C1', \n           edgecolors='black', linewidths=1.5, marker='^', label='Faux positifs')\n\nax.scatter(query[0], query[1], s=200, c='red', marker='*', zorder=10, label='Requête')\n\nax.set_xlim(-3, 3)\nax.set_ylim(-3, 3)\nax.set_aspect('equal')\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_title(f'Recherche approximative (rappel = {len(common_idx)}/{k})')\nax.legend(loc='upper left', fontsize=9)\n\nplt.tight_layout()\n\n\n\n","type":"content","url":"/knn#recherche-approximative","position":43},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Compromis précision-vitesse","lvl2":"Passage à l’échelle"},"type":"lvl3","url":"/knn#compromis-pr-cision-vitesse","position":44},{"hierarchy":{"lvl1":"K plus proches voisins","lvl3":"Compromis précision-vitesse","lvl2":"Passage à l’échelle"},"content":"Le choix entre recherche exacte et approximative dépend de l’application. Pour un diagnostic médical, une erreur dans l’identification des cas similaires peut avoir des conséquences graves: la recherche exacte est préférable. Pour suggérer des vidéos similaires sur une plateforme de streaming, quelques voisins manqués importent peu si les suggestions restent pertinentes.\n\nMéthode\n\nComplexité requête\n\nExacte\n\nDimension\n\nForce brute\n\nO(Nd)\n\nOui\n\nToute\n\nArbre k-d\n\nO(\\log N) à O(N)\n\nOui\n\nd \\lesssim 20\n\nLSH\n\nO(1) à O(N^{\\rho})\n\nNon\n\nHaute\n\nGraphes de proximité\n\nO(\\log N)\n\nNon\n\nHaute\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulated query times for different methods\nN = np.logspace(2, 7, 50)  # 100 to 10 million points\n\n# Brute force: O(N)\nbrute_force = N * 1e-7\n\n# k-d tree: O(log N) in low dim, degrades to O(N) in high dim\n# We show the ideal low-dim case\nkd_tree = np.log2(N) * 1e-5\n\n# ANN (e.g., HNSW): nearly constant with slight log factor\nann = np.log2(N) * 5e-6 + 1e-4\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nax.loglog(N, brute_force, 'C0-', linewidth=2, label='Force brute $O(N)$')\nax.loglog(N, kd_tree, 'C1-', linewidth=2, label=r'Arbre k-d $O(\\log N)$')\nax.loglog(N, ann, 'C2-', linewidth=2, label=r'ANN $O(\\log N)$')\n\n# Add shaded regions for practical regimes\nax.axvspan(100, 1e4, alpha=0.1, color='C0')\nax.axvspan(1e4, 1e6, alpha=0.1, color='C1')\nax.axvspan(1e6, 1e7, alpha=0.1, color='C2')\n\nax.set_xlabel('Taille de l\\'ensemble $N$')\nax.set_ylabel('Temps par requête (s)')\nax.legend(loc='upper left')\nax.set_xlim(100, 1e7)\nax.grid(True, alpha=0.3, which='both')\n\n# Annotations\nax.annotate('Petit\\nensemble', xy=(500, 1e-3), fontsize=9, ha='center', alpha=0.7)\nax.annotate('Moyen', xy=(1e5, 1e-3), fontsize=9, ha='center', alpha=0.7)\nax.annotate('Grande\\néchelle', xy=(3e6, 1e-3), fontsize=9, ha='center', alpha=0.7)\n\nplt.tight_layout()\n\n\n\nLa figure illustre comment le temps de requête évolue avec la taille de l’ensemble. La force brute devient rapidement prohibitive. Les méthodes approximatives maintiennent des temps de réponse acceptables même pour des millions de points.","type":"content","url":"/knn#compromis-pr-cision-vitesse","position":45},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Méthodes paramétriques et non paramétriques"},"type":"lvl2","url":"/knn#m-thodes-param-triques-et-non-param-triques","position":46},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Méthodes paramétriques et non paramétriques"},"content":"Les k-ppv sont une méthode non paramétrique: les données sont le modèle. Il n’y a pas de paramètres appris; les prédictions consultent directement l’ensemble d’entraînement. La complexité du modèle croît avec N.\n\n\n\nNon paramétrique\n\nParamétrique\n\nModèle\n\nLes données\n\nUn vecteur \\theta \\in \\mathbb{R}^p\n\nComplexité\n\nCroît avec N\n\nFixe\n\nInférence\n\nRequiert les données\n\nRequiert seulement \\theta\n\nLes méthodes paramétriques distillent l’information dans un vecteur de paramètres de taille fixe. Un réseau de neurones entraîné sur des milliards d’exemples n’a besoin que de ses poids pour faire des prédictions, pas des données d’entraînement.","type":"content","url":"/knn#m-thodes-param-triques-et-non-param-triques","position":47},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Résumé"},"type":"lvl2","url":"/knn#r-sum","position":48},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Résumé"},"content":"Les k plus proches voisins classifient un point par vote majoritaire parmi ses k voisins les plus proches. Le paramètre k contrôle le compromis biais-variance. Le choix de la distance encode les hypothèses sur la similarité. Le fléau de la dimensionnalité limite l’efficacité en haute dimension.\n\nLa méthode illustre la tension entre mémorisation et généralisation: avec k=1, l’erreur d’entraînement est nulle mais la généralisation est mauvaise. Elle illustre aussi la distinction entre approches non paramétriques (les données sont le modèle) et paramétriques (un vecteur de paramètres résume les données).\n\nLe chapitre suivant développe l’approche paramétrique: l’apprentissage comme problème d’optimisation, où nous cherchons les paramètres qui minimisent une fonction de perte.","type":"content","url":"/knn#r-sum","position":49},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Exercices"},"type":"lvl2","url":"/knn#exercices","position":50},{"hierarchy":{"lvl1":"K plus proches voisins","lvl2":"Exercices"},"content":"Exercice 1: Classification manuelle\n\nConsidérez les points d’entraînement suivants en 2D:\n\nPoint\n\nx_1\n\nx_2\n\nClasse\n\nA\n\n0\n\n0\n\n0\n\nB\n\n1\n\n0\n\n0\n\nC\n\n0\n\n1\n\n1\n\nD\n\n2\n\n2\n\n1\n\nE\n\n3\n\n1\n\n1\n\nPour le point requête x = (1, 1), identifiez les 3 plus proches voisins avec la distance euclidienne. Quelle est la prédiction du 3-ppv?\n\nRépétez avec k = 1 et k = 5. Les prédictions changent-elles?\n\nCalculez les distances avec la norme \\ell_1 (Manhattan). Les 3 plus proches voisins sont-ils les mêmes?\n\nExercice 2: Effet de la normalisation\n\nUn dataset contient deux variables: l’âge (entre 20 et 70 ans) et le revenu annuel (entre 20 000 et 200 000 euros).\n\nCalculez la distance euclidienne entre les points x_1 = (30, 50000) et x_2 = (35, 51000).\n\nCalculez la distance entre x_1 = (30, 50000) et x_3 = (31, 150000).\n\nLaquelle des deux paires est “plus proche”? Ce résultat est-il intuitivement raisonnable?\n\nProposez une transformation des données qui rendrait les deux variables comparables. Recalculez les distances après transformation.\n\nExercice 3: Compromis biais-variance\n\nSoit un problème de régression 1D où la vraie fonction est f(x) = \\sin(2\\pi x) et les observations sont bruitées: y = f(x) + \\varepsilon avec \\varepsilon \\sim \\mathcal{N}(0, 0.1).\n\nGénérez 50 points d’entraînement uniformément répartis sur [0, 1].\n\nImplémentez la régression k-ppv et tracez les prédictions pour k = 1, 5, 20, 50.\n\nCalculez l’erreur quadratique moyenne (MSE) sur un ensemble de test de 200 points pour chaque valeur de k.\n\nTracez le MSE en fonction de k. Quelle valeur de k minimise l’erreur de test?\n\nQue se passe-t-il quand k = N (nombre total de points d’entraînement)?\n\nExercice 4: Fléau de la dimensionnalité\n\nConsidérez N = 1000 points uniformément distribués dans l’hypercube [0, 1]^d.\n\nPour d = 1, 2, 5, 10, 20, 50, 100, calculez la distance moyenne au plus proche voisin parmi ces points. Utilisez la simulation Monte Carlo.\n\nTracez cette distance en fonction de d. Que constatez-vous?\n\nPour capturer les 10 plus proches voisins (soit 1% des données), quel est le rayon de la boule centrée sur un point arbitraire? Calculez ce rayon pour différentes dimensions.\n\nExpliquez pourquoi les k-ppv deviennent inefficaces en haute dimension, même avec beaucoup de données.\n\nExercice 5: Distances pour texte\n\nConsidérez trois documents représentés par leurs vecteurs de fréquence de mots (bag-of-words) sur un vocabulaire de 5 mots:\n\nDocument\n\nchat\n\nchien\n\nmaison\n\nvoiture\n\narbre\n\nd_1\n\n3\n\n0\n\n1\n\n0\n\n2\n\nd_2\n\n2\n\n1\n\n0\n\n0\n\n1\n\nd_3\n\n0\n\n0\n\n2\n\n3\n\n0\n\nCalculez la distance euclidienne entre chaque paire de documents.\n\nCalculez la similarité cosinus entre chaque paire, puis convertissez en distance (d = 1 - \\text{sim}).\n\nSelon chaque mesure, quels sont les deux documents les plus similaires?\n\nPourquoi la similarité cosinus est-elle souvent préférée pour les documents textuels?\n\nExercice 6: Nadaraya-Watson\n\nSoit les données d’entraînement: x = [0, 1, 2, 3] et y = [1, 2, 1.5, 3].\n\nAvec un noyau gaussien K_\\lambda(u) = \\exp(-u^2 / 2\\lambda^2) et \\lambda = 0.5, calculez manuellement la prédiction de Nadaraya-Watson pour x^* = 1.5.\n\nRépétez avec \\lambda = 2. Comment la prédiction change-t-elle?\n\nImplémentez l’estimateur et tracez les prédictions pour \\lambda = 0.2, 0.5, 1, 2 sur l’intervalle [0, 3].\n\nComparez visuellement avec la régression k-ppv pour k = 1, 2, 3. Quelle méthode produit des prédictions plus lisses?\n\nExercice 7: Complexité computationnelle\n\nVous développez un système de recommandation pour une plateforme avec 10 millions d’utilisateurs. Chaque utilisateur est représenté par un vecteur de 100 dimensions (embeddings).\n\nCombien d’opérations (multiplications et additions) faut-il pour trouver le plus proche voisin d’un utilisateur par force brute?\n\nSi chaque opération prend 1 nanoseconde, quel est le temps de réponse pour une requête?\n\nSi vous devez traiter 1000 requêtes par seconde, cette approche est-elle viable?\n\nUn arbre k-d réduit la complexité à O(\\log N) en basse dimension. Pourquoi cette structure n’est-elle pas efficace pour d = 100?\n\nProposez une stratégie pour ce problème à grande échelle.","type":"content","url":"/knn#exercices","position":51},{"hierarchy":{"lvl1":"Le problème d’apprentissage"},"type":"lvl1","url":"/learning-problem","position":0},{"hierarchy":{"lvl1":"Le problème d’apprentissage"},"content":"Objectifs d’apprentissage\n\nÀ la fin de ce chapitre, vous serez en mesure de:\n\nDéfinir formellement le problème d’apprentissage supervisé\n\nDistinguer les tâches de classification et de régression\n\nDéfinir le risque et le risque empirique\n\nExpliquer le principe de minimisation du risque empirique\n\nDériver l’estimateur du maximum de vraisemblance\n\nRelier le maximum de vraisemblance à la divergence de Kullback-Leibler\n\nIdentifier les sources d’écart entre performance mesurée et performance réelle\n\nDans le chapitre précédent, nous avons vu les k plus proches voisins, une méthode intuitive qui prédit en consultant directement les données d’entraînement. Cette approche est simple à comprendre et à implémenter, mais elle a un coût: les données doivent être conservées en mémoire, et chaque prédiction requiert de parcourir l’ensemble d’entraînement. Ce chapitre développe une approche différente: plutôt que de garder les données, nous cherchons à les résumer dans un ensemble de paramètres. L’apprentissage devient alors un problème d’optimisation.","type":"content","url":"/learning-problem","position":1},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Apprentissage supervisé"},"type":"lvl2","url":"/learning-problem#apprentissage-supervis","position":2},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Apprentissage supervisé"},"content":"Une ingénieure automobile mesure la distance de freinage d’un véhicule à différentes vitesses. Ses données ressemblent à ceci:\n\nVitesse (mph)\n\nDistance (ft)\n\n4\n\n2\n\n7\n\n4\n\n12\n\n20\n\n18\n\n56\n\n24\n\n93\n\nElle veut prédire la distance de freinage à 30 mph sans faire l’essai. Pour cela, elle cherche une fonction f telle que f(\\text{vitesse}) \\approx \\text{distance} sur ses observations. Si la fonction capture la relation sous-jacente, elle devrait donner une prédiction raisonnable pour des vitesses non mesurées.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mlbook_code.datasets import load_braking\n\nspeed, dist = load_braking()\n\nplt.figure(figsize=(6, 4))\nplt.scatter(speed, dist, alpha=0.7, label='Observations')\n\n# Fit quadratic\ncoeffs = np.polyfit(speed, dist, 2)\nspeed_grid = np.linspace(0, 30, 100)\ndist_pred = np.polyval(coeffs, speed_grid)\nplt.plot(speed_grid, dist_pred, 'k--', alpha=0.6, label='Fonction ajustée')\n\n# Prediction at 30 mph\npred_30 = np.polyval(coeffs, 30)\nplt.scatter([30], [pred_30], marker='x', s=80, color='C1', zorder=5, label=f'Prédiction à 30 mph: {pred_30:.0f} ft')\n\nplt.xlabel('Vitesse (mph)')\nplt.ylabel('Distance de freinage (ft)')\nplt.legend()\nplt.tight_layout()\n\n\n\nCe processus est l’ajustement de courbe (curve fitting). Nous avons des paires (entrée, sortie), nous ajustons une fonction, et nous utilisons cette fonction pour prédire. L’apprentissage supervisé généralise cette idée: les entrées peuvent être des vecteurs de dimension quelconque, les sorties peuvent être continues ou discrètes, et les fonctions candidates peuvent être bien plus complexes qu’un polynôme.\n\nFormellement, nous disposons d’un jeu de données \\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N composé de N paires, où chaque x_i \\in \\mathcal{X} est une entrée et y_i \\in \\mathcal{Y} est la sortie correspondante. L’objectif est de trouver une fonction f: \\mathcal{X} \\to \\mathcal{Y} qui approxime bien la relation entre entrées et sorties, y compris pour des exemples que nous n’avons pas encore observés.\n\nDans de nombreuses applications, les entrées sont des vecteurs de caractéristiques. Chaque exemple x_i \\in \\mathbb{R}^d est un vecteur de dimension d, où chaque composante représente une mesure ou un attribut. Pour prédire le prix d’une maison, les entrées pourraient être la superficie, le nombre de chambres et l’âge du bâtiment. Pour filtrer les pourriels, les entrées pourraient être des fréquences de mots. Pour diagnostiquer une maladie, les entrées pourraient être des résultats d’analyses sanguines.\n\nLorsque la sortie est une valeur continue, nous parlons de régression: f: \\mathbb{R}^d \\to \\mathbb{R} pour une sortie scalaire, ou f: \\mathbb{R}^d \\to \\mathbb{R}^p pour une sortie vectorielle. La distance de freinage, le prix d’une maison, la concentration d’un médicament dans le sang sont des exemples de régression.\n\nLorsque la sortie est une catégorie parmi un ensemble fini, nous parlons de classification. Pour la classification binaire, f: \\mathbb{R}^d \\to \\{0, 1\\}. Pour la classification multiclasse avec m catégories, f: \\mathbb{R}^d \\to \\{0, \\ldots, m-1\\}. Déterminer si un courriel est un pourriel, diagnostiquer une maladie, ou reconnaître un chiffre manuscrit sont des exemples de classification.","type":"content","url":"/learning-problem#apprentissage-supervis","position":3},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Mesurer l’erreur"},"type":"lvl2","url":"/learning-problem#mesurer-lerreur","position":4},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Mesurer l’erreur"},"content":"Pour choisir entre deux fonctions candidates, nous avons besoin d’un critère qui quantifie la qualité des prédictions. Une fonction de perte \\ell: \\mathcal{Y} \\times \\mathcal{Y} \\to \\mathbb{R}_+ mesure l’écart entre une prédiction \\hat{y} et la vraie valeur y. Une perte de zéro indique une prédiction parfaite; plus la perte est grande, plus l’erreur est importante.\n\nPour la régression, nous utilisons généralement la perte quadratique:\\ell_2(y, \\hat{y}) = (y - \\hat{y})^2\n\nCette perte pénalise les grandes erreurs de manière quadratique. Une erreur de 2 coûte quatre fois plus qu’une erreur de 1.\n\nReprenons les données de freinage. Supposons que notre fonction prédise 50 ft pour une vitesse où la vraie distance est 56 ft. La perte quadratique est (56 - 50)^2 = 36. Si elle prédit 70 ft, la perte est (56 - 70)^2 = 196. La perte quadratique pénalise sévèrement les grandes erreurs.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mlbook_code.datasets import load_braking\n\nspeed, dist = load_braking()\ncoeffs = np.polyfit(speed, dist, 2)\npredictions = np.polyval(coeffs, speed)\nresiduals = dist - predictions\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n# Left: predictions vs observations\nax = axes[0]\nax.scatter(speed, dist, alpha=0.7, label='Observations')\nspeed_grid = np.linspace(4, 25, 100)\nax.plot(speed_grid, np.polyval(coeffs, speed_grid), 'k--', alpha=0.6, label='Prédictions')\nfor i in range(0, len(speed), 5):\n    ax.plot([speed[i], speed[i]], [dist[i], predictions[i]], 'C1-', alpha=0.5)\nax.set_xlabel('Vitesse (mph)')\nax.set_ylabel('Distance (ft)')\nax.legend()\nax.set_title('Résidus: écarts entre observations et prédictions')\n\n# Right: histogram of squared residuals\nax = axes[1]\nax.hist(residuals**2, bins=15, edgecolor='black', alpha=0.7)\nax.set_xlabel('Perte quadratique $(y - \\hat{y})^2$')\nax.set_ylabel('Fréquence')\nax.set_title(f'MSE = {np.mean(residuals**2):.1f}')\n\nplt.tight_layout()\n\n\n\n\n\nPour la classification, un choix naturel est la perte 0-1:\\ell_{0-1}(y, \\hat{y}) = \\mathbb{1}_{y \\neq \\hat{y}} = \\begin{cases} 0 & \\text{si } y = \\hat{y} \\\\ 1 & \\text{si } y \\neq \\hat{y} \\end{cases}\n\nCette perte compte simplement les erreurs: elle vaut 1 pour une mauvaise prédiction, 0 sinon.\n\nLe choix de la fonction de perte dépend du problème. En diagnostic médical, manquer une maladie grave (faux négatif) peut avoir des conséquences bien plus importantes que de prescrire un test supplémentaire à un patient sain (faux positif). Une perte asymétrique refléterait cette différence. En régression, si les grandes erreurs sont particulièrement problématiques, la perte quadratique est appropriée; si nous voulons être robustes aux valeurs aberrantes, la perte absolue |y - \\hat{y}| est préférable.","type":"content","url":"/learning-problem#mesurer-lerreur","position":5},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Le risque"},"type":"lvl2","url":"/learning-problem#le-risque","position":6},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Le risque"},"content":"La perte évalue une seule prédiction. Pour évaluer un modèle dans son ensemble, nous voulons mesurer sa performance moyenne sur toutes les données possibles, pas seulement sur les exemples que nous avons observés.","type":"content","url":"/learning-problem#le-risque","position":7},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Pourquoi des variables aléatoires?","lvl2":"Le risque"},"type":"lvl3","url":"/learning-problem#pourquoi-des-variables-al-atoires","position":8},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Pourquoi des variables aléatoires?","lvl2":"Le risque"},"content":"Une question naturelle se pose: si nous ajustons une fonction déterministe f à des données, pourquoi avons-nous besoin de variables aléatoires et d’espérances? La fonction obtenue n’est-elle pas simplement une courbe fixe?\n\nLa réponse tient en un mot: généralisation. Nous ne nous intéressons pas vraiment à la performance sur les données d’entraînement car ces points sont déjà connus. Ce qui compte, c’est la performance sur des données futures que nous n’avons pas encore observées.\n\nConsidérons l’exemple de la distance de freinage. Les 50 mesures dans notre tableau sont un échantillon de toutes les mesures possibles. Si nous retournions sur le terrain et mesurions à nouveau, nous obtiendrions des valeurs légèrement différentes. En effet, le même véhicule à 20 mph ne s’arrête pas exactement à la même distance à chaque essai. Il y a de la variabilité intrinsèque: état de la route, température des freins, réflexes du conducteur.\n\nCette variabilité est capturée par une distribution de probabilité p(x, y). Nos 50 points sont des tirages de cette distribution. La question fondamentale devient alors:\n\nNotre modèle f prédira-t-il bien sur de nouveaux tirages de cette même distribution?\n\nLa fonction f elle-même est déterministe une fois entraînée. Mais son évaluation, savoir si elle prédit bien ou mal, dépend de quelles données futures elle rencontrera. Et ces données futures sont incertaines: elles seront tirées de p(x, y), mais nous ne savons pas lesquelles.\n\nLe risque formalise cette idée: c’est la perte moyenne que subira notre modèle f lorsqu’il sera confronté à des données tirées de p(x, y). C’est une mesure de performance prospective, tournée vers le futur.\n\nModèles déterministes vs stochastiques (et pourquoi on s’en fiche un peu)\n\nIl existe deux façons de raconter la même histoire.\n\nModèle déterministe: on suppose qu’il existe une relation y \\approx f^\\star(x), et que les écarts proviennent de facteurs non modélisés (mesure bruitée, variabilité du monde réel). Ici, f est une fonction déterministe; l’aléatoire vit dans les données que l’on observe et dans celles que l’on observera demain.\n\nModèle stochastique: on suppose plutôt que Y est une variable aléatoire conditionnellement à X=x, via une distribution p(y\\mid x). La \"bonne\" prédiction devient alors une question de moyenne/quantile/probabilité, selon la perte.\n\nDans la pratique, ces deux points de vue sont surtout des langages différents. Le formalisme probabiliste est souvent plus commode: il permet d’exprimer simplement \"la performance moyenne sur des données futures\" via une espérance. Ce chapitre adopte ce langage parce qu’il rend la généralisation et les garanties mathématiques plus propres, sans changer l’objectif final: produire une règle de prédiction utile.","type":"content","url":"/learning-problem#pourquoi-des-variables-al-atoires","position":9},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Définition formelle","lvl2":"Le risque"},"type":"lvl3","url":"/learning-problem#d-finition-formelle","position":10},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Définition formelle","lvl2":"Le risque"},"content":"Le risque d’une fonction f est l’espérance de la perte sur la distribution des données:\\mathcal{R}(f) = \\mathbb{E}_{(X,Y) \\sim p}\\left[\\ell(Y, f(X))\\right] = \\int \\ell(y, f(x)) \\, p(x, y) \\, dx \\, dy\n\nDécomposons cette formule étape par étape:\n\n\\mathbb{E}_{(X,Y) \\sim p}: L’espérance mathématique signifie “moyenne sur tous les exemples possibles”. La notation (X,Y) \\sim p indique que nous tirons les paires (x, y) selon la distribution p(x, y) de la nature.\n\n\\ell(Y, f(X)): Pour chaque exemple aléatoire (X, Y), nous calculons la perte entre la vraie valeur Y et la prédiction f(X) du modèle.\n\nL’intégrale \\int \\ell(y, f(x)) \\, p(x, y) \\, dx \\, dy: Cette intégrale calcule une moyenne pondérée. Pour chaque paire possible (x, y), nous multiplions la perte \\ell(y, f(x)) par la probabilité p(x, y) que cette paire apparaisse dans la nature, puis nous sommons (intégrons) sur toutes les paires possibles.","type":"content","url":"/learning-problem#d-finition-formelle","position":11},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Exemple concret","lvl2":"Le risque"},"type":"lvl3","url":"/learning-problem#exemple-concret","position":12},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Exemple concret","lvl2":"Le risque"},"content":"Considérons un problème de classification binaire en 2D. Supposons que x \\in [0, 1]^2 et y \\in \\{0, 1\\}. Pour calculer le risque, nous devrions:\n\nDiviser l’espace [0,1]^2 en une grille fine (par exemple, 1000 \\times 1000 points)\n\nPour chaque point x de la grille, considérer les deux valeurs possibles de y (0 et 1)\n\nPour chaque combinaison (x, y), calculer:\n\nLa probabilité p(x, y) que cette combinaison apparaisse\n\nLa perte \\ell(y, f(x)) si notre modèle prédit f(x)\n\nFaire la somme pondérée: \\sum_{x} \\sum_{y \\in \\{0,1\\}} \\ell(y, f(x)) \\cdot p(x, y)\n\nEn pratique, pour un espace continu, cette somme devient une intégrale sur un domaine continu, ce qui est encore plus complexe à calculer.\n\nVisualisons ceci concrètement. La figure suivante montre un problème de classification binaire où chaque classe suit une distribution gaussienne en 2D. Les contours représentent la densité p(x|y) pour chaque classe. La ligne pointillée est la frontière de décision d’un classificateur linéaire. Les régions ombrées indiquent où le classificateur fait des erreurs: la région rouge correspond aux points de classe 0 classés comme classe 1, et la région bleue correspond aux points de classe 1 classés comme classe 0.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mlbook_code.datasets import make_gaussian_mixture, gaussian_pdf\n\n# Get distribution parameters\n_, _, params = make_gaussian_mixture(n=100, seed=42)\nmu0, mu1, cov = params['mu0'], params['mu1'], params['cov']\n\n# Create grid for visualization\nx_range = np.linspace(-3, 5, 200)\ny_range = np.linspace(-3, 4, 200)\nX_grid, Y_grid = np.meshgrid(x_range, y_range)\npos = np.dstack([X_grid, Y_grid])\n\n# Compute class-conditional densities\np_x_given_0 = gaussian_pdf(pos, mu0, cov)\np_x_given_1 = gaussian_pdf(pos, mu1, cov)\n\n# Joint densities (with equal priors)\nprior = 0.5\np_x_y0 = p_x_given_0 * (1 - prior)\np_x_y1 = p_x_given_1 * prior\n\n# Linear decision boundary (Bayes optimal for equal covariances)\n# w^T x + b = 0 where w = Sigma^{-1}(mu1 - mu0)\ncov_inv = np.linalg.inv(cov)\nw = cov_inv @ (mu1 - mu0)\nb = -0.5 * (mu1 @ cov_inv @ mu1 - mu0 @ cov_inv @ mu0)\n\n# Decision boundary: w[0]*x + w[1]*y + b = 0  =>  y = -(w[0]*x + b)/w[1]\nx_boundary = np.linspace(-3, 5, 100)\ny_boundary = -(w[0] * x_boundary + b) / w[1]\n\n# Classifier prediction: classify as 1 if w^T x + b > 0\npredictions = (w[0] * X_grid + w[1] * Y_grid + b) > 0\n\n# Misclassification regions\n# Class 0 misclassified as 1: true class is 0, but prediction is 1\nmisclass_0 = predictions  # region where we predict 1\n# Class 1 misclassified as 0: true class is 1, but prediction is 0\nmisclass_1 = ~predictions  # region where we predict 0\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Plot class-conditional densities as contours\nlevels = [0.01, 0.05, 0.1, 0.15]\nax.contour(X_grid, Y_grid, p_x_given_0, levels=levels, colors='C0', alpha=0.7)\nax.contour(X_grid, Y_grid, p_x_given_1, levels=levels, colors='C1', alpha=0.7)\n\n# Shade misclassification regions weighted by probability\n# Red: class 0 points incorrectly classified as 1\nerror_region_0 = np.where(misclass_0, p_x_y0, 0)\n# Blue: class 1 points incorrectly classified as 0  \nerror_region_1 = np.where(misclass_1, p_x_y1, 0)\n\nax.contourf(X_grid, Y_grid, error_region_0, levels=[0.001, 0.01, 0.05, 0.1], \n            colors=['#ff000010', '#ff000030', '#ff000050'], extend='max')\nax.contourf(X_grid, Y_grid, error_region_1, levels=[0.001, 0.01, 0.05, 0.1],\n            colors=['#0000ff10', '#0000ff30', '#0000ff50'], extend='max')\n\n# Decision boundary\nax.plot(x_boundary, y_boundary, 'k--', linewidth=2, label='Frontière de décision')\n\n# Class centers\nax.scatter(*mu0, s=100, c='C0', marker='x', linewidths=3, zorder=5, label='Centre classe 0')\nax.scatter(*mu1, s=100, c='C1', marker='x', linewidths=3, zorder=5, label='Centre classe 1')\n\nax.set_xlim(-3, 5)\nax.set_ylim(-3, 4)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.legend(loc='upper left')\nax.set_aspect('equal')\n\nplt.tight_layout()\n\n\n\nLe risque est l’intégrale de la perte sur tout l’espace, pondérée par p(x, y). Les régions ombrées contribuent au risque: chaque point dans ces régions est mal classé, et sa contribution dépend de la densité de probabilité à cet endroit. Les régions denses proches de la frontière contribuent le plus au risque.","type":"content","url":"/learning-problem#exemple-concret","position":13},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Pourquoi le risque est important","lvl2":"Le risque"},"type":"lvl3","url":"/learning-problem#pourquoi-le-risque-est-important","position":14},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Pourquoi le risque est important","lvl2":"Le risque"},"content":"Le risque mesure ce que nous obtiendrons en moyenne si nous appliquons f à de nouvelles données tirées de la même distribution. Un modèle avec un faible risque fait de bonnes prédictions en général, pas seulement sur les exemples d’entraînement. C’est exactement ce que nous voulons optimiser: un modèle qui performe bien sur des données jamais vues, pas seulement sur celles qu’il a déjà observées.\n\nCette quantité est ce que nous voulons minimiser. Le problème fondamental est que nous ne connaissons pas la distribution p(x, y) de la nature. Nous n’y avons accès qu’indirectement, via un échantillon fini \\mathcal{D}.","type":"content","url":"/learning-problem#pourquoi-le-risque-est-important","position":15},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Le risque empirique"},"type":"lvl2","url":"/learning-problem#le-risque-empirique","position":16},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Le risque empirique"},"content":"Puisque le risque est inaccessible, nous l’approximons par une moyenne sur les données disponibles. Le risque empirique est:\\hat{\\mathcal{R}}(f, \\mathcal{D}) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell(y_i, f(x_i))\n\nCette quantité est calculable: c’est la moyenne des pertes sur l’échantillon d’entraînement. Pour la perte 0-1, le risque empirique est le taux d’erreur sur les données d’entraînement. Pour la perte quadratique, c’est l’erreur quadratique moyenne.","type":"content","url":"/learning-problem#le-risque-empirique","position":17},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Pourquoi le risque est-il inaccessible?","lvl2":"Le risque empirique"},"type":"lvl3","url":"/learning-problem#pourquoi-le-risque-est-il-inaccessible","position":18},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Pourquoi le risque est-il inaccessible?","lvl2":"Le risque empirique"},"content":"La nécessité d’utiliser le risque empirique découle de deux obstacles fondamentaux, l’un conceptuel et l’autre computationnel.","type":"content","url":"/learning-problem#pourquoi-le-risque-est-il-inaccessible","position":19},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Obstacle 1: La distribution p(x, y) est inconnue","lvl3":"Pourquoi le risque est-il inaccessible?","lvl2":"Le risque empirique"},"type":"lvl4","url":"/learning-problem#obstacle-1-la-distribution-p-x-y-est-inconnue","position":20},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Obstacle 1: La distribution p(x, y) est inconnue","lvl3":"Pourquoi le risque est-il inaccessible?","lvl2":"Le risque empirique"},"content":"La nature possède une distribution p(x, y) qui génère les données, mais nous ne la connaissons pas. Nous n’observons qu’un échantillon fini \\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N tiré de cette distribution. C’est comme si nous regardions quelques gouttes d’eau d’un océan: nous pouvons analyser ces gouttes, mais nous ne connaissons pas la composition complète de l’océan.\n\nMême si nous tentions d’estimer p(x, y) à partir des données (par exemple, via des techniques d’estimation de densité comme les mélanges de gaussiennes ou les estimateurs à noyau), nous n’obtiendrions qu’une approximation \\hat{p}(x, y) de la vraie distribution. Cette approximation serait elle-même imparfaite et dépendrait de nos hypothèses sur la forme de la distribution.\n\nLa figure suivante illustre ce problème. À gauche, la vraie distribution p(x, y) que la nature utilise pour générer les données (que nous ne connaissons pas). À droite, un échantillon de N = 50 points tirés de cette distribution (ce que nous observons).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mlbook_code.datasets import make_gaussian_mixture, gaussian_pdf\n\n# Get distribution parameters and sample\nX, y, params = make_gaussian_mixture(n=50, seed=42)\nmu0, mu1, cov = params['mu0'], params['mu1'], params['cov']\n\n# Create grid for visualization\nx_range = np.linspace(-3, 5, 150)\ny_range = np.linspace(-3, 4, 150)\nX_grid, Y_grid = np.meshgrid(x_range, y_range)\npos = np.dstack([X_grid, Y_grid])\n\n# Compute class-conditional densities\np_x_given_0 = gaussian_pdf(pos, mu0, cov)\np_x_given_1 = gaussian_pdf(pos, mu1, cov)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Left: True distribution (what nature knows)\nax = axes[0]\nax.contourf(X_grid, Y_grid, p_x_given_0, levels=15, cmap='Blues', alpha=0.6)\nax.contourf(X_grid, Y_grid, p_x_given_1, levels=15, cmap='Oranges', alpha=0.6)\nax.contour(X_grid, Y_grid, p_x_given_0, levels=5, colors='C0', alpha=0.8)\nax.contour(X_grid, Y_grid, p_x_given_1, levels=5, colors='C1', alpha=0.8)\nax.scatter(*mu0, s=100, c='C0', marker='x', linewidths=3, zorder=5)\nax.scatter(*mu1, s=100, c='C1', marker='x', linewidths=3, zorder=5)\nax.set_xlim(-3, 5)\nax.set_ylim(-3, 4)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_title('Distribution vraie $p(x, y)$\\n(inconnue)')\nax.set_aspect('equal')\n\n# Right: Finite sample (what we observe)\nax = axes[1]\nax.scatter(X[y == 0, 0], X[y == 0, 1], c='C0', alpha=0.7, s=50, label='Classe 0')\nax.scatter(X[y == 1, 0], X[y == 1, 1], c='C1', alpha=0.7, s=50, label='Classe 1')\nax.set_xlim(-3, 5)\nax.set_ylim(-3, 4)\nax.set_xlabel('$x_1$')\nax.set_ylabel('$x_2$')\nax.set_title(f'Échantillon observé $\\\\mathcal{{D}}$\\n($N = {len(X)}$ points)')\nax.legend()\nax.set_aspect('equal')\n\nplt.tight_layout()\n\n\n\nNous ne voyons que les points à droite. La structure continue à gauche, incluant les contours, les densités, ainsi que les régions de haute et basse probabilité, nous est cachée. C’est à partir de ces quelques points que nous devons estimer la performance de notre modèle.","type":"content","url":"/learning-problem#obstacle-1-la-distribution-p-x-y-est-inconnue","position":21},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Obstacle 2: L’intégration est computationnellement intractable","lvl3":"Pourquoi le risque est-il inaccessible?","lvl2":"Le risque empirique"},"type":"lvl4","url":"/learning-problem#obstacle-2-lint-gration-est-computationnellement-intractable","position":22},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl4":"Obstacle 2: L’intégration est computationnellement intractable","lvl3":"Pourquoi le risque est-il inaccessible?","lvl2":"Le risque empirique"},"content":"Supposons, par un miracle, que nous connaissions exactement p(x, y). Pourrions-nous alors calculer le risque \\mathcal{R}(f) = \\int \\ell(y, f(x)) \\, p(x, y) \\, dx \\, dy?\n\nLa réponse est généralement non, pour plusieurs raisons:\n\nPour les espaces continus: L’intégrale est une intégrale de grande dimension. Si x \\in \\mathbb{R}^d avec d grand (par exemple, d = 1000 pour des images ou d = 10^6 pour des données textuelles), nous devons intégrer sur un espace de dimension d+1.\n\nPour vous rappeler l’idée, en calcul on approche une intégrale en 1D par une somme: on découpe l’intervalle en petites tranches et on additionne des aires de rectangles ou de trapèzes. Par exemple, sur [a,b]:\\int_a^b g(x)\\,dx \\;\\approx\\; \\sum_{m=1}^{M} g(x_m)\\,\\Delta x\n\nCette idée générale, qui consiste à remplacer une intégrale par une somme pondérée de valeurs de g évaluées à des points x_m, s’appelle l’intégration numérique (ou quadrature).\n\nLe problème en apprentissage est que notre intégrale n’est pas en 1D. Si on applique le même raisonnement en dimension d en mettant, disons, M points par dimension, on obtient une grille de taille M^d (et ici d peut être très grand). Le nombre de points à évaluer explose donc exponentiellement avec d. C’est exactement la malédiction de la dimensionnalité.\n\nPour les espaces discrets: Si x et y sont discrets mais prennent de nombreuses valeurs, la somme \\sum_x \\sum_y \\ell(y, f(x)) \\cdot p(x, y) peut avoir un nombre exponentiel de termes. Par exemple, si x est un vecteur binaire de dimension d, il y a 2^d valeurs possibles pour x. Pour d = 100, cela fait déjà 2^{100} \\approx 10^{30} termes à sommer, ce qui est computationnellement impossible.\n\nIntégration de Monte Carlo: On pourrait penser utiliser l’intégration de Monte Carlo: tirer des échantillons (x, y) selon p(x, y) et estimer l’intégrale par la moyenne empirique. Mais pour obtenir une estimation précise du risque, nous aurions besoin d’un très grand nombre d’échantillons (potentiellement infini pour une précision parfaite). De plus, cela nécessiterait de pouvoir échantillonner efficacement depuis p(x, y), ce qui est lui-même un problème difficile si la distribution est complexe.\n\nLa figure suivante illustre la malédiction de la dimensionnalité. Avec seulement 10 points par dimension pour une quadrature numérique, le nombre total de points d’évaluation explose rapidement.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Number of grid points per dimension\npoints_per_dim = 10\n\n# Dimensions to consider\ndimensions = np.array([1, 2, 3, 5, 10, 20, 50, 100])\n\n# Total grid points = points_per_dim^d\ntotal_points = points_per_dim ** dimensions.astype(float)\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nbars = ax.bar(range(len(dimensions)), total_points, color='steelblue', edgecolor='black')\n\n# Add reference lines\nax.axhline(y=1e9, color='C1', linestyle='--', alpha=0.7, label='1 milliard (limite pratique)')\nax.axhline(y=1e80, color='C3', linestyle=':', alpha=0.7, label='$10^{80}$ (atomes dans l\\'univers)')\n\nax.set_yscale('log')\nax.set_xticks(range(len(dimensions)))\nax.set_xticklabels([f'd={d}' for d in dimensions])\nax.set_xlabel('Dimension de l\\'espace des entrées')\nax.set_ylabel('Nombre de points de grille')\nax.set_title(f'Points nécessaires pour l\\'intégration numérique\\n({points_per_dim} points par dimension)')\nax.legend(loc='upper left')\n\n# Annotate a few bars\nfor i, (d, n) in enumerate(zip(dimensions, total_points)):\n    if d <= 5:\n        ax.annotate(f'$10^{{{d}}}$', (i, n), ha='center', va='bottom', fontsize=9)\n    elif d == 10:\n        ax.annotate(f'$10^{{{10}}}$', (i, n), ha='center', va='bottom', fontsize=9)\n    elif d == 100:\n        ax.annotate(f'$10^{{{100}}}$', (i, n), ha='center', va='bottom', fontsize=9)\n\nax.set_ylim(1, 1e105)\n\nplt.tight_layout()\n\n\n\nEn dimension 10, il faut déjà \n\n1010 points, soit dix milliards. En dimension 100, il en faut \n\n10100, un nombre qui dépasse le nombre d’atomes dans l’univers observable. L’intégration numérique directe est donc impossible pour les problèmes de haute dimension, même si nous connaissions p(x, y) exactement.","type":"content","url":"/learning-problem#obstacle-2-lint-gration-est-computationnellement-intractable","position":23},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Le risque empirique comme seule option pratique","lvl2":"Le risque empirique"},"type":"lvl3","url":"/learning-problem#le-risque-empirique-comme-seule-option-pratique","position":24},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Le risque empirique comme seule option pratique","lvl2":"Le risque empirique"},"content":"Face à ces obstacles, le risque empirique est notre seule option calculable. Il remplace l’intégrale sur la distribution inconnue par une moyenne sur l’échantillon fini que nous possédons:\\hat{\\mathcal{R}}(f, \\mathcal{D}) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell(y_i, f(x_i))\n\nCette formule est simple à calculer: nous parcourons nos N exemples d’entraînement, calculons la perte pour chacun, et faisons la moyenne.\n\nReprenons les données de freinage. Divisons-les en deux parties: les mesures à vitesses faibles (4-19 mph) pour l’entraînement, et les mesures à vitesses élevées (20-25 mph) pour le test. Le risque empirique sur l’ensemble d’entraînement mesure la qualité de l’ajustement. Le risque empirique sur l’ensemble de test estime la performance sur des vitesses non vues pendant l’entraînement.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mlbook_code.datasets import load_braking\n\nspeed, dist = load_braking()\n\n# Split: train on low speeds, test on high speeds\ntrain_mask = speed < 20\ntest_mask = speed >= 20\n\nspeed_train, dist_train = speed[train_mask], dist[train_mask]\nspeed_test, dist_test = speed[test_mask], dist[test_mask]\n\n# Fit on training data\ncoeffs = np.polyfit(speed_train, dist_train, 2)\n\n# Compute MSE on train and test\npred_train = np.polyval(coeffs, speed_train)\npred_test = np.polyval(coeffs, speed_test)\nmse_train = np.mean((dist_train - pred_train)**2)\nmse_test = np.mean((dist_test - pred_test)**2)\n\nplt.figure(figsize=(7, 4))\nplt.scatter(speed_train, dist_train, alpha=0.7, label=f'Entraînement (MSE={mse_train:.1f})')\nplt.scatter(speed_test, dist_test, alpha=0.7, marker='s', label=f'Test (MSE={mse_test:.1f})')\n\nspeed_grid = np.linspace(4, 28, 100)\nplt.plot(speed_grid, np.polyval(coeffs, speed_grid), 'k--', alpha=0.6, label='Fonction ajustée')\n\nplt.axvline(x=20, color='gray', linestyle=':', alpha=0.5)\nplt.xlabel('Vitesse (mph)')\nplt.ylabel('Distance (ft)')\nplt.legend()\nplt.tight_layout()\n\n\n\nDans cet exemple, le MSE sur l’ensemble de test est plus élevé que sur l’ensemble d’entraînement. Cet écart est typique: la fonction a été optimisée pour les données d’entraînement, pas pour les données de test.\n\nSous l’hypothèse que les exemples (x_i, y_i) sont tirés indépendamment et identiquement distribués (i.i.d.) selon p(x, y), le risque empirique est un estimateur non biaisé du vrai risque: \\mathbb{E}[\\hat{\\mathcal{R}}(f, \\mathcal{D})] = \\mathcal{R}(f). Cela signifie qu’en moyenne, sur tous les échantillons possibles, le risque empirique est égal au vrai risque.\n\nPar la loi des grands nombres, lorsque N \\to \\infty, le risque empirique converge vers le vrai risque (presque sûrement). Avec suffisamment de données, si l’échantillon est représentatif de la distribution, le risque empirique devrait être proche du risque.\n\nLa figure suivante illustre cette convergence. Nous utilisons le problème de classification gaussienne pour lequel nous pouvons calculer le vrai risque analytiquement. Chaque courbe montre l’évolution du risque empirique pour un échantillon de taille croissante. Toutes les courbes convergent vers le vrai risque (ligne pointillée), mais avec des fluctuations qui diminuent à mesure que N augmente.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom mlbook_code.datasets import make_gaussian_mixture, gaussian_pdf\n\n# Get distribution parameters\n_, _, params = make_gaussian_mixture(n=100, seed=0)\nmu0, mu1, cov = params['mu0'], params['mu1'], params['cov']\n\n# Compute Bayes-optimal classifier error rate (true risk)\n# For Gaussian classes with equal covariance, the Bayes error is:\n# P(error) = Phi(-d/2) where d is the Mahalanobis distance between means\ncov_inv = np.linalg.inv(cov)\nd_squared = (mu1 - mu0) @ cov_inv @ (mu1 - mu0)\nd = np.sqrt(d_squared)\ntrue_risk = norm.cdf(-d / 2)\n\n# Simulate empirical risk for different sample sizes\nsample_sizes = np.arange(10, 1001, 10)\nn_runs = 20\n\nfig, ax = plt.subplots(figsize=(9, 5))\n\n# Store all runs for confidence band\nall_risks = np.zeros((n_runs, len(sample_sizes)))\n\nfor run in range(n_runs):\n    empirical_risks = []\n    # Generate a large dataset and compute cumulative empirical risk\n    X, y, _ = make_gaussian_mixture(n=1000, seed=run)\n    \n    # Bayes-optimal classifier: predict 1 if w^T x + b > 0\n    w = cov_inv @ (mu1 - mu0)\n    b = -0.5 * (mu1 @ cov_inv @ mu1 - mu0 @ cov_inv @ mu0)\n    \n    for n in sample_sizes:\n        X_n, y_n = X[:n], y[:n]\n        predictions = (X_n @ w + b > 0).astype(float)\n        emp_risk = np.mean(predictions != y_n)\n        empirical_risks.append(emp_risk)\n    \n    all_risks[run] = empirical_risks\n    ax.plot(sample_sizes, empirical_risks, 'C0-', alpha=0.15, linewidth=0.8)\n\n# Mean and confidence bands\nmean_risk = np.mean(all_risks, axis=0)\nstd_risk = np.std(all_risks, axis=0)\nax.fill_between(sample_sizes, mean_risk - 2*std_risk, mean_risk + 2*std_risk, \n                alpha=0.3, color='C0', label='Intervalle ± 2 écarts-types')\nax.plot(sample_sizes, mean_risk, 'C0-', linewidth=2, label='Moyenne empirique')\n\n# True risk\nax.axhline(y=true_risk, color='C3', linestyle='--', linewidth=2, \n           label=f'Vrai risque = {true_risk:.3f}')\n\nax.set_xlabel('Taille de l\\'échantillon $N$')\nax.set_ylabel('Risque empirique (taux d\\'erreur)')\nax.set_title('Convergence du risque empirique vers le vrai risque')\nax.legend(loc='upper right')\nax.set_xlim(0, 1000)\nax.set_ylim(0, 0.35)\n\nplt.tight_layout()\n\n\n\nAvec N = 50, le risque empirique peut facilement varier de 0.10 à 0.25 selon l’échantillon. Avec N = 500, la variabilité est beaucoup plus faible. C’est la loi des grands nombres en action: plus l’échantillon est grand, plus l’estimation est précise.","type":"content","url":"/learning-problem#le-risque-empirique-comme-seule-option-pratique","position":25},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Le compromis fondamental","lvl2":"Le risque empirique"},"type":"lvl3","url":"/learning-problem#le-compromis-fondamental","position":26},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Le compromis fondamental","lvl2":"Le risque empirique"},"content":"Cette situation crée un compromis fondamental en apprentissage automatique:\n\nCe que nous voulons minimiser: Le risque \\mathcal{R}(f), qui mesure la performance sur toutes les données possibles\n\nCe que nous pouvons minimiser: Le risque empirique \\hat{\\mathcal{R}}(f, \\mathcal{D}), qui mesure la performance sur nos données d’entraînement\n\nL’écart entre ces deux quantités est au cœur de l’apprentissage automatique. Un modèle peut avoir un risque empirique très faible (il performe bien sur les données d’entraînement) tout en ayant un risque élevé (il performe mal sur de nouvelles données). C’est le problème du surapprentissage, que nous explorerons plus en détail dans le chapitre sur la généralisation.\n\nLa question de savoir quand et à quelle vitesse l’approximation du risque par le risque empirique est fiable relève de la théorie de la généralisation, que nous aborderons au chapitre suivant.","type":"content","url":"/learning-problem#le-compromis-fondamental","position":27},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Minimisation du risque empirique"},"type":"lvl2","url":"/learning-problem#minimisation-du-risque-empirique","position":28},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Minimisation du risque empirique"},"content":"Nous avons maintenant les éléments pour formuler l’apprentissage comme un problème d’optimisation. Nous cherchons la fonction f dans une classe \\mathcal{F} qui minimise le risque:f^\\star = \\arg\\min_{f \\in \\mathcal{F}} \\mathcal{R}(f)\n\nPuisque le risque est inaccessible, nous le remplaçons par le risque empirique:\\hat{f} = \\arg\\min_{f \\in \\mathcal{F}} \\hat{\\mathcal{R}}(f, \\mathcal{D})\n\nCe principe est la minimisation du risque empirique (MRE). L’idée est simple: choisir la fonction qui fait le moins d’erreurs sur les données d’entraînement, en espérant que cette performance se transfère aux nouvelles données.\n\nLa classe \\mathcal{F} est notre classe d’hypothèses. Elle représente l’ensemble des fonctions que nous sommes prêts à considérer. Par exemple, si \\mathcal{F} est l’ensemble des fonctions linéaires, nous cherchons la meilleure fonction linéaire. Si \\mathcal{F} est l’ensemble des polynômes de degré au plus k, nous cherchons le meilleur polynôme de ce degré. Le choix de \\mathcal{F} encode nos hypothèses sur la forme de la relation entre entrées et sorties.\n\nLa question centrale de la théorie de l’apprentissage est: quand le minimiseur du risque empirique a-t-il un faible risque? Si \\hat{f} minimise \\hat{\\mathcal{R}} et f^\\star minimise \\mathcal{R}, nous voulons que \\mathcal{R}(\\hat{f}) soit proche de \\mathcal{R}(f^\\star). Cette question dépend de la taille de l’échantillon N, de la complexité de la classe \\mathcal{F}, et de propriétés de la distribution p.","type":"content","url":"/learning-problem#minimisation-du-risque-empirique","position":29},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Fonctions de perte de substitution"},"type":"lvl2","url":"/learning-problem#fonctions-de-perte-de-substitution","position":30},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Fonctions de perte de substitution"},"content":"La perte 0-1 pose un problème pratique. Pour trouver le minimiseur du risque empirique, nous utilisons généralement des méthodes d’optimisation itératives comme la descente de gradient. Ces méthodes requièrent que la fonction objectif soit différentiable, or la perte 0-1 est constante par morceaux: sa dérivée est nulle presque partout et indéfinie aux points de discontinuité.\n\nNous contournons ce problème en utilisant des fonctions de perte de substitution: des approximations convexes et différentiables de la perte originale.\n\nPour la classification binaire avec y \\in \\{-1, +1\\}, la perte logistique est:\\ell_{\\text{log}}(y, s) = \\log(1 + e^{-y \\cdot s})\n\noù s = f(x) est le score produit par le modèle. Cette fonction est convexe et différentiable partout. Lorsque y et s ont le même signe (prédiction correcte avec confiance), la perte est faible. Lorsqu’ils ont des signes opposés (erreur), la perte croît linéairement avec l’amplitude de l’erreur.\n\nLa perte à charnière (hinge loss) est utilisée dans les machines à vecteurs de support:\\ell_{\\text{hinge}}(y, s) = \\max(0, 1 - y \\cdot s)\n\nCette fonction est convexe mais non différentiable au point y \\cdot s = 1. Elle est nulle lorsque la prédiction est correcte avec une marge suffisante (y \\cdot s \\geq 1), et croît linéairement sinon.\n\nCes deux fonctions majorent la perte 0-1: pour tout y et s, nous avons \\ell_{0-1} \\leq \\ell_{\\text{log}} et \\ell_{0-1} \\leq \\ell_{\\text{hinge}}. Minimiser ces substituts garantit donc un certain contrôle sur la perte originale.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Margin: y * s (positive = correct prediction, negative = error)\nmargin = np.linspace(-3, 3, 500)\n\n# 0-1 loss: 1 if margin < 0, else 0\nloss_01 = (margin < 0).astype(float)\n\n# Logistic loss: log(1 + exp(-margin))\nloss_log = np.log(1 + np.exp(-margin))\n\n# Hinge loss: max(0, 1 - margin)\nloss_hinge = np.maximum(0, 1 - margin)\n\nfig, ax = plt.subplots(figsize=(8, 5))\n\nax.plot(margin, loss_01, 'k-', linewidth=2, label='Perte 0-1')\nax.plot(margin, loss_log, 'C0-', linewidth=2, label='Perte logistique')\nax.plot(margin, loss_hinge, 'C1-', linewidth=2, label='Perte à charnière')\n\nax.axvline(0, color='gray', linestyle=':', alpha=0.5)\nax.axhline(1, color='gray', linestyle=':', alpha=0.3)\n\nax.set_xlabel(r'Marge $y \\cdot s$')\nax.set_ylabel('Perte')\nax.set_xlim(-3, 3)\nax.set_ylim(-0.1, 4)\nax.legend()\nax.set_title('Fonctions de perte de substitution comme bornes supérieures convexes')\n\n# Annotate regions\nax.text(-1.5, 3.5, 'Erreur\\n(prédiction incorrecte)', ha='center', fontsize=9, color='gray')\nax.text(1.5, 0.3, 'Correct\\n(prédiction juste)', ha='center', fontsize=9, color='gray')\n\nplt.tight_layout()\n\n\n\nLa figure montre les trois fonctions de perte en fonction de la marge y \\cdot s. Une marge positive indique une prédiction correcte (le signe de s correspond à y), une marge négative indique une erreur. La perte 0-1 est discontinue au point y \\cdot s = 0. Les pertes logistique et à charnière sont continues et convexes, ce qui permet d’utiliser des méthodes d’optimisation par gradient. Elles majorent partout la perte 0-1.","type":"content","url":"/learning-problem#fonctions-de-perte-de-substitution","position":31},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Maximum de vraisemblance"},"type":"lvl2","url":"/learning-problem#maximum-de-vraisemblance","position":32},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Maximum de vraisemblance"},"content":"Jusqu’ici, nous avons choisi des fonctions de perte de manière ad hoc: la perte quadratique semble raisonnable pour la régression, la perte logistique pour la classification. Mais d’où viennent ces choix? Existe-t-il un principe unificateur?\n\nLe maximum de vraisemblance offre une réponse: plutôt que de choisir une perte arbitraire, nous modélisons explicitement comment les données ont été générées, puis nous cherchons les paramètres qui rendent nos observations les plus probables.","type":"content","url":"/learning-problem#maximum-de-vraisemblance","position":33},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Construction de la vraisemblance","lvl2":"Maximum de vraisemblance"},"type":"lvl3","url":"/learning-problem#construction-de-la-vraisemblance","position":34},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Construction de la vraisemblance","lvl2":"Maximum de vraisemblance"},"content":"Supposons que nous avons un modèle paramétrique p(y|x; \\theta) qui, pour chaque entrée x et choix de paramètres \\theta, définit une distribution sur les sorties possibles y. Par exemple, en régression, ce pourrait être une gaussienne centrée sur f(x; \\theta).\n\nConsidérons un seul exemple (x_1, y_1). Pour des paramètres \\theta fixés, nous pouvons évaluer p(y_1 | x_1; \\theta): la probabilité (ou densité) que le modèle assigne à l’observation y_1. Si cette valeur est élevée, les paramètres \\theta “expliquent bien” cette observation. Si elle est faible, y_1 est une valeur improbable sous ce modèle.\n\nAvec deux exemples indépendants (x_1, y_1) et (x_2, y_2), la probabilité conjointe est le produit:p(y_1, y_2 | x_1, x_2; \\theta) = p(y_1 | x_1; \\theta) \\cdot p(y_2 | x_2; \\theta)\n\nAvec N exemples indépendants, nous obtenons la vraisemblance:\\mathcal{L}(\\theta) = \\prod_{i=1}^N p(y_i | x_i; \\theta)\n\nCette quantité est une fonction de \\theta. Elle répond à la question: pour ce choix de paramètres, quelle est la probabilité d’avoir observé exactement ces données?","type":"content","url":"/learning-problem#construction-de-la-vraisemblance","position":35},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Pourquoi maximiser?","lvl2":"Maximum de vraisemblance"},"type":"lvl3","url":"/learning-problem#pourquoi-maximiser","position":36},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Pourquoi maximiser?","lvl2":"Maximum de vraisemblance"},"content":"Si \\mathcal{L}(\\theta_A) > \\mathcal{L}(\\theta_B), alors les données observées sont plus probables sous \\theta_A que sous \\theta_B. Les paramètres \\theta_A rendent les observations moins “surprenantes”.\n\nL’estimateur du maximum de vraisemblance (EMV) choisit les paramètres qui maximisent cette probabilité:\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_\\theta \\mathcal{L}(\\theta) = \\arg\\max_\\theta \\prod_{i=1}^N p(y_i | x_i; \\theta)\n\nC’est le choix de paramètres sous lequel nos données sont les plus “attendues”.","type":"content","url":"/learning-problem#pourquoi-maximiser","position":37},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Du produit à la somme","lvl2":"Maximum de vraisemblance"},"type":"lvl3","url":"/learning-problem#du-produit-la-somme","position":38},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Du produit à la somme","lvl2":"Maximum de vraisemblance"},"content":"En pratique, multiplier N probabilités (souvent petites) pose des problèmes numériques: le résultat devient rapidement trop petit pour être représenté par un ordinateur. Le logarithme résout ce problème: il transforme le produit en somme et, comme c’est une fonction croissante, il ne change pas le maximiseur:\\log \\mathcal{L}(\\theta) = \\sum_{i=1}^N \\log p(y_i | x_i; \\theta)\n\nPour l’optimisation, nous préférons minimiser plutôt que maximiser (par convention). La log-vraisemblance négative (negative log-likelihood, NLL) est notre fonction objectif:\\text{NLL}(\\theta) = -\\sum_{i=1}^N \\log p(y_i | x_i; \\theta)\n\nRemarquez la structure: c’est une somme sur les exemples d’une quantité -\\log p(y_i | x_i; \\theta) qui dépend de chaque observation. Cette quantité joue le rôle d’une fonction de perte. Le maximum de vraisemblance est donc un cas particulier de la minimisation du risque empirique, où la perte est définie par le modèle probabiliste lui-même.","type":"content","url":"/learning-problem#du-produit-la-somme","position":39},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Régression avec bruit gaussien: d’où vient la perte quadratique?","lvl2":"Maximum de vraisemblance"},"type":"lvl3","url":"/learning-problem#r-gression-avec-bruit-gaussien-do-vient-la-perte-quadratique","position":40},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Régression avec bruit gaussien: d’où vient la perte quadratique?","lvl2":"Maximum de vraisemblance"},"content":"Appliquons ce principe à la régression. Le modèle de génération des données est simple: la sortie observée est la prédiction “vraie” du modèle, corrompue par un bruit aléatoire gaussien:y = f(x; \\theta) + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)\n\nCe modèle dit que si nous connaissions les vrais paramètres \\theta et que nous mesurions y pour un x donné, nous obtiendrions f(x; \\theta) plus ou moins \\sigma la plupart du temps.\n\nLa distribution conditionnelle qui en découle est:p(y|x; \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - f(x; \\theta))^2}{2\\sigma^2}\\right)\n\nCalculons la log-vraisemblance négative:\\text{NLL}(\\theta) = -\\sum_{i=1}^N \\log p(y_i | x_i; \\theta) = \\frac{1}{2\\sigma^2} \\sum_{i=1}^N (y_i - f(x_i; \\theta))^2 + \\frac{N}{2}\\log(2\\pi\\sigma^2)\n\nLe second terme ne dépend pas de \\theta. Minimiser la NLL revient donc exactement à minimiser la somme des erreurs quadratiques.\n\nC’est un résultat fondamental: la perte quadratique n’est pas un choix arbitraire. Elle découle naturellement de l’hypothèse que les erreurs de mesure suivent une loi gaussienne. Le maximum de vraisemblance sous bruit gaussien coïncide avec les moindres carrés.","type":"content","url":"/learning-problem#r-gression-avec-bruit-gaussien-do-vient-la-perte-quadratique","position":41},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Exemple: pharmacocinétique","lvl2":"Maximum de vraisemblance"},"type":"lvl3","url":"/learning-problem#exemple-pharmacocin-tique","position":42},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Exemple: pharmacocinétique","lvl2":"Maximum de vraisemblance"},"content":"Le MLE s’applique à des modèles non linéaires. Considérons la concentration d’un médicament dans le sang après administration orale. Les données suivantes proviennent d’une étude sur la théophylline, un bronchodilatateur:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.optimize import minimize\nfrom mlbook_code.datasets import load_theophylline\n\ntime, conc = load_theophylline(subject=1)\n\n# Model: C(t) = C0 * exp(-k * t) for t > t_peak\n# We'll fit on the decay phase (after peak)\npeak_idx = np.argmax(conc)\nt_decay = time[peak_idx:]\nc_decay = conc[peak_idx:]\n\n# MLE: minimize NLL under Gaussian noise\ndef neg_log_likelihood(params, t, c):\n    C0, k, sigma = params\n    if sigma <= 0 or k <= 0 or C0 <= 0:\n        return np.inf\n    pred = C0 * np.exp(-k * (t - t[0]))\n    nll = 0.5 * len(t) * np.log(2 * np.pi * sigma**2)\n    nll += 0.5 * np.sum((c - pred)**2) / sigma**2\n    return nll\n\n# Initial guess and optimization\nx0 = [c_decay[0], 0.1, 1.0]\nresult = minimize(neg_log_likelihood, x0, args=(t_decay, c_decay), method='Nelder-Mead')\nC0_mle, k_mle, sigma_mle = result.x\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\n# Left: data and fit\nax = axes[0]\nax.scatter(time, conc, s=50, zorder=5, label='Observations')\nt_grid = np.linspace(time[peak_idx], 25, 100)\nax.plot(t_grid, C0_mle * np.exp(-k_mle * (t_grid - time[peak_idx])), 'k--', \n        label=f'MLE: $C_0$={C0_mle:.1f}, $k$={k_mle:.2f}')\nax.axvline(time[peak_idx], color='gray', linestyle=':', alpha=0.5)\nax.set_xlabel('Temps (h)')\nax.set_ylabel('Concentration (mg/L)')\nax.legend()\nax.set_title('Concentration plasmatique de théophylline')\n\n# Right: residuals\nax = axes[1]\npred_decay = C0_mle * np.exp(-k_mle * (t_decay - t_decay[0]))\nresiduals = c_decay - pred_decay\nax.stem(t_decay, residuals, basefmt=' ')\nax.axhline(0, color='gray', linestyle='-', alpha=0.3)\nax.set_xlabel('Temps (h)')\nax.set_ylabel('Résidu (mg/L)')\nax.set_title(rf'$\\sigma$ estimé: {sigma_mle:.2f} mg/L')\n\nplt.tight_layout()\n\n\n\nLe modèle C(t) = C_0 e^{-kt} décrit la décroissance exponentielle après le pic de concentration. Les paramètres C_0 (concentration initiale) et k (constante d’élimination) sont estimés par maximum de vraisemblance sous l’hypothèse d’un bruit gaussien. Cette approche est identique à celle des moindres carrés, mais elle fournit également une estimation de l’écart-type du bruit \\sigma.","type":"content","url":"/learning-problem#exemple-pharmacocin-tique","position":43},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Classification binaire","lvl2":"Maximum de vraisemblance"},"type":"lvl3","url":"/learning-problem#classification-binaire","position":44},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Classification binaire","lvl2":"Maximum de vraisemblance"},"content":"Pour la classification binaire avec y \\in \\{0, 1\\}, nous modélisons la probabilité de la classe positive par:p(y = 1 | x; \\theta) = \\sigma(f(x; \\theta)) = \\frac{1}{1 + e^{-f(x; \\theta)}}\n\noù \\sigma est la fonction sigmoïde. La distribution conditionnelle suit une loi de Bernoulli:p(y|x; \\theta) = \\sigma(f(x; \\theta))^y (1 - \\sigma(f(x; \\theta)))^{1-y}\n\nLa log-vraisemblance négative est:\\text{NLL}(\\theta) = -\\sum_{i=1}^N \\left[ y_i \\log \\sigma(f(x_i; \\theta)) + (1-y_i) \\log(1 - \\sigma(f(x_i; \\theta))) \\right]\n\nCette quantité est l’entropie croisée binaire. Elle correspond à la perte logistique, à une reparamétrisation près.","type":"content","url":"/learning-problem#classification-binaire","position":45},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Lien entre MRE et maximum de vraisemblance"},"type":"lvl2","url":"/learning-problem#lien-entre-mre-et-maximum-de-vraisemblance","position":46},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Lien entre MRE et maximum de vraisemblance"},"content":"La minimisation du risque empirique et l’estimation par maximum de vraisemblance sont deux formulations du même problème lorsque nous choisissons la perte logarithmique \\ell(y, f(x)) = -\\log p(y | f(x)).\n\nLe risque empirique avec cette perte est:\\hat{\\mathcal{R}}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N -\\log p(y_i | x_i; \\theta) = \\frac{1}{N} \\text{NLL}(\\theta)\n\nLe minimiseur du risque empirique est donc l’estimateur du maximum de vraisemblance. Les deux approches, l’une fondée sur la théorie de la décision et l’autre sur l’inférence statistique, convergent vers le même algorithme.","type":"content","url":"/learning-problem#lien-entre-mre-et-maximum-de-vraisemblance","position":47},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Interprétation informationnelle"},"type":"lvl2","url":"/learning-problem#interpr-tation-informationnelle","position":48},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Interprétation informationnelle"},"content":"Nous pouvons interpréter le maximum de vraisemblance du point de vue de la théorie de l’information. L’EMV trouve le modèle paramétrique le plus proche de la distribution empirique des données.\n\nLa distribution empirique place une masse 1/N sur chaque observation:p_{\\mathcal{D}}(y) = \\frac{1}{N} \\sum_{i=1}^N \\delta(y - y_i)\n\nLa divergence de Kullback-Leibler mesure la dissimilarité entre deux distributions:D_{\\text{KL}}(p \\| q) = \\sum_y p(y) \\log \\frac{p(y)}{q(y)}\n\nCette quantité est toujours positive ou nulle, et vaut zéro si et seulement si p = q. Elle n’est pas symétrique: D_{\\text{KL}}(p \\| q) \\neq D_{\\text{KL}}(q \\| p) en général.\n\nEn posant p = p_{\\mathcal{D}} et q = p(\\cdot | \\theta):D_{\\text{KL}}(p_{\\mathcal{D}} \\| p(\\cdot|\\theta)) = -\\mathbb{H}(p_{\\mathcal{D}}) - \\frac{1}{N} \\sum_{i=1}^N \\log p(y_i | \\theta)\n\nLe premier terme, l’entropie de la distribution empirique, ne dépend pas de \\theta. Minimiser la divergence KL revient à minimiser la log-vraisemblance négative:\\arg\\min_\\theta D_{\\text{KL}}(p_{\\mathcal{D}} \\| p(\\cdot|\\theta)) = \\arg\\min_\\theta \\text{NLL}(\\theta)\n\nL’EMV trouve les paramètres qui rendent le modèle aussi proche que possible de la distribution empirique au sens de la divergence KL.","type":"content","url":"/learning-problem#interpr-tation-informationnelle","position":49},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Classes de modèles"},"type":"lvl2","url":"/learning-problem#classes-de-mod-les","position":50},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Classes de modèles"},"content":"Nous n’avons pas encore précisé la forme des fonctions f que nous considérons. Le choix de la classe d’hypothèses \\mathcal{F} détermine ce que le modèle peut représenter.\n\nLe modèle le plus simple est la régression linéaire:f(x; w, b) = w^\\top x + b = \\sum_{j=1}^d w_j x_j + b\n\nLes paramètres sont le vecteur de poids w \\in \\mathbb{R}^d et le biais b \\in \\mathbb{R}. Ce modèle suppose que la sortie est une combinaison linéaire des entrées.\n\nPour capturer des relations non linéaires tout en gardant un modèle linéaire dans les paramètres, nous pouvons transformer les entrées. En régression polynomiale, nous appliquons une fonction \\phi: \\mathbb{R} \\to \\mathbb{R}^{k+1}:\\phi(x) = [1, x, x^2, \\ldots, x^k]\n\nLa prédiction devient f(x; w) = w^\\top \\phi(x). Le modèle est polynomial en x mais linéaire en w, ce qui permet d’utiliser les mêmes algorithmes d’optimisation.\n\nLe degré k contrôle la capacité du modèle: sa capacité à représenter des fonctions complexes. Avec k = 1, nous avons une droite. Avec k élevé, le polynôme peut osciller pour passer par tous les points d’entraînement. Avec k = N - 1, nous pouvons interpoler exactement les N points: le risque empirique atteint zéro. Mais un polynôme qui passe exactement par les points d’entraînement n’a aucune raison de bien prédire les nouveaux points.\n\nIllustrons ce phénomène avec les données de freinage. Nous ajustons des polynômes de degrés 1, 2, 5 et 15, et comparons leurs erreurs sur les ensembles d’entraînement et de test.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nfrom mlbook_code.datasets import load_braking\n\n# Suppress polyfit warnings for high-degree polynomials (expected for this demo)\nwarnings.filterwarnings('ignore', message='Polyfit may be poorly conditioned')\n\nspeed, dist = load_braking()\n\n# Train/test split\nnp.random.seed(42)\nindices = np.random.permutation(len(speed))\ntrain_idx, test_idx = indices[:35], indices[35:]\nspeed_train, dist_train = speed[train_idx], dist[train_idx]\nspeed_test, dist_test = speed[test_idx], dist[test_idx]\n\ndegrees_to_plot = [1, 2, 5, 15]\ndegrees_eval = range(1, 16)\nfig, axes = plt.subplots(2, 2, figsize=(10, 8))\n\n# Pre-compute all errors for the summary plot later\nall_train_errors = []\nall_test_errors = []\nfor deg in degrees_eval:\n    coeffs = np.polyfit(speed_train, dist_train, deg)\n    all_train_errors.append(np.mean((dist_train - np.polyval(coeffs, speed_train))**2))\n    all_test_errors.append(np.mean((dist_test - np.polyval(coeffs, speed_test))**2))\n\nfor ax, deg in zip(axes.flat, degrees_to_plot):\n    # Fit polynomial\n    coeffs = np.polyfit(speed_train, dist_train, deg)\n    \n    # Predictions\n    pred_train = np.polyval(coeffs, speed_train)\n    pred_test = np.polyval(coeffs, speed_test)\n    \n    # MSE\n    mse_train = np.mean((dist_train - pred_train)**2)\n    mse_test = np.mean((dist_test - pred_test)**2)\n    \n    # Plot\n    ax.scatter(speed_train, dist_train, alpha=0.6, s=30, label='Train')\n    ax.scatter(speed_test, dist_test, alpha=0.6, s=30, marker='s', label='Test')\n    \n    speed_grid = np.linspace(3, 26, 200)\n    pred_grid = np.polyval(coeffs, speed_grid)\n    # Clip extreme predictions for visualization\n    pred_grid = np.clip(pred_grid, -50, 200)\n    ax.plot(speed_grid, pred_grid, 'k-', alpha=0.7)\n    \n    ax.set_xlim(3, 26)\n    ax.set_ylim(-20, 150)\n    ax.set_xlabel('Vitesse (mph)')\n    ax.set_ylabel('Distance (ft)')\n    ax.set_title(f'Degré {deg}: Train MSE={mse_train:.1f}, Test MSE={mse_test:.1f}')\n    if deg == 1:\n        ax.legend()\n\nplt.tight_layout()\n\n\n\nLe polynôme de degré 1 (droite) ne capture pas la courbure des données: c’est du sous-apprentissage. Le polynôme de degré 2 capture bien la relation quadratique. Le polynôme de degré 5 commence à osciller. Le polynôme de degré 15 passe près de tous les points d’entraînement, mais ses oscillations produisent des prédictions absurdes entre les points: c’est du surapprentissage.\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax.plot(degrees_eval, all_train_errors, 'o-', linewidth=2, label='Erreur entraînement')\nax.plot(degrees_eval, all_test_errors, 's-', linewidth=2, label='Erreur test')\n\n# Utiliser une échelle logarithmique car l'erreur de test explose\nax.set_yscale('log')\n\nax.set_xlabel('Degré du polynôme (complexité)')\nax.set_ylabel('MSE (échelle log)')\nax.set_xticks(range(1, 16, 2))\nax.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\nax.legend()\n\nax.set_title('Compromis biais-variance (Échelle logarithmique)')\nplt.tight_layout()\n\n\n\nL’erreur d’entraînement diminue avec le degré du polynôme. L’erreur de test diminue d’abord (quand le modèle gagne en expressivité), puis augmente (quand le modèle commence à mémoriser le bruit). Le meilleur modèle se trouve à l’intersection de ces deux tendances.","type":"content","url":"/learning-problem#classes-de-mod-les","position":51},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Régularisation"},"type":"lvl2","url":"/learning-problem#r-gularisation","position":52},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Régularisation"},"content":"Une alternative à la réduction du degré du polynôme consiste à pénaliser la complexité du modèle directement dans la fonction objectif. Le risque empirique régularisé est:\\hat{\\mathcal{R}}_\\lambda(\\theta) = \\hat{\\mathcal{R}}(\\theta) + \\lambda \\, C(\\theta)\n\noù C(\\theta) mesure la complexité du modèle et \\lambda \\geq 0 contrôle l’intensité de la pénalisation. Un choix courant est la régularisation \\ell_2 (ou weight decay):C(\\theta) = \\|\\theta\\|_2^2 = \\sum_j \\theta_j^2\n\nCette pénalisation pousse les paramètres vers zéro, ce qui a pour effet de lisser la fonction apprise. En régression linéaire, l’ajout de cette pénalité donne la régression ridge:\\hat{w}_{\\text{ridge}} = \\arg\\min_w \\frac{1}{N}\\sum_{i=1}^N (y_i - w^\\top x_i)^2 + \\lambda \\|w\\|_2^2\n\nIllustrons l’effet de la régularisation sur le même problème de régression polynomiale. Avec un polynôme de degré 15 et différentes valeurs de \\lambda:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mlbook_code.datasets import load_braking\n\nspeed, dist = load_braking()\n\n# Train/test split\nnp.random.seed(42)\nindices = np.random.permutation(len(speed))\ntrain_idx, test_idx = indices[:35], indices[35:]\nspeed_train, dist_train = speed[train_idx], dist[train_idx]\nspeed_test, dist_test = speed[test_idx], dist[test_idx]\n\n# Build polynomial features (degree 15)\ndegree = 15\ndef poly_features(x, deg):\n    return np.vstack([x**i for i in range(deg+1)]).T\n\nX_train = poly_features(speed_train, degree)\nX_test = poly_features(speed_test, degree)\n\n# Ridge regression for different lambda values\nlambdas = [0, 1e-6, 1e-3, 1]\nfig, axes = plt.subplots(2, 2, figsize=(10, 8))\n\nfor ax, lam in zip(axes.flat, lambdas):\n    # Ridge solution: (X^T X + lambda I)^{-1} X^T y\n    I = np.eye(X_train.shape[1])\n    I[0, 0] = 0  # Don't regularize bias\n    w = np.linalg.solve(X_train.T @ X_train + lam * I, X_train.T @ dist_train)\n    \n    # Predictions\n    pred_train = X_train @ w\n    pred_test = X_test @ w\n    mse_train = np.mean((dist_train - pred_train)**2)\n    mse_test = np.mean((dist_test - pred_test)**2)\n    \n    # Plot\n    ax.scatter(speed_train, dist_train, alpha=0.6, s=30, label='Train')\n    ax.scatter(speed_test, dist_test, alpha=0.6, s=30, marker='s', label='Test')\n    \n    speed_grid = np.linspace(3, 26, 200)\n    X_grid = poly_features(speed_grid, degree)\n    pred_grid = X_grid @ w\n    pred_grid = np.clip(pred_grid, -50, 200)\n    ax.plot(speed_grid, pred_grid, 'k-', alpha=0.7)\n    \n    ax.set_xlim(3, 26)\n    ax.set_ylim(-20, 150)\n    ax.set_xlabel('Vitesse (mph)')\n    ax.set_ylabel('Distance (ft)')\n    ax.set_title(f'$\\\\lambda$ = {lam}: Train MSE={mse_train:.1f}, Test MSE={mse_test:.1f}')\n    if lam == 0:\n        ax.legend()\n\nplt.tight_layout()\n\n\n\nSans régularisation (\\lambda = 0), le polynôme de degré 15 oscille fortement. Avec une régularisation modérée (\\lambda = 10^{-3}), les oscillations sont atténuées et l’erreur de test diminue. Avec une régularisation trop forte (\\lambda = 1), le modèle devient trop contraint et sous-apprend.","type":"content","url":"/learning-problem#r-gularisation","position":53},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Interprétation bayésienne","lvl2":"Régularisation"},"type":"lvl3","url":"/learning-problem#interpr-tation-bay-sienne","position":54},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Interprétation bayésienne","lvl2":"Régularisation"},"content":"La régularisation \\ell_2 admet une interprétation probabiliste. Si nous plaçons un a priori gaussien sur les paramètres p(w) = \\mathcal{N}(w | 0, \\sigma_w^2 I), l’estimateur du maximum a posteriori (MAP) est:\\hat{w}_{\\text{MAP}} = \\arg\\max_w \\log p(\\mathcal{D}|w) + \\log p(w)\n\nEn développant:\\hat{w}_{\\text{MAP}} = \\arg\\min_w \\text{NLL}(w) + \\frac{1}{2\\sigma_w^2}\\|w\\|_2^2\n\nL’estimation MAP avec un a priori gaussien coïncide avec la régression ridge, avec \\lambda = 1/(2\\sigma_w^2). Un a priori avec une petite variance (forte croyance que les paramètres sont proches de zéro) correspond à une grande valeur de \\lambda.","type":"content","url":"/learning-problem#interpr-tation-bay-sienne","position":55},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Choix de l’hyperparamètre","lvl2":"Régularisation"},"type":"lvl3","url":"/learning-problem#choix-de-lhyperparam-tre","position":56},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Choix de l’hyperparamètre","lvl2":"Régularisation"},"content":"La valeur de \\lambda est un hyperparamètre qui doit être choisi avant l’entraînement. Ce choix se fait généralement par validation: on entraîne le modèle pour plusieurs valeurs de \\lambda et on retient celle qui minimise l’erreur sur un ensemble de validation.","type":"content","url":"/learning-problem#choix-de-lhyperparam-tre","position":57},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Généralisation"},"type":"lvl2","url":"/learning-problem#g-n-ralisation","position":58},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Généralisation"},"content":"La différence entre le risque et le risque empirique est l’écart de généralisation:\\text{Écart} = \\mathcal{R}(f) - \\hat{\\mathcal{R}}(f; \\mathcal{D}_{\\text{train}})\n\nUn modèle qui minimise le risque empirique peut avoir un risque élevé si cet écart est grand. Ce phénomène est le surapprentissage: le modèle s’ajuste aux particularités de l’échantillon d’entraînement, y compris le bruit, plutôt qu’aux régularités sous-jacentes. L’erreur d’entraînement est faible, mais l’erreur sur de nouvelles données est élevée.\n\nÀ l’inverse, un modèle trop simple peut avoir un risque empirique et un risque tous deux élevés. C’est le sous-apprentissage: le modèle n’a pas la capacité de capturer la structure des données.","type":"content","url":"/learning-problem#g-n-ralisation","position":59},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Extrapolation","lvl2":"Généralisation"},"type":"lvl3","url":"/learning-problem#extrapolation","position":60},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl3":"Extrapolation","lvl2":"Généralisation"},"content":"Un cas particulier de mauvaise généralisation est l’extrapolation: prédire pour des entrées en dehors de la plage des données d’entraînement. Même un modèle bien ajusté peut échouer spectaculairement lorsqu’on lui demande de prédire au-delà de ce qu’il a vu.\n\nConsidérons des essais en soufflerie pour mesurer la portance d’une aile à différentes vitesses. Les tests sont effectués entre 20 et 60 m/s. L’ingénieur veut prédire la portance à 100 m/s.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mlbook_code.datasets import make_lift_data\n\n# Generate training data in limited range\nnp.random.seed(42)\nv_train = np.linspace(20, 60, 8)\nrho, S, C_L = 1.225, 20.0, 0.5\nL_true_train = 0.5 * rho * v_train**2 * S * C_L\nL_train = L_true_train + np.random.normal(0, 400, len(v_train))\n\n# Fit quadratic (correct model) and higher-degree polynomial\ncoeffs_2 = np.polyfit(v_train, L_train, 2)\ncoeffs_5 = np.polyfit(v_train, L_train, 5)\n\n# Extrapolation range\nv_extrap = np.linspace(15, 110, 200)\nL_true_extrap = 0.5 * rho * v_extrap**2 * S * C_L\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\nfor ax, coeffs, deg in zip(axes, [coeffs_2, coeffs_5], [2, 5]):\n    ax.scatter(v_train, L_train, s=50, zorder=5, label='Observations')\n    ax.plot(v_extrap, L_true_extrap, 'g-', alpha=0.5, label='Vraie relation')\n    \n    L_pred = np.polyval(coeffs, v_extrap)\n    ax.plot(v_extrap, L_pred, 'k--', label=f'Polynôme degré {deg}')\n    \n    # Mark extrapolation region\n    ax.axvline(60, color='gray', linestyle=':', alpha=0.5)\n    ax.axvspan(60, 110, alpha=0.1, color='red')\n    \n    ax.set_xlabel('Vitesse (m/s)')\n    ax.set_ylabel('Portance (N)')\n    ax.set_title(f'Degré {deg}')\n    ax.legend(loc='upper left')\n    ax.set_ylim(-5000, 80000)\n    ax.text(85, 5000, 'Extrapolation', ha='center', fontsize=10, color='red', alpha=0.7)\n\nplt.tight_layout()\n\n\n\nLe polynôme de degré 2 (qui correspond au vrai modèle physique L \\propto v^2) extrapole correctement. Le polynôme de degré 5, bien qu’il ajuste aussi bien les données d’entraînement, diverge complètement en dehors de la plage observée.\n\nEn pratique, nous estimons le risque par le risque empirique sur un ensemble de test \\mathcal{D}_{\\text{test}} disjoint de l’ensemble d’entraînement. Un troisième ensemble, l’ensemble de validation, sert à choisir parmi plusieurs modèles ou à régler des hyperparamètres. L’ensemble de test doit rester intact jusqu’à l’évaluation finale, pour fournir une estimation non biaisée.\n\nCette séparation est importante. Si nous utilisons l’ensemble de test pour faire des choix (quel modèle garder, quelle valeur d’hyperparamètre utiliser), l’estimation de performance sur ce même ensemble devient optimiste. Nous aurions alors besoin d’un quatrième ensemble pour obtenir une estimation fiable.\n\nMise en garde: la fuite d’information\n\nLes outils modernes de génération de code peuvent produire des pipelines d’apprentissage machine complets en quelques minutes. Mais ces pipelines peuvent contenir des erreurs subtiles qui mènent à des résultats trop beaux pour être vrais.\n\nUn exemple vécu: un praticien utilise un assistant de programmation pour construire un modèle prédictif. L’erreur d’entraînement passe de 0.20 à 0.01 en quelques itérations, un résultat spectaculaire. Mais en examinant le code de plus près, il découvre que le modèle utilise des caractéristiques qui ne seraient pas disponibles au moment du déploiement. Par exemple, dans un problème de prédiction temporelle, le modèle avait accès à des informations futures. Il s’agissait de données qui existent dans l’ensemble d’entraînement historique, mais qui n’existeront pas quand le modèle sera utilisé en production.\n\nCe phénomène s’appelle la fuite d’information (data leakage). Le modèle ne généralise pas: il triche. Les métriques d’entraînement sont excellentes, mais le modèle échouera en déploiement.\n\nVotre rôle dans un monde où le code s’écrit facilement: auditer les pipelines, vérifier que les caractéristiques utilisées seront disponibles en production, et maintenir une séparation stricte entre les données d’entraînement et de test.","type":"content","url":"/learning-problem#extrapolation","position":61},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Biais inductifs"},"type":"lvl2","url":"/learning-problem#biais-inductifs","position":62},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Biais inductifs"},"content":"Il n’existe pas de modèle universel qui fonctionne optimalement pour tous les problèmes. Ce résultat, connu sous le nom de théorème du no free lunch, affirme qu’un algorithme d’apprentissage qui performe bien sur une classe de problèmes performe nécessairement moins bien sur d’autres.\n\nTout modèle encode des biais inductifs: des hypothèses implicites ou explicites sur la structure du problème. La régression linéaire suppose que la relation entre entrées et sorties est linéaire. Les k plus proches voisins supposent que les points proches dans l’espace des entrées ont des sorties similaires. Les réseaux convolutifs supposent que les motifs locaux dans une image sont informatifs indépendamment de leur position.\n\nCes hypothèses sont nécessaires pour que l’apprentissage soit possible. Sans elles, nous n’aurions aucune raison de croire que la performance sur l’échantillon d’entraînement prédit la performance sur de nouvelles données. Le choix du modèle et de ses hypothèses est une décision que l’algorithme ne peut pas prendre seul; elle requiert une connaissance du domaine.","type":"content","url":"/learning-problem#biais-inductifs","position":63},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Résumé"},"type":"lvl2","url":"/learning-problem#r-sum","position":64},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Résumé"},"content":"Ce chapitre a établi le cadre formel de l’apprentissage supervisé. Nous avons défini le risque comme la mesure de performance que nous voulons optimiser, et le risque empirique comme son approximation calculable. Le principe de minimisation du risque empirique consiste à choisir le modèle qui minimise cette approximation.\n\nL’estimation par maximum de vraisemblance offre une perspective complémentaire, fondée sur l’inférence statistique. Les deux approches coïncident pour la perte logarithmique. L’interprétation en termes de divergence KL montre que le maximum de vraisemblance trouve le modèle le plus proche de la distribution empirique.\n\nLa question centrale que ce chapitre laisse ouverte est celle de la généralisation: quand le risque empirique est-il un bon indicateur du vrai risque? Cette question dépend de la taille de l’échantillon, de la complexité du modèle, et de la distribution des données. Le chapitre suivant développe les outils pour y répondre.","type":"content","url":"/learning-problem#r-sum","position":65},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Exercices"},"type":"lvl2","url":"/learning-problem#exercices","position":66},{"hierarchy":{"lvl1":"Le problème d’apprentissage","lvl2":"Exercices"},"content":"Exercice 1: Usure d’outil\n\nUn machiniste mesure l’usure d’un outil de coupe (en mm) à différents temps de coupe (en minutes):from mlbook_code.datasets import make_tool_wear\n\ntime, wear = make_tool_wear(n=10, seed=0)\nfor t, w in zip(time, wear):\n    print(f\"t = {t:5.1f} min, usure = {w:.3f} mm\")\n\nL’outil doit être remplacé lorsque l’usure atteint 0.4 mm.\n\nVisualisation. Tracez les données. Quelle forme de relation observez-vous?\n\nAjustement. Ajustez un modèle linéaire w(t) = at + b et un modèle en loi de puissance w(t) = at^b aux données. Pour le second modèle, utilisez une transformation logarithmique: \\log w = \\log a + b \\log t.\n\nComparaison. Calculez le MSE de chaque modèle sur les données. Lequel ajuste mieux?\n\nPrédiction. Selon chaque modèle, à quel moment l’usure atteindra-t-elle 0.4 mm? Les deux modèles donnent-ils la même réponse?\n\nExtrapolation. Si vous n’aviez mesuré que jusqu’à t = 15 min, vos prédictions changeraient-elles? Discutez du risque d’extrapolation.\n\nExercice 2: Risque et risque empirique\n\nSoit un problème de classification binaire avec la perte 0-1. Un classificateur f fait 3 erreurs sur 20 exemples d’entraînement.\n\nQuel est le risque empirique de f sur l’ensemble d’entraînement?\n\nPeut-on en déduire le vrai risque \\mathcal{R}(f)? Pourquoi ou pourquoi pas?\n\nSi nous avions 1000 exemples de test et que f fait 45 erreurs, quelle serait notre meilleure estimation du vrai risque?\n\nExercice 3: Maximum de vraisemblance\n\nSoit \\{y_1, \\ldots, y_N\\} un échantillon i.i.d. d’une distribution exponentielle de paramètre \\lambda > 0:p(y | \\lambda) = \\lambda e^{-\\lambda y}, \\quad y \\geq 0\n\nÉcrivez la vraisemblance \\mathcal{L}(\\lambda) et la log-vraisemblance \\log \\mathcal{L}(\\lambda).\n\nDérivez l’estimateur du maximum de vraisemblance \\hat{\\lambda}_{\\text{MLE}}.\n\nSi les observations sont y = \\{0.5, 1.2, 0.8, 2.1, 0.3\\}, calculez \\hat{\\lambda}_{\\text{MLE}}.\n\nExercice 4: Fonctions de perte\n\nSoit y = 1 (classe positive) et un score s = f(x) = 2.\n\nCalculez la perte 0-1, la perte logistique, et la perte à charnière.\n\nRépétez pour s = -0.5 (prédiction incorrecte).\n\nTracez les trois fonctions de perte en fonction de y \\cdot s pour y \\cdot s \\in [-3, 3]. Vérifiez que les pertes de substitution majorent la perte 0-1.","type":"content","url":"/learning-problem#exercices","position":67}]}