{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1: Le problème d'apprentissage\n",
    "\n",
    "**IFT6390 - Fondements de l'apprentissage machine**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pierrelux/mlbook/blob/main/exercises/tp1_learning_problem.ipynb)\n",
    "\n",
    "Ce notebook accompagne le [Chapitre 2: Le problème d'apprentissage](https://pierrelux.github.io/mlbook/learning-problem).\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "À la fin de ce TP, vous serez en mesure de:\n",
    "- Calculer l'erreur quadratique moyenne (MSE)\n",
    "- Observer le phénomène de surapprentissage avec des polynômes\n",
    "- Comprendre le compromis biais-variance\n",
    "- Implémenter la régularisation Ridge\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 0: Configuration\n",
    "\n",
    "Exécutez cette cellule pour importer les bibliothèques nécessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Polyfit may be poorly conditioned')\n",
    "\n",
    "# Pour de jolis graphiques\n",
    "plt.rcParams['figure.figsize'] = (8, 5)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Configuration terminee!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1: Les données de freinage\n",
    "\n",
    "Nous utilisons les données classiques d'Ezekiel (1930): la distance de freinage d'un véhicule en fonction de sa vitesse.\n",
    "\n",
    "**Question préliminaire**: Quelle relation attendez-vous entre vitesse et distance de freinage? Linéaire? Quadratique? Autre?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Données de freinage (Ezekiel, 1930): vitesse (mph) vs distance d'arrêt (ft)\n",
    "speed = np.array([4, 4, 7, 7, 8, 9, 10, 10, 10, 11, 11, 12, 12, 12, 12, 13, 13, 13, 13, 14,\n",
    "                  14, 14, 14, 15, 15, 15, 16, 16, 17, 17, 17, 18, 18, 18, 18, 19, 19, 19,\n",
    "                  20, 20, 20, 20, 20, 22, 23, 24, 24, 24, 24, 25], dtype=float)\n",
    "dist = np.array([2, 10, 4, 22, 16, 10, 18, 26, 34, 17, 28, 14, 20, 24, 28, 26, 34, 34, 46,\n",
    "                 26, 36, 60, 80, 20, 26, 54, 32, 40, 32, 40, 50, 42, 56, 76, 84, 36, 46,\n",
    "                 68, 32, 48, 52, 56, 64, 66, 54, 70, 92, 93, 120, 85], dtype=float)\n",
    "\n",
    "print(f\"Nombre d'observations: {len(speed)}\")\n",
    "print(f\"Vitesse: min={speed.min():.0f}, max={speed.max():.0f} mph\")\n",
    "print(f\"Distance: min={dist.min():.0f}, max={dist.max():.0f} ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des données\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(speed, dist, alpha=0.7, s=50)\n",
    "plt.xlabel('Vitesse (mph)')\n",
    "plt.ylabel('Distance de freinage (ft)')\n",
    "plt.title('Données de freinage')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Partie 2: Ajuster un polynôme\n",
    "\n",
    "Nous allons ajuster des polynômes de différents degrés aux données.\n",
    "\n",
    "### 2.1 Ajustement simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuster un polynôme de degré 2\n",
    "degree = 2\n",
    "coeffs = np.polyfit(speed, dist, degree)\n",
    "\n",
    "print(f\"Coefficients du polynôme de degré {degree}:\")\n",
    "for i, c in enumerate(coeffs):\n",
    "    print(f\"  coefficient de x^{degree-i}: {c:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser l'ajustement\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(speed, dist, alpha=0.7, s=50, label='Données')\n",
    "\n",
    "# Grille pour tracer la courbe\n",
    "speed_grid = np.linspace(0, 30, 100)\n",
    "dist_pred = np.polyval(coeffs, speed_grid)\n",
    "plt.plot(speed_grid, dist_pred, 'r-', linewidth=2, label=f'Polynôme degré {degree}')\n",
    "\n",
    "plt.xlabel('Vitesse (mph)')\n",
    "plt.ylabel('Distance de freinage (ft)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1: Calculer l'erreur quadratique moyenne (MSE)\n",
    "\n",
    "L'erreur quadratique moyenne est définie comme:\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**Complétez la fonction ci-dessous:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcule l'erreur quadratique moyenne.\n",
    "    \n",
    "    Args:\n",
    "        y_true: valeurs réelles (array)\n",
    "        y_pred: prédictions (array)\n",
    "    \n",
    "    Returns:\n",
    "        MSE (float)\n",
    "    \"\"\"\n",
    "    # ============================================\n",
    "    # TODO: Complétez cette fonction\n",
    "    # Indice: utilisez np.mean() et l'opérateur **2\n",
    "    # ============================================\n",
    "    \n",
    "    mse = None  # <- Remplacez None par votre code\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de votre fonction\n",
    "predictions = np.polyval(coeffs, speed)\n",
    "mse = compute_mse(dist, predictions)\n",
    "\n",
    "if mse is not None:\n",
    "    print(f\"MSE pour le polynôme de degré {degree}: {mse:.2f}\")\n",
    "    \n",
    "    # Vérification\n",
    "    expected_mse = np.mean((dist - predictions)**2)\n",
    "    if np.isclose(mse, expected_mse):\n",
    "        print(\"Correct!\")\n",
    "    else:\n",
    "        print(f\"Attendu: {expected_mse:.2f}\")\n",
    "else:\n",
    "    print(\"La fonction retourne None. Completez le code!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Cliquez pour voir la solution</summary>\n",
    "\n",
    "```python\n",
    "def compute_mse(y_true, y_pred):\n",
    "    mse = np.mean((y_true - y_pred)**2)\n",
    "    return mse\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2: Explorer differents degres\n",
    "\n",
    "**Modifiez la variable `degree` ci-dessous et observez:**\n",
    "- Comment change la courbe?\n",
    "- Comment change le MSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TODO: Essayez différentes valeurs: 1, 2, 5, 10, 15, 20\n",
    "# ============================================\n",
    "degree = 2  # <- Modifiez cette valeur!\n",
    "\n",
    "# Ajustement\n",
    "coeffs = np.polyfit(speed, dist, degree)\n",
    "predictions = np.polyval(coeffs, speed)\n",
    "mse = np.mean((dist - predictions)**2)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(speed, dist, alpha=0.7, s=50, label='Données')\n",
    "\n",
    "speed_grid = np.linspace(3, 26, 200)\n",
    "pred_grid = np.polyval(coeffs, speed_grid)\n",
    "pred_grid = np.clip(pred_grid, -50, 200)  # Limiter pour la visualisation\n",
    "\n",
    "plt.plot(speed_grid, pred_grid, 'r-', linewidth=2, label=f'Polynôme degré {degree}')\n",
    "plt.xlabel('Vitesse (mph)')\n",
    "plt.ylabel('Distance de freinage (ft)')\n",
    "plt.title(f'Degré {degree} — MSE = {mse:.1f}')\n",
    "plt.legend()\n",
    "plt.ylim(-20, 150)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMSE: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions de reflexion:**\n",
    "1. Quel degré donne le MSE le plus bas?\n",
    "2. Le polynôme de degré 20 semble-t-il raisonnable pour prédire à de nouvelles vitesses?\n",
    "3. C'est quoi le problème avec minimiser uniquement le MSE sur les données d'entraînement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Partie 3: Train/Test Split — Le surapprentissage\n",
    "\n",
    "Pour détecter le **surapprentissage**, nous séparons les données en:\n",
    "- **Ensemble d'entraînement** (70%): pour ajuster le modèle\n",
    "- **Ensemble de test** (30%): pour évaluer la généralisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation train/test\n",
    "np.random.seed(42)  # Pour reproductibilité\n",
    "indices = np.random.permutation(len(speed))\n",
    "n_train = 35\n",
    "\n",
    "train_idx = indices[:n_train]\n",
    "test_idx = indices[n_train:]\n",
    "\n",
    "speed_train, dist_train = speed[train_idx], dist[train_idx]\n",
    "speed_test, dist_test = speed[test_idx], dist[test_idx]\n",
    "\n",
    "print(f\"Entraînement: {len(speed_train)} exemples\")\n",
    "print(f\"Test: {len(speed_test)} exemples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la séparation\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(speed_train, dist_train, alpha=0.7, s=50, label='Entraînement')\n",
    "plt.scatter(speed_test, dist_test, alpha=0.7, s=50, marker='s', label='Test')\n",
    "plt.xlabel('Vitesse (mph)')\n",
    "plt.ylabel('Distance de freinage (ft)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3: Courbe biais-variance\n",
    "\n",
    "**Complétez le code pour calculer l'erreur sur train ET test pour chaque degré:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = range(1, 16)\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for deg in degrees:\n",
    "    # Ajuster sur l'ensemble d'entraînement\n",
    "    coeffs = np.polyfit(speed_train, dist_train, deg)\n",
    "    \n",
    "    # ============================================\n",
    "    # TODO: Calculez les prédictions et erreurs\n",
    "    # ============================================\n",
    "    \n",
    "    # Prédictions sur train\n",
    "    pred_train = None  # <- Complétez avec np.polyval(...)\n",
    "    \n",
    "    # Prédictions sur test\n",
    "    pred_test = None  # <- Complétez avec np.polyval(...)\n",
    "    \n",
    "    # MSE sur train\n",
    "    mse_train = None  # <- Complétez\n",
    "    \n",
    "    # MSE sur test\n",
    "    mse_test = None  # <- Complétez\n",
    "    \n",
    "    train_errors.append(mse_train)\n",
    "    test_errors.append(mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation (exécutez après avoir complété le code ci-dessus)\n",
    "if None not in train_errors and None not in test_errors:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(degrees, train_errors, 'o-', linewidth=2, markersize=8, label='Erreur entraînement')\n",
    "    plt.plot(degrees, test_errors, 's-', linewidth=2, markersize=8, label='Erreur test')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Degré du polynôme (complexité)')\n",
    "    plt.ylabel('MSE (échelle log)')\n",
    "    plt.title('Compromis biais-variance')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(range(1, 16, 2))\n",
    "    plt.show()\n",
    "    \n",
    "    # Trouver le meilleur degré\n",
    "    best_deg = degrees[np.argmin(test_errors)]\n",
    "    print(f\"\\nMeilleur degre (selon erreur test): {best_deg}\")\n",
    "    print(f\"   MSE train: {train_errors[best_deg-1]:.1f}\")\n",
    "    print(f\"   MSE test: {test_errors[best_deg-1]:.1f}\")\n",
    "else:\n",
    "    print(\"Completez le code de l'exercice 3!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Cliquez pour voir la solution</summary>\n",
    "\n",
    "```python\n",
    "# Prédictions sur train\n",
    "pred_train = np.polyval(coeffs, speed_train)\n",
    "\n",
    "# Prédictions sur test\n",
    "pred_test = np.polyval(coeffs, speed_test)\n",
    "\n",
    "# MSE sur train\n",
    "mse_train = np.mean((dist_train - pred_train)**2)\n",
    "\n",
    "# MSE sur test\n",
    "mse_test = np.mean((dist_test - pred_test)**2)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Partie 4: Régularisation Ridge\n",
    "\n",
    "Au lieu de choisir un degré bas, on peut utiliser un polynôme de **haut degré** avec **régularisation**.\n",
    "\n",
    "La régularisation Ridge ajoute une pénalité sur les coefficients:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\theta}}_{\\text{Ridge}} = \\arg\\min_{\\boldsymbol{\\theta}} \\left[ \\sum_{i=1}^{N} (y_i - \\boldsymbol{\\theta}^\\top \\mathbf{x}_i)^2 + \\lambda \\|\\boldsymbol{\\theta}\\|^2 \\right]$$\n",
    "\n",
    "La solution en forme fermée est:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\theta}}_{\\text{Ridge}} = (\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}^\\top \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4: Implementer Ridge Regression\n",
    "\n",
    "**Completez la fonction ci-dessous:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(x, degree):\n",
    "    \"\"\"\n",
    "    Crée la matrice de caractéristiques polynomiales.\n",
    "    \n",
    "    Args:\n",
    "        x: vecteur d'entrées (n,)\n",
    "        degree: degré du polynôme\n",
    "    \n",
    "    Returns:\n",
    "        X: matrice (n, degree+1) avec colonnes [1, x, x², ..., x^degree]\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    X = np.zeros((n, degree + 1))\n",
    "    for d in range(degree + 1):\n",
    "        X[:, d] = x ** d\n",
    "    return X\n",
    "\n",
    "\n",
    "def ridge_regression(X, y, lambda_reg):\n",
    "    \"\"\"\n",
    "    Calcule les coefficients Ridge.\n",
    "    \n",
    "    Args:\n",
    "        X: matrice de features (n, d)\n",
    "        y: vecteur cible (n,)\n",
    "        lambda_reg: coefficient de régularisation\n",
    "    \n",
    "    Returns:\n",
    "        theta: coefficients (d,)\n",
    "    \"\"\"\n",
    "    # ============================================\n",
    "    # TODO: Implémentez la formule Ridge\n",
    "    # theta = (X'X + lambda*I)^(-1) X'y\n",
    "    # \n",
    "    # Indices:\n",
    "    # - X.T pour la transposée\n",
    "    # - @ pour le produit matriciel\n",
    "    # - np.eye(n) pour la matrice identité n×n\n",
    "    # - np.linalg.inv() pour l'inverse\n",
    "    # ============================================\n",
    "    \n",
    "    d = X.shape[1]  # nombre de features\n",
    "    \n",
    "    theta = None  # <- Remplacez par votre code\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de votre implémentation\n",
    "degree = 10\n",
    "lambda_reg = 1.0\n",
    "\n",
    "X_train = polynomial_features(speed_train, degree)\n",
    "theta = ridge_regression(X_train, dist_train, lambda_reg)\n",
    "\n",
    "if theta is not None:\n",
    "    print(f\"Coefficients Ridge (degré={degree}, λ={lambda_reg}):\")\n",
    "    print(f\"  Norme des coefficients: {np.linalg.norm(theta):.2f}\")\n",
    "    \n",
    "    # Prédictions\n",
    "    X_test = polynomial_features(speed_test, degree)\n",
    "    pred_test = X_test @ theta\n",
    "    mse = np.mean((dist_test - pred_test)**2)\n",
    "    print(f\"  MSE test: {mse:.1f}\")\n",
    "    print(\"\\nImplementation correcte!\")\n",
    "else:\n",
    "    print(\"Completez la fonction ridge_regression!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Cliquez pour voir la solution</summary>\n",
    "\n",
    "```python\n",
    "def ridge_regression(X, y, lambda_reg):\n",
    "    d = X.shape[1]\n",
    "    I = np.eye(d)\n",
    "    theta = np.linalg.inv(X.T @ X + lambda_reg * I) @ X.T @ y\n",
    "    return theta\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 5: Trouver le meilleur lambda\n",
    "\n",
    "**Explorez différentes valeurs de λ pour un polynôme de degré 10:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 10\n",
    "X_train = polynomial_features(speed_train, degree)\n",
    "X_test = polynomial_features(speed_test, degree)\n",
    "\n",
    "lambdas = [0, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "results = []\n",
    "\n",
    "print(f\"{'λ':>10} | {'MSE Train':>10} | {'MSE Test':>10} | {'||θ||':>10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for lam in lambdas:\n",
    "    theta = ridge_regression(X_train, dist_train, lam)\n",
    "    if theta is not None:\n",
    "        mse_train = np.mean((dist_train - X_train @ theta)**2)\n",
    "        mse_test = np.mean((dist_test - X_test @ theta)**2)\n",
    "        norm_theta = np.linalg.norm(theta)\n",
    "        results.append((lam, mse_train, mse_test, norm_theta))\n",
    "        print(f\"{lam:>10.3f} | {mse_train:>10.1f} | {mse_test:>10.1f} | {norm_theta:>10.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'effet de λ\n",
    "if len(results) > 0:\n",
    "    lambdas_plot = [r[0] for r in results]\n",
    "    train_plot = [r[1] for r in results]\n",
    "    test_plot = [r[2] for r in results]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.semilogx([max(l, 1e-4) for l in lambdas_plot], train_plot, 'o-', label='Train')\n",
    "    plt.semilogx([max(l, 1e-4) for l in lambdas_plot], test_plot, 's-', label='Test')\n",
    "    plt.xlabel('λ (échelle log)')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.title(f'Effet de la régularisation Ridge (degré={degree})')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "1. Quelle valeur de λ minimise l'erreur test?\n",
    "2. Que se passe-t-il quand λ → 0? Et quand λ → ∞?\n",
    "3. Comment la norme des coefficients ||θ|| change avec λ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Partie 5: SVD et la solution MCO\n",
    "\n",
    "La **decomposition en valeurs singulieres** (SVD) offre une autre facon de comprendre les solutions MCO et Ridge. Cette section explore visuellement pourquoi Ridge fonctionne.\n",
    "\n",
    "### Qu'est-ce que la SVD?\n",
    "\n",
    "Toute matrice $\\mathbf{X}$ peut s'ecrire:\n",
    "\n",
    "$$\\mathbf{X} = \\mathbf{U} \\mathbf{D} \\mathbf{V}^\\top$$\n",
    "\n",
    "- $\\mathbf{V}$: directions principales dans l'espace des features\n",
    "- $\\mathbf{D}$: valeurs singulieres $d_1 \\geq d_2 \\geq \\cdots \\geq d_p$ (amplitudes)\n",
    "- $\\mathbf{U}$: directions correspondantes dans l'espace des observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creons des donnees 2D pour visualiser la SVD\n",
    "np.random.seed(123)\n",
    "n = 50\n",
    "\n",
    "# Donnees avec correlation (une direction forte, une faible)\n",
    "X_2d = np.random.randn(n, 2)\n",
    "# Ajouter de la correlation\n",
    "X_2d[:, 1] = 0.9 * X_2d[:, 0] + 0.3 * X_2d[:, 1]\n",
    "\n",
    "# SVD\n",
    "U, s, Vt = np.linalg.svd(X_2d, full_matrices=False)\n",
    "V = Vt.T\n",
    "\n",
    "print(\"Valeurs singulieres:\")\n",
    "print(f\"  d1 = {s[0]:.2f} (direction forte)\")\n",
    "print(f\"  d2 = {s[1]:.2f} (direction faible)\")\n",
    "print(f\"  Ratio d1/d2 = {s[0]/s[1]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les directions principales\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Gauche: donnees et directions principales\n",
    "ax = axes[0]\n",
    "ax.scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.5, s=30)\n",
    "\n",
    "# Tracer les directions principales (vecteurs propres de X'X)\n",
    "origin = np.mean(X_2d, axis=0)\n",
    "for i, (sv, color, label) in enumerate(zip(s, ['C0', 'C1'], ['v1 (forte)', 'v2 (faible)'])):\n",
    "    direction = V[:, i] * sv / 5  # Echelle pour visualisation\n",
    "    ax.annotate('', xy=origin + direction, xytext=origin,\n",
    "                arrowprops=dict(arrowstyle='->', color=color, lw=3))\n",
    "    ax.annotate(f'{label}\\nd={sv:.1f}', xy=origin + direction * 1.2, fontsize=10, color=color)\n",
    "\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_title('Donnees et directions principales (SVD)')\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Droite: valeurs singulieres\n",
    "ax = axes[1]\n",
    "ax.bar([1, 2], s, color=['C0', 'C1'], alpha=0.7)\n",
    "ax.set_xticks([1, 2])\n",
    "ax.set_xticklabels(['$d_1$', '$d_2$'])\n",
    "ax.set_ylabel('Valeur singuliere')\n",
    "ax.set_title('Spectre des valeurs singulieres')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facteurs de retrecissement Ridge\n",
    "\n",
    "La solution Ridge s'ecrit:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\theta}}_{\\text{ridge}} = \\sum_{j=1}^d \\underbrace{\\frac{d_j^2}{d_j^2 + \\lambda}}_{\\text{retrecissement}} \\cdot \\frac{\\mathbf{u}_j^\\top \\mathbf{y}}{d_j} \\cdot \\mathbf{v}_j$$\n",
    "\n",
    "Le facteur $\\frac{d_j^2}{d_j^2 + \\lambda}$ est toujours < 1, et il est **plus petit pour les directions faibles** (petit $d_j$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les facteurs de retrecissement\n",
    "lambdas = [0.01, 0.1, 1, 10]\n",
    "d_values = np.array([s[0], s[1]])  # Nos deux valeurs singulieres\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "x_pos = np.arange(len(d_values))\n",
    "width = 0.2\n",
    "\n",
    "for i, lam in enumerate(lambdas):\n",
    "    shrinkage = d_values**2 / (d_values**2 + lam)\n",
    "    bars = ax.bar(x_pos + i * width, shrinkage, width, label=f'$\\\\lambda$ = {lam}', alpha=0.8)\n",
    "\n",
    "ax.set_xticks(x_pos + width * 1.5)\n",
    "ax.set_xticklabels([f'$d_1$ = {s[0]:.1f}\\n(forte)', f'$d_2$ = {s[1]:.1f}\\n(faible)'])\n",
    "ax.set_ylabel('Facteur de retrecissement $d_j^2 / (d_j^2 + \\\\lambda)$')\n",
    "ax.set_title('Ridge retrecit plus les directions faibles')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.axhline(1, color='gray', linestyle='--', alpha=0.5, label='MCO (pas de retrecissement)')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation: La direction faible (d2) est beaucoup plus retrecit que la direction forte (d1).\")\n",
    "print(\"C'est exactement ce qu'on veut: reduire l'incertitude la ou elle est grande.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Pourquoi diviser par une petite valeur singuliere $d_j$ amplifie-t-il le bruit dans MCO?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Partie 6: ERM comme integration de Monte Carlo\n",
    "\n",
    "Le **risque** mesure la performance moyenne sur toutes les donnees possibles:\n",
    "\n",
    "$$\\mathcal{R}(f) = \\mathbb{E}_{(\\mathbf{X},Y) \\sim p}[\\ell(Y, f(\\mathbf{X}))] = \\int \\ell(y, f(\\mathbf{x})) \\, p(\\mathbf{x}, y) \\, d\\mathbf{x} \\, dy$$\n",
    "\n",
    "Probleme: cette integrale est **impossible a calculer** car:\n",
    "1. On ne connait pas $p(\\mathbf{x}, y)$\n",
    "2. Meme si on la connaissait, l'integrale en haute dimension est intractable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La malediction de la dimensionnalite pour l'integration numerique\n",
    "dimensions = [1, 2, 3, 5, 10, 20, 50, 100]\n",
    "points_per_dim = 10  # 10 points par dimension pour une grille\n",
    "\n",
    "# On trace log10 du nombre de points (evite les overflows)\n",
    "log_grid_points = [d for d in dimensions]  # log10(10^d) = d\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "colors = plt.cm.Reds(np.linspace(0.3, 0.9, len(dimensions)))\n",
    "bars = ax.bar(range(len(dimensions)), log_grid_points, color=colors)\n",
    "\n",
    "ax.set_xticks(range(len(dimensions)))\n",
    "ax.set_xticklabels([f'd={d}' for d in dimensions])\n",
    "ax.set_xlabel('Dimension')\n",
    "ax.set_ylabel('$\\\\log_{10}$(nombre de points)')\n",
    "ax.set_title('Integration numerique: explosion combinatoire')\n",
    "\n",
    "# Annotations\n",
    "ax.axhline(10, color='blue', linestyle='--', alpha=0.7)\n",
    "ax.text(0.5, 11, '10 milliards ($10^{10}$, limite pratique)', color='blue', fontsize=10)\n",
    "\n",
    "ax.axhline(80, color='red', linestyle='--', alpha=0.7)\n",
    "ax.text(4, 82, 'Atomes dans l\\'univers (~$10^{80}$)', color='red', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAvec seulement 10 points par dimension:\")\n",
    "print(f\"  d=10: 10^10 = 10 milliards de points\")\n",
    "print(f\"  d=100: 10^100 points (impossible!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La solution: Monte Carlo\n",
    "\n",
    "Le **risque empirique** est une estimation Monte Carlo du vrai risque:\n",
    "\n",
    "$$\\hat{\\mathcal{R}}(f) = \\frac{1}{N} \\sum_{i=1}^{N} \\ell(y_i, f(\\mathbf{x}_i))$$\n",
    "\n",
    "| Methode | Complexite | Exigence |\n",
    "|---------|------------|----------|\n",
    "| Quadrature | $O(M^d)$ | Connaitre $p(\\mathbf{x},y)$ |\n",
    "| **Monte Carlo** | $O(N)$ | Avoir des echantillons |\n",
    "\n",
    "La complexite Monte Carlo est **independante de la dimension**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Monte Carlo converge independamment de d\n",
    "def true_integral_1d():\n",
    "    \"\"\"Integrale de x^2 sur [0,1] = 1/3\"\"\"\n",
    "    return 1/3\n",
    "\n",
    "def monte_carlo_estimate(n_samples, dim=1):\n",
    "    \"\"\"Estime E[||x||^2/d] pour x uniforme sur [0,1]^d\"\"\"\n",
    "    # Pour x uniforme sur [0,1]^d, E[x_i^2] = 1/3 pour chaque composante\n",
    "    # Donc E[||x||^2/d] = 1/3\n",
    "    samples = np.random.uniform(0, 1, (n_samples, dim))\n",
    "    return np.mean(np.sum(samples**2, axis=1) / dim)\n",
    "\n",
    "# Comparer convergence pour differentes dimensions\n",
    "np.random.seed(42)\n",
    "n_values = [10, 50, 100, 500, 1000, 5000]\n",
    "dims = [1, 10, 100]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "true_value = 1/3\n",
    "\n",
    "for dim in dims:\n",
    "    errors = []\n",
    "    for n in n_values:\n",
    "        estimates = [monte_carlo_estimate(n, dim) for _ in range(50)]  # 50 repetitions\n",
    "        errors.append(np.std(estimates))\n",
    "    ax.loglog(n_values, errors, 'o-', label=f'd = {dim}', markersize=8)\n",
    "\n",
    "# Ligne de reference: 1/sqrt(N)\n",
    "n_ref = np.array(n_values)\n",
    "ax.loglog(n_ref, 0.5 / np.sqrt(n_ref), 'k--', alpha=0.5, label='$1/\\\\sqrt{N}$')\n",
    "\n",
    "ax.set_xlabel('Nombre d\\'echantillons N')\n",
    "ax.set_ylabel('Ecart-type de l\\'estimation')\n",
    "ax.set_title('Convergence Monte Carlo: independante de la dimension!')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation: Les trois courbes se superposent!\")\n",
    "print(\"La precision Monte Carlo depend de N, pas de d.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Pourquoi la complexite Monte Carlo est-elle $O(N)$ independamment de la dimension $d$?\n",
    "\n",
    "**Reponse a reflechir**: Parce que chaque echantillon $(\\mathbf{x}_i, y_i)$ contribue une evaluation de la perte, peu importe la dimension de $\\mathbf{x}_i$. On ne discretise pas l'espace; on utilise les echantillons tels quels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Recapitulatif\n",
    "\n",
    "Dans ce TP, vous avez appris:\n",
    "\n",
    "1. **MSE**: L'erreur quadratique moyenne mesure la qualite des predictions\n",
    "\n",
    "2. **Surapprentissage**: Un modele trop complexe memorise le bruit\n",
    "   - Erreur train diminue mais erreur test augmente\n",
    "   \n",
    "3. **Compromis biais-variance**: \n",
    "   - Modele simple = biais eleve (sous-apprentissage)\n",
    "   - Modele complexe = variance elevee (surapprentissage)\n",
    "   \n",
    "4. **Regularisation Ridge**: Penalise les grands coefficients\n",
    "   - Permet d'utiliser des modeles complexes sans surapprentissage\n",
    "   - Lambda controle la force de la regularisation\n",
    "\n",
    "5. **SVD et Ridge**: Ridge retrecit plus les directions faibles (petites valeurs singulieres)\n",
    "\n",
    "6. **ERM = Monte Carlo**: Le risque empirique est une estimation Monte Carlo du vrai risque, avec complexite independante de la dimension\n",
    "\n",
    "---\n",
    "\n",
    "**Pour aller plus loin**: [Chapitre complet sur le site du cours](https://pierrelux.github.io/mlbook/learning-problem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
