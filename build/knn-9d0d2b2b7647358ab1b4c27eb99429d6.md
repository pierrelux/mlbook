# K plus proches voisins

```{admonition} Objectifs d'apprentissage
:class: note

À la fin de ce chapitre, vous serez en mesure de:
- Expliquer le fonctionnement de l'algorithme des k plus proches voisins
- Définir et appliquer différentes fonctions de distance
- Analyser l'effet du paramètre k sur le compromis biais-variance
- Expliquer le fléau de la dimensionnalité et ses conséquences
- Implémenter l'algorithme k-ppv pour la classification et la régression
```

## Un premier algorithme de classification

Supposons que vous développiez une application mobile qui identifie les espèces de fleurs à partir de photos. Un utilisateur pointe son téléphone vers une fleur dans un jardin, et l'application doit prédire s'il s'agit d'une iris setosa, versicolor ou virginica. Vous disposez d'une base de données de 150 fleurs pour lesquelles un botaniste a mesuré quatre caractéristiques (longueur et largeur des pétales et sépales) et identifié l'espèce.

Comment procéder? Une approche naturelle consiste à comparer la nouvelle fleur aux exemples connus. Si ses mesures ressemblent à celles d'une iris setosa dans la base de données, il est raisonnable de prédire qu'il s'agit aussi d'une setosa. Cette intuition est le fondement des k plus proches voisins (k-ppv, en anglais *k-nearest neighbors*).

L'algorithme est simple. Pour classifier une nouvelle fleur:

1. Calculer la distance entre cette fleur et chaque fleur de la base de données
2. Identifier les $k$ fleurs les plus proches
3. Prédire l'espèce la plus fréquente parmi ces $k$ voisins

Avec $k = 5$, si les cinq fleurs les plus similaires sont trois setosas et deux versicolors, nous prédisons setosa. L'algorithme ne fait aucune hypothèse sur la forme de la frontière entre les classes. Il se contente de consulter les exemples les plus proches et de prendre une décision par vote majoritaire.

Cette simplicité a un prix: il faut conserver toute la base de données en mémoire et la parcourir à chaque nouvelle requête. Le k-ppv est une méthode **à mémoire** (*memory-based* ou *instance-based*): l'entraînement consiste uniquement à stocker les données, et le travail se fait au moment de l'inférence.

## Formulation de l'algorithme

Formalisons cette procédure. Soit $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N$ un ensemble d'entraînement où chaque $x_i \in \mathbb{R}^d$ est un vecteur de caractéristiques et $y_i$ est l'étiquette de classe correspondante. Pour une nouvelle observation $x$, nous définissons $\mathcal{N}_k(x)$ comme l'ensemble des indices des $k$ points de $\mathcal{D}$ les plus proches de $x$ selon une métrique donnée.

La prédiction du k-ppv est la classe majoritaire parmi les voisins:

$$
\hat{y} = \arg\max_{c} \sum_{i \in \mathcal{N}_k(x)} \mathbb{1}_{y_i = c}
$$

La somme compte le nombre de voisins appartenant à chaque classe $c$, et nous retenons la classe qui apparaît le plus souvent.

Cette formulation admet une interprétation probabiliste. Plutôt que de retourner une seule classe, nous pouvons estimer la probabilité de chaque classe:

$$
p(y = c \mid x, \mathcal{D}) = \frac{1}{k} \sum_{i \in \mathcal{N}_k(x)} \mathbb{1}_{y_i = c}
$$

Cette probabilité est la proportion de voisins appartenant à la classe $c$. La prédiction déterministe correspond au mode de cette distribution. Si trois des cinq voisins sont des setosas, nous estimons $p(\text{setosa} \mid x) = 0.6$.

## La notion de proximité

L'algorithme repose sur la capacité à identifier les exemples "proches". Mais que signifie être proche? Le choix de la fonction de distance encode nos hypothèses sur la structure du problème.

Une fonction de distance (ou métrique) $d: \mathcal{X} \times \mathcal{X} \to [0, \infty)$ doit satisfaire trois propriétés. Elle est nulle si et seulement si les deux points sont identiques. Elle est symétrique: la distance de $x$ à $y$ est la même que de $y$ à $x$. Elle satisfait l'inégalité triangulaire: le chemin direct entre deux points est toujours le plus court.

Le choix le plus courant est la **distance euclidienne**, qui mesure la longueur du segment de droite entre deux points:

$$
d_2(x, y) = \sqrt{\sum_{j=1}^{d} (x_j - y_j)^2} = \|x - y\|_2
$$

La **distance de Manhattan** mesure plutôt la distance parcourue en suivant une grille, comme dans les rues d'une ville:

$$
d_1(x, y) = \sum_{j=1}^{d} |x_j - y_j| = \|x - y\|_1
$$

Ces deux distances font partie d'une famille plus générale, les distances $\ell_p$, définies par $\|x - y\|_p = \left(\sum_j |x_j - y_j|^p\right)^{1/p}$. Les cas $p = 1$ et $p = 2$ correspondent aux distances de Manhattan et euclidienne.

La distance euclidienne traite toutes les dimensions de manière égale. Si une caractéristique varie de 0 à 1 et une autre de 0 à 1000, la seconde dominera le calcul de distance. Une solution est de normaliser les variables avant d'appliquer l'algorithme.

Une approche plus sophistiquée est la **distance de Mahalanobis**:

$$
d_M(x, y) = \sqrt{(x - y)^\top M (x - y)}
$$

où $M$ est une matrice définie positive. Si $M$ est la matrice identité, nous retrouvons la distance euclidienne. Si $M = \Sigma^{-1}$ où $\Sigma$ est la matrice de covariance des données, la distance de Mahalanobis normalise les variables et supprime les corrélations. Deux points éloignés dans une direction de faible variance sont considérés plus différents que deux points également éloignés dans une direction de forte variance.

## L'effet du paramètre k

Le choix de $k$ contrôle la complexité du modèle. Ce paramètre illustre un compromis fondamental en apprentissage machine: le compromis biais-variance.

Considérons le cas extrême $k = 1$. Chaque nouvelle observation est classifiée selon l'étiquette de son plus proche voisin. Cette approche est très sensible aux données d'entraînement: si un exemple est mal étiqueté ou s'il s'agit d'une valeur aberrante, toute une région de l'espace sera mal classifiée. La frontière de décision est irrégulière, avec de nombreuses petites "îles" correspondant aux points isolés. Le modèle a une faible erreur d'entraînement (en fait, exactement zéro, car chaque point est son propre plus proche voisin) mais peut mal généraliser.

À l'autre extrême, $k = N$ signifie que tous les points d'entraînement sont des voisins. La prédiction est alors la classe majoritaire globale, indépendamment de l'entrée $x$. Le modèle est très simple mais ignore complètement la structure locale des données.

Entre ces deux extrêmes, augmenter $k$ a pour effet de lisser la frontière de décision. Avec un grand $k$, les petites variations locales sont moyennées et la frontière devient plus régulière. Cela réduit la variance (les prédictions sont plus stables face aux fluctuations des données) mais peut introduire un biais (le modèle peut manquer des structures locales légitimes).

Comment choisir $k$? En pratique, on trace la courbe d'erreur de validation en fonction de $k$. L'erreur est généralement élevée pour $k$ très petit (surapprentissage) et pour $k$ très grand (sous-apprentissage), avec un minimum quelque part entre les deux. On choisit souvent des valeurs impaires pour éviter les égalités dans les votes. Une règle empirique suggère $k \approx \sqrt{N}$, mais la validation croisée reste la méthode la plus fiable.

## Le diagramme de Voronoï

Le cas $k = 1$ induit une structure géométrique qui mérite d'être étudiée. Chaque point d'entraînement définit une région de l'espace: l'ensemble des points plus proches de lui que de tout autre point d'entraînement. Cette partition de l'espace s'appelle le **diagramme de Voronoï**.

La cellule $V_i$ associée au point $x_i$ est définie par:

$$
V_i = \{x \in \mathbb{R}^d : d(x, x_i) \leq d(x, x_j) \text{ pour tout } j \neq i\}
$$

En deux dimensions, les frontières entre cellules sont des segments de droite. En dimension supérieure, ce sont des hyperplans. Avec le 1-ppv, la frontière de décision suit exactement le diagramme de Voronoï: tout point dans la cellule $V_i$ est classifié selon l'étiquette de $x_i$.

Cette structure géométrique permet de visualiser le comportement du classificateur. Elle montre aussi pourquoi le 1-ppv peut être problématique: un point d'entraînement mal étiqueté crée une cellule entière où les prédictions seront incorrectes.

## Le fléau de la dimensionnalité

Les k-ppv fonctionnent bien en basse dimension, mais leur performance se dégrade lorsque le nombre de caractéristiques augmente. Ce phénomène, appelé **fléau de la dimensionnalité** (*curse of dimensionality*), affecte toutes les méthodes basées sur la notion de voisinage.

Pour comprendre le problème, considérons des points uniformément distribués dans un hypercube $[0, 1]^d$. En une dimension, pour capturer 10% des points, il suffit d'un intervalle de longueur 0.1. En deux dimensions, il faut un carré de côté $\sqrt{0.1} \approx 0.32$ pour capturer la même fraction. En dimension 100, il faut un hypercube de côté $0.1^{1/100} \approx 0.98$, qui couvre presque tout l'espace.

| Dimension $d$ | Côté $r$ pour capturer 10% des points |
|---------------|---------------------------------------|
| 1 | 0.10 |
| 2 | 0.32 |
| 5 | 0.63 |
| 10 | 0.79 |
| 100 | 0.98 |

En haute dimension, les "voisins" ne sont plus vraiment proches. Pour qu'un voisinage contienne une fraction raisonnable des données, il doit s'étendre sur presque tout l'espace. La notion de localité perd son sens.

Ce phénomène a plusieurs conséquences. La distance au plus proche voisin augmente avec la dimension, et tous les voisins deviennent approximativement équidistants. La distinction entre le plus proche voisin et le dixième plus proche voisin s'estompe. Pour maintenir une densité constante de points, le nombre d'exemples requis croît exponentiellement avec la dimension.

Plusieurs stratégies atténuent ce problème. La réduction de dimension (PCA, sélection de variables) projette les données dans un espace de dimension plus faible. Les distances adaptatives, comme la distance de Mahalanobis ou l'apprentissage de métrique, peuvent mieux refléter la structure des données. Mais fondamentalement, les méthodes de voisinage deviennent moins efficaces à mesure que la dimension augmente.

## Extension à la régression

L'algorithme k-ppv s'adapte naturellement aux problèmes de régression, où la cible $y$ est continue plutôt que discrète. Plutôt que de prendre un vote majoritaire, nous moyennons les valeurs cibles des voisins:

$$
\hat{y} = \frac{1}{k} \sum_{i \in \mathcal{N}_k(x)} y_i
$$

Cette moyenne locale estime l'espérance conditionnelle $\mathbb{E}[Y \mid X = x]$. L'idée est la même qu'en classification: les points proches dans l'espace des entrées devraient avoir des sorties similaires.

Une variante pondère les contributions des voisins selon leur distance. Les voisins plus proches ont plus d'influence sur la prédiction:

$$
\hat{y} = \frac{\sum_{i \in \mathcal{N}_k(x)} w_i \, y_i}{\sum_{i \in \mathcal{N}_k(x)} w_i}
$$

où $w_i = 1/d(x, x_i)$ ou $w_i = \exp(-d(x, x_i)^2/\sigma^2)$. Cette pondération adoucit la frontière du voisinage: plutôt qu'une coupure nette entre voisins et non-voisins, l'influence décroît progressivement avec la distance.

## Complexité computationnelle

L'entraînement d'un k-ppv est trivial: il suffit de stocker les données. La complexité est $O(N)$ pour copier les $N$ exemples, ou $O(1)$ si nous conservons simplement une référence.

L'inférence est plus coûteuse. Pour chaque nouvelle requête, nous devons calculer la distance à tous les exemples d'entraînement, ce qui requiert $O(Nd)$ opérations. Il faut ensuite identifier les $k$ plus petites distances, ce qui peut se faire en $O(N)$ avec un algorithme de sélection ou $O(N \log k)$ avec un tas.

Pour de grands ensembles de données, ce coût devient prohibitif. Des structures de données spécialisées accélèrent la recherche de voisins. Les arbres k-d partitionnent l'espace et permettent une recherche en $O(\log N)$ en moyenne, mais leur efficacité se dégrade en haute dimension. Le hachage sensible à la localité (LSH) permet une recherche approximative en temps sous-linéaire. Les méthodes modernes d'indexation par graphe, comme HNSW, offrent un bon compromis entre précision et vitesse.

## Implémentation

Voici une implémentation en Python qui suit directement la description de l'algorithme:

```python
import numpy as np

def knn_classify(X_train, y_train, X_test, k=3):
    """Classification par k plus proches voisins."""
    predictions = []
    for x in X_test:
        # Calculer les distances à tous les points d'entraînement
        distances = np.sqrt(np.sum((X_train - x)**2, axis=1))
        
        # Trouver les indices des k plus proches
        k_nearest_idx = np.argsort(distances)[:k]
        
        # Vote majoritaire
        k_nearest_labels = y_train[k_nearest_idx]
        unique, counts = np.unique(k_nearest_labels, return_counts=True)
        predictions.append(unique[np.argmax(counts)])
        
    return np.array(predictions)
```

L'implémentation pour la régression remplace le vote majoritaire par une moyenne:

```python
def knn_regression(X_train, y_train, X_test, k=3):
    """Régression par k plus proches voisins."""
    predictions = []
    for x in X_test:
        distances = np.sqrt(np.sum((X_train - x)**2, axis=1))
        k_nearest_idx = np.argsort(distances)[:k]
        k_nearest_values = y_train[k_nearest_idx]
        predictions.append(np.mean(k_nearest_values))
        
    return np.array(predictions)
```

Ces implémentations sont pédagogiques. En pratique, des bibliothèques comme scikit-learn offrent des implémentations optimisées avec des structures de données efficaces pour la recherche de voisins.

## Méthodes non paramétriques et leurs limites

Les k-ppv sont une méthode **non paramétrique**: la complexité du modèle croît avec la taille des données. Lors du déploiement, il faut conserver l'ensemble d'entraînement en mémoire. Les données *sont* le modèle.

Cette caractéristique a des avantages. L'algorithme peut capturer des frontières de décision arbitrairement complexes sans faire d'hypothèses sur leur forme. Il s'adapte automatiquement à la structure locale des données. Les nouvelles données peuvent être ajoutées sans réentraînement.

Mais elle a aussi des inconvénients majeurs. Le coût d'inférence est $O(Nd)$ par requête, ce qui devient prohibitif pour de grands ensembles de données. Le stockage de toutes les données d'entraînement peut être impraticable. Et comme nous l'avons vu, la méthode souffre du fléau de la dimensionnalité.

| | Non paramétrique | Paramétrique |
|--|------------------|--------------|
| **Modèle** | Les données elles-mêmes | Un vecteur $\theta \in \mathbb{R}^p$ |
| **Complexité** | Croît avec $N$ | Fixe (indépendante de $N$) |
| **Inférence** | Requiert les données | Requiert seulement $\theta$ |
| **Exemples** | k-ppv, processus gaussiens | Régression linéaire, réseaux de neurones |

À l'opposé, les méthodes **paramétriques** distillent l'information des données dans un vecteur de paramètres de taille fixe. Un réseau de neurones peut être entraîné sur des milliards d'exemples, mais lors de l'inférence, seuls ses paramètres sont nécessaires. Un modèle de langage comme GPT, bien qu'entraîné sur une portion significative d'internet, ne garde pas ces textes en mémoire. Déployer un tel modèle serait impossible s'il fallait stocker toutes les données d'entraînement.

## Résumé

Les k plus proches voisins constituent une introduction naturelle à l'apprentissage supervisé. L'algorithme est intuitif: pour classifier un nouvel exemple, nous consultons les exemples similaires que nous avons déjà vus et prenons une décision par vote. Cette simplicité permet de l'implémenter en quelques lignes de code et d'en comprendre le comportement.

Le paramètre $k$ contrôle le compromis entre un modèle très flexible qui risque de surapprendre ($k$ petit) et un modèle trop rigide qui ignore la structure locale ($k$ grand). Le choix de la fonction de distance encode nos hypothèses sur ce que signifie "similaire" dans le contexte du problème. Et le fléau de la dimensionnalité nous rappelle que les intuitions géométriques de basse dimension ne se généralisent pas toujours.

Les k-ppv illustrent parfaitement la tension entre mémorisation et généralisation: avec $k=1$, le modèle mémorise les données et atteint une erreur d'entraînement nulle, mais généralise mal. Cette méthode est non paramétrique: les données sont le modèle, ce qui implique un coût mémoire et computationnel à l'inférence qui croît avec $N$.

Ces limitations motivent une approche différente. Plutôt que de garder toutes les données en mémoire, nous pouvons chercher à *distiller* l'information des données dans un ensemble fixe de paramètres. Le chapitre suivant formalise cette idée: l'apprentissage devient un problème d'optimisation où nous cherchons les paramètres qui minimisent une fonction de perte sur les données d'entraînement.
