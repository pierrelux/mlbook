{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1: Le probleme d'apprentissage\n",
    "\n",
    "**IFT6390 - Fondements de l'apprentissage machine**\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pierrelux/mlbook/blob/main/exercises/tp1_learning_problem.ipynb)\n",
    "\n",
    "Ce notebook accompagne le [Chapitre 2: Le probleme d'apprentissage](https://pierrelux.github.io/mlbook/learning-problem).\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "√Ä la fin de ce TP, vous serez en mesure de:\n",
    "- Calculer l'erreur quadratique moyenne (MSE)\n",
    "- Observer le ph√©nom√®ne de surapprentissage avec des polyn√¥mes\n",
    "- Comprendre le compromis biais-variance\n",
    "- Impl√©menter la r√©gularisation Ridge\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 0: Configuration\n",
    "\n",
    "Ex√©cutez cette cellule pour importer les biblioth√®ques n√©cessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Polyfit may be poorly conditioned')\n",
    "\n",
    "# Pour de jolis graphiques\n",
    "plt.rcParams['figure.figsize'] = (8, 5)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"‚úÖ Configuration termin√©e!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie 1: Les donn√©es de freinage\n",
    "\n",
    "Nous utilisons les donn√©es classiques d'Ezekiel (1930): la distance de freinage d'un v√©hicule en fonction de sa vitesse.\n",
    "\n",
    "**Question pr√©liminaire**: Quelle relation attendez-vous entre vitesse et distance de freinage? Lin√©aire? Quadratique? Autre?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donn√©es de freinage (Ezekiel, 1930): vitesse (mph) vs distance d'arr√™t (ft)\n",
    "speed = np.array([4, 4, 7, 7, 8, 9, 10, 10, 10, 11, 11, 12, 12, 12, 12, 13, 13, 13, 13, 14,\n",
    "                  14, 14, 14, 15, 15, 15, 16, 16, 17, 17, 17, 18, 18, 18, 18, 19, 19, 19,\n",
    "                  20, 20, 20, 20, 20, 22, 23, 24, 24, 24, 24, 25], dtype=float)\n",
    "dist = np.array([2, 10, 4, 22, 16, 10, 18, 26, 34, 17, 28, 14, 20, 24, 28, 26, 34, 34, 46,\n",
    "                 26, 36, 60, 80, 20, 26, 54, 32, 40, 32, 40, 50, 42, 56, 76, 84, 36, 46,\n",
    "                 68, 32, 48, 52, 56, 64, 66, 54, 70, 92, 93, 120, 85], dtype=float)\n",
    "\n",
    "print(f\"Nombre d'observations: {len(speed)}\")\n",
    "print(f\"Vitesse: min={speed.min():.0f}, max={speed.max():.0f} mph\")\n",
    "print(f\"Distance: min={dist.min():.0f}, max={dist.max():.0f} ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des donn√©es\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(speed, dist, alpha=0.7, s=50)\n",
    "plt.xlabel('Vitesse (mph)')\n",
    "plt.ylabel('Distance de freinage (ft)')\n",
    "plt.title('Donn√©es de freinage')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Partie 2: Ajuster un polyn√¥me\n",
    "\n",
    "Nous allons ajuster des polyn√¥mes de diff√©rents degr√©s aux donn√©es.\n",
    "\n",
    "### 2.1 Ajustement simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuster un polyn√¥me de degr√© 2\n",
    "degree = 2\n",
    "coeffs = np.polyfit(speed, dist, degree)\n",
    "\n",
    "print(f\"Coefficients du polyn√¥me de degr√© {degree}:\")\n",
    "for i, c in enumerate(coeffs):\n",
    "    print(f\"  coefficient de x^{degree-i}: {c:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser l'ajustement\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(speed, dist, alpha=0.7, s=50, label='Donn√©es')\n",
    "\n",
    "# Grille pour tracer la courbe\n",
    "speed_grid = np.linspace(0, 30, 100)\n",
    "dist_pred = np.polyval(coeffs, speed_grid)\n",
    "plt.plot(speed_grid, dist_pred, 'r-', linewidth=2, label=f'Polyn√¥me degr√© {degree}')\n",
    "\n",
    "plt.xlabel('Vitesse (mph)')\n",
    "plt.ylabel('Distance de freinage (ft)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercice 1: Calculer l'erreur quadratique moyenne (MSE)\n",
    "\n",
    "L'erreur quadratique moyenne est d√©finie comme:\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**Compl√©tez la fonction ci-dessous:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcule l'erreur quadratique moyenne.\n",
    "    \n",
    "    Args:\n",
    "        y_true: valeurs r√©elles (array)\n",
    "        y_pred: pr√©dictions (array)\n",
    "    \n",
    "    Returns:\n",
    "        MSE (float)\n",
    "    \"\"\"\n",
    "    # ============================================\n",
    "    # TODO: Compl√©tez cette fonction\n",
    "    # Indice: utilisez np.mean() et l'op√©rateur **2\n",
    "    # ============================================\n",
    "    \n",
    "    mse = None  # <- Remplacez None par votre code\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de votre fonction\n",
    "predictions = np.polyval(coeffs, speed)\n",
    "mse = compute_mse(dist, predictions)\n",
    "\n",
    "if mse is not None:\n",
    "    print(f\"MSE pour le polyn√¥me de degr√© {degree}: {mse:.2f}\")\n",
    "    \n",
    "    # V√©rification\n",
    "    expected_mse = np.mean((dist - predictions)**2)\n",
    "    if np.isclose(mse, expected_mse):\n",
    "        print(\"‚úÖ Correct!\")\n",
    "    else:\n",
    "        print(f\"‚ùå Attendu: {expected_mse:.2f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è La fonction retourne None. Compl√©tez le code!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Cliquez pour voir la solution</summary>\n",
    "\n",
    "```python\n",
    "def compute_mse(y_true, y_pred):\n",
    "    mse = np.mean((y_true - y_pred)**2)\n",
    "    return mse\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercice 2: Explorer diff√©rents degr√©s\n",
    "\n",
    "**Modifiez la variable `degree` ci-dessous et observez:**\n",
    "- Comment change la courbe?\n",
    "- Comment change le MSE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TODO: Essayez diff√©rentes valeurs: 1, 2, 5, 10, 15, 20\n",
    "# ============================================\n",
    "degree = 2  # <- Modifiez cette valeur!\n",
    "\n",
    "# Ajustement\n",
    "coeffs = np.polyfit(speed, dist, degree)\n",
    "predictions = np.polyval(coeffs, speed)\n",
    "mse = np.mean((dist - predictions)**2)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(speed, dist, alpha=0.7, s=50, label='Donn√©es')\n",
    "\n",
    "speed_grid = np.linspace(3, 26, 200)\n",
    "pred_grid = np.polyval(coeffs, speed_grid)\n",
    "pred_grid = np.clip(pred_grid, -50, 200)  # Limiter pour la visualisation\n",
    "\n",
    "plt.plot(speed_grid, pred_grid, 'r-', linewidth=2, label=f'Polyn√¥me degr√© {degree}')\n",
    "plt.xlabel('Vitesse (mph)')\n",
    "plt.ylabel('Distance de freinage (ft)')\n",
    "plt.title(f'Degr√© {degree} ‚Äî MSE = {mse:.1f}')\n",
    "plt.legend()\n",
    "plt.ylim(-20, 150)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nMSE: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚ùì Questions de r√©flexion:**\n",
    "1. Quel degr√© donne le MSE le plus bas?\n",
    "2. Le polyn√¥me de degr√© 20 semble-t-il raisonnable pour pr√©dire √† de nouvelles vitesses?\n",
    "3. C'est quoi le probl√®me avec minimiser uniquement le MSE sur les donn√©es d'entra√Ænement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Partie 3: Train/Test Split ‚Äî Le surapprentissage\n",
    "\n",
    "Pour d√©tecter le **surapprentissage**, nous s√©parons les donn√©es en:\n",
    "- **Ensemble d'entra√Ænement** (70%): pour ajuster le mod√®le\n",
    "- **Ensemble de test** (30%): pour √©valuer la g√©n√©ralisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©paration train/test\n",
    "np.random.seed(42)  # Pour reproductibilit√©\n",
    "indices = np.random.permutation(len(speed))\n",
    "n_train = 35\n",
    "\n",
    "train_idx = indices[:n_train]\n",
    "test_idx = indices[n_train:]\n",
    "\n",
    "speed_train, dist_train = speed[train_idx], dist[train_idx]\n",
    "speed_test, dist_test = speed[test_idx], dist[test_idx]\n",
    "\n",
    "print(f\"Entra√Ænement: {len(speed_train)} exemples\")\n",
    "print(f\"Test: {len(speed_test)} exemples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la s√©paration\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(speed_train, dist_train, alpha=0.7, s=50, label='Entra√Ænement')\n",
    "plt.scatter(speed_test, dist_test, alpha=0.7, s=50, marker='s', label='Test')\n",
    "plt.xlabel('Vitesse (mph)')\n",
    "plt.ylabel('Distance de freinage (ft)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercice 3: Courbe biais-variance\n",
    "\n",
    "**Compl√©tez le code pour calculer l'erreur sur train ET test pour chaque degr√©:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = range(1, 16)\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for deg in degrees:\n",
    "    # Ajuster sur l'ensemble d'entra√Ænement\n",
    "    coeffs = np.polyfit(speed_train, dist_train, deg)\n",
    "    \n",
    "    # ============================================\n",
    "    # TODO: Calculez les pr√©dictions et erreurs\n",
    "    # ============================================\n",
    "    \n",
    "    # Pr√©dictions sur train\n",
    "    pred_train = None  # <- Compl√©tez avec np.polyval(...)\n",
    "    \n",
    "    # Pr√©dictions sur test\n",
    "    pred_test = None  # <- Compl√©tez avec np.polyval(...)\n",
    "    \n",
    "    # MSE sur train\n",
    "    mse_train = None  # <- Compl√©tez\n",
    "    \n",
    "    # MSE sur test\n",
    "    mse_test = None  # <- Compl√©tez\n",
    "    \n",
    "    train_errors.append(mse_train)\n",
    "    test_errors.append(mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation (ex√©cutez apr√®s avoir compl√©t√© le code ci-dessus)\n",
    "if None not in train_errors and None not in test_errors:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(degrees, train_errors, 'o-', linewidth=2, markersize=8, label='Erreur entra√Ænement')\n",
    "    plt.plot(degrees, test_errors, 's-', linewidth=2, markersize=8, label='Erreur test')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Degr√© du polyn√¥me (complexit√©)')\n",
    "    plt.ylabel('MSE (√©chelle log)')\n",
    "    plt.title('Compromis biais-variance')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(range(1, 16, 2))\n",
    "    plt.show()\n",
    "    \n",
    "    # Trouver le meilleur degr√©\n",
    "    best_deg = degrees[np.argmin(test_errors)]\n",
    "    print(f\"\\nüéØ Meilleur degr√© (selon erreur test): {best_deg}\")\n",
    "    print(f\"   MSE train: {train_errors[best_deg-1]:.1f}\")\n",
    "    print(f\"   MSE test: {test_errors[best_deg-1]:.1f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Compl√©tez le code de l'exercice 3!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Cliquez pour voir la solution</summary>\n",
    "\n",
    "```python\n",
    "# Pr√©dictions sur train\n",
    "pred_train = np.polyval(coeffs, speed_train)\n",
    "\n",
    "# Pr√©dictions sur test\n",
    "pred_test = np.polyval(coeffs, speed_test)\n",
    "\n",
    "# MSE sur train\n",
    "mse_train = np.mean((dist_train - pred_train)**2)\n",
    "\n",
    "# MSE sur test\n",
    "mse_test = np.mean((dist_test - pred_test)**2)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Partie 4: R√©gularisation Ridge\n",
    "\n",
    "Au lieu de choisir un degr√© bas, on peut utiliser un polyn√¥me de **haut degr√©** avec **r√©gularisation**.\n",
    "\n",
    "La r√©gularisation Ridge ajoute une p√©nalit√© sur les coefficients:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\theta}}_{\\text{Ridge}} = \\arg\\min_{\\boldsymbol{\\theta}} \\left[ \\sum_{i=1}^{N} (y_i - \\boldsymbol{\\theta}^\\top \\mathbf{x}_i)^2 + \\lambda \\|\\boldsymbol{\\theta}\\|^2 \\right]$$\n",
    "\n",
    "La solution en forme ferm√©e est:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\theta}}_{\\text{Ridge}} = (\\mathbf{X}^\\top \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}^\\top \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercice 4: Impl√©menter Ridge Regression\n",
    "\n",
    "**Compl√©tez la fonction ci-dessous:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_features(x, degree):\n",
    "    \"\"\"\n",
    "    Cr√©e la matrice de caract√©ristiques polynomiales.\n",
    "    \n",
    "    Args:\n",
    "        x: vecteur d'entr√©es (n,)\n",
    "        degree: degr√© du polyn√¥me\n",
    "    \n",
    "    Returns:\n",
    "        X: matrice (n, degree+1) avec colonnes [1, x, x¬≤, ..., x^degree]\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    X = np.zeros((n, degree + 1))\n",
    "    for d in range(degree + 1):\n",
    "        X[:, d] = x ** d\n",
    "    return X\n",
    "\n",
    "\n",
    "def ridge_regression(X, y, lambda_reg):\n",
    "    \"\"\"\n",
    "    Calcule les coefficients Ridge.\n",
    "    \n",
    "    Args:\n",
    "        X: matrice de features (n, d)\n",
    "        y: vecteur cible (n,)\n",
    "        lambda_reg: coefficient de r√©gularisation\n",
    "    \n",
    "    Returns:\n",
    "        theta: coefficients (d,)\n",
    "    \"\"\"\n",
    "    # ============================================\n",
    "    # TODO: Impl√©mentez la formule Ridge\n",
    "    # theta = (X'X + lambda*I)^(-1) X'y\n",
    "    # \n",
    "    # Indices:\n",
    "    # - X.T pour la transpos√©e\n",
    "    # - @ pour le produit matriciel\n",
    "    # - np.eye(n) pour la matrice identit√© n√ón\n",
    "    # - np.linalg.inv() pour l'inverse\n",
    "    # ============================================\n",
    "    \n",
    "    d = X.shape[1]  # nombre de features\n",
    "    \n",
    "    theta = None  # <- Remplacez par votre code\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de votre impl√©mentation\n",
    "degree = 10\n",
    "lambda_reg = 1.0\n",
    "\n",
    "X_train = polynomial_features(speed_train, degree)\n",
    "theta = ridge_regression(X_train, dist_train, lambda_reg)\n",
    "\n",
    "if theta is not None:\n",
    "    print(f\"Coefficients Ridge (degr√©={degree}, Œª={lambda_reg}):\")\n",
    "    print(f\"  Norme des coefficients: {np.linalg.norm(theta):.2f}\")\n",
    "    \n",
    "    # Pr√©dictions\n",
    "    X_test = polynomial_features(speed_test, degree)\n",
    "    pred_test = X_test @ theta\n",
    "    mse = np.mean((dist_test - pred_test)**2)\n",
    "    print(f\"  MSE test: {mse:.1f}\")\n",
    "    print(\"\\n‚úÖ Impl√©mentation correcte!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Compl√©tez la fonction ridge_regression!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° Cliquez pour voir la solution</summary>\n",
    "\n",
    "```python\n",
    "def ridge_regression(X, y, lambda_reg):\n",
    "    d = X.shape[1]\n",
    "    I = np.eye(d)\n",
    "    theta = np.linalg.inv(X.T @ X + lambda_reg * I) @ X.T @ y\n",
    "    return theta\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercice 5: Trouver le meilleur Œª\n",
    "\n",
    "**Explorez diff√©rentes valeurs de Œª pour un polyn√¥me de degr√© 10:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 10\n",
    "X_train = polynomial_features(speed_train, degree)\n",
    "X_test = polynomial_features(speed_test, degree)\n",
    "\n",
    "lambdas = [0, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "results = []\n",
    "\n",
    "print(f\"{'Œª':>10} | {'MSE Train':>10} | {'MSE Test':>10} | {'||Œ∏||':>10}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for lam in lambdas:\n",
    "    theta = ridge_regression(X_train, dist_train, lam)\n",
    "    if theta is not None:\n",
    "        mse_train = np.mean((dist_train - X_train @ theta)**2)\n",
    "        mse_test = np.mean((dist_test - X_test @ theta)**2)\n",
    "        norm_theta = np.linalg.norm(theta)\n",
    "        results.append((lam, mse_train, mse_test, norm_theta))\n",
    "        print(f\"{lam:>10.3f} | {mse_train:>10.1f} | {mse_test:>10.1f} | {norm_theta:>10.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'effet de Œª\n",
    "if len(results) > 0:\n",
    "    lambdas_plot = [r[0] for r in results]\n",
    "    train_plot = [r[1] for r in results]\n",
    "    test_plot = [r[2] for r in results]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.semilogx([max(l, 1e-4) for l in lambdas_plot], train_plot, 'o-', label='Train')\n",
    "    plt.semilogx([max(l, 1e-4) for l in lambdas_plot], test_plot, 's-', label='Test')\n",
    "    plt.xlabel('Œª (√©chelle log)')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.title(f'Effet de la r√©gularisation Ridge (degr√©={degree})')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**‚ùì Questions:**\n",
    "1. Quelle valeur de Œª minimise l'erreur test?\n",
    "2. Que se passe-t-il quand Œª ‚Üí 0? Et quand Œª ‚Üí ‚àû?\n",
    "3. Comment la norme des coefficients ||Œ∏|| change avec Œª?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ R√©capitulatif\n",
    "\n",
    "Dans ce TP, vous avez appris:\n",
    "\n",
    "1. **MSE**: L'erreur quadratique moyenne mesure la qualit√© des pr√©dictions\n",
    "\n",
    "2. **Surapprentissage**: Un mod√®le trop complexe m√©morise le bruit\n",
    "   - Erreur train ‚Üì mais erreur test ‚Üë\n",
    "   \n",
    "3. **Compromis biais-variance**: \n",
    "   - Mod√®le simple = biais √©lev√© (sous-apprentissage)\n",
    "   - Mod√®le complexe = variance √©lev√©e (surapprentissage)\n",
    "   \n",
    "4. **R√©gularisation Ridge**: P√©nalise les grands coefficients\n",
    "   - Permet d'utiliser des mod√®les complexes sans surapprentissage\n",
    "   - Œª contr√¥le la force de la r√©gularisation\n",
    "\n",
    "---\n",
    "\n",
    "**üìö Pour aller plus loin**: [Chapitre complet sur le site du cours](https://pierrelux.github.io/mlbook/learning-problem)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
