# K plus proches voisins

```{admonition} Objectifs d'apprentissage
:class: note

À la fin de ce chapitre, vous serez en mesure de:
- Expliquer le fonctionnement de l'algorithme des k plus proches voisins
- Définir et appliquer différentes fonctions de distance
- Analyser l'effet du paramètre k sur le compromis biais-variance
- Expliquer le fléau de la dimensionnalité et ses conséquences
- Implémenter l'algorithme k-ppv pour la classification et la régression
```

## L'idée de base

Soit $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^N$ un ensemble d'entraînement avec $x_i \in \mathbb{R}^d$ et $y_i \in \{1, \ldots, C\}$. Nous voulons prédire l'étiquette d'un nouveau point $x$. L'approche la plus simple consiste à regarder les exemples connus qui ressemblent à $x$ et à prédire la même chose.

Les k plus proches voisins (k-ppv) formalisent cette intuition. Pour classifier $x$, nous identifions les $k$ points de $\mathcal{D}$ les plus proches de $x$ et prenons un vote majoritaire sur leurs étiquettes. La méthode ne fait aucune hypothèse sur la forme de la relation entre $x$ et $y$. Elle se contente de consulter les données au moment de la prédiction.

Soit $\mathcal{N}_k(x)$ l'ensemble des indices des $k$ plus proches voisins de $x$. La prédiction est:

$$
\hat{y} = \arg\max_{c} \sum_{i \in \mathcal{N}_k(x)} \mathbb{1}_{y_i = c}
$$

La somme compte combien de voisins appartiennent à chaque classe $c$, et nous retenons la classe la plus fréquente.

Cette formulation admet une interprétation probabiliste. La proportion de voisins appartenant à la classe $c$ estime la probabilité conditionnelle:

$$
p(y = c \mid x, \mathcal{D}) = \frac{1}{k} \sum_{i \in \mathcal{N}_k(x)} \mathbb{1}_{y_i = c}
$$

La prédiction déterministe correspond au mode de cette distribution empirique.

## Fonctions de distance

L'algorithme repose sur la capacité à mesurer la proximité entre points. Une **fonction de distance** $d: \mathcal{X} \times \mathcal{X} \to [0, \infty)$ doit satisfaire trois axiomes: $d(x, y) = 0$ si et seulement si $x = y$ (identité), $d(x, y) = d(y, x)$ (symétrie), et $d(x, z) \leq d(x, y) + d(y, z)$ (inégalité triangulaire).

La **distance euclidienne** est le choix le plus courant:

$$
d_2(x, y) = \sqrt{\sum_{j=1}^{d} (x_j - y_j)^2} = \|x - y\|_2
$$

La **distance de Manhattan** suit les axes plutôt que la ligne droite:

$$
d_1(x, y) = \sum_{j=1}^{d} |x_j - y_j| = \|x - y\|_1
$$

Ces deux distances appartiennent à la famille des normes $\ell_p$, définies par $\|x\|_p = \left(\sum_j |x_j|^p\right)^{1/p}$.

La distance euclidienne traite toutes les dimensions de manière égale. Si les variables ont des échelles différentes, certaines domineront le calcul. La **distance de Mahalanobis** corrige ce problème:

$$
d_M(x, y) = \sqrt{(x - y)^\top M (x - y)}
$$

où $M$ est une matrice définie positive. Avec $M = I$, on retrouve la distance euclidienne. Avec $M = \Sigma^{-1}$ où $\Sigma$ est la covariance des données, les variables sont normalisées et décorrélées.

Le choix de la distance encode des hypothèses sur ce que signifie "similaire" dans le contexte du problème. Des données de natures différentes appellent des distances différentes.

## L'effet du paramètre k

Le paramètre $k$ contrôle la complexité du modèle. Avec $k = 1$, chaque point est classifié selon son plus proche voisin. La frontière de décision est très irrégulière et s'adapte étroitement aux données. L'erreur d'entraînement est exactement zéro: chaque point est son propre plus proche voisin. Mais cette adaptation excessive aux données d'entraînement nuit à la généralisation.

Avec un grand $k$, la prédiction moyenne sur plus de voisins et la frontière devient plus lisse. Le cas extrême $k = N$ prédit toujours la classe majoritaire globale, ignorant complètement l'entrée.

Entre ces deux extrêmes se trouve le compromis biais-variance. Un petit $k$ donne un modèle à faible biais mais haute variance: les prédictions sont sensibles aux fluctuations des données. Un grand $k$ donne un modèle à haute biais mais faible variance: les prédictions sont stables mais peuvent manquer des structures locales.

Le choix de $k$ se fait par validation. On trace l'erreur sur un ensemble de validation en fonction de $k$ et on retient la valeur qui minimise cette erreur. Des valeurs impaires évitent les égalités dans les votes.

## Diagramme de Voronoï

Le cas $k = 1$ induit une partition de l'espace en cellules. La cellule $V_i$ associée au point $x_i$ contient tous les points plus proches de $x_i$ que de tout autre point d'entraînement:

$$
V_i = \{x \in \mathbb{R}^d : d(x, x_i) \leq d(x, x_j) \text{ pour tout } j \neq i\}
$$

Cette partition s'appelle le **diagramme de Voronoï**. Les frontières entre cellules sont des hyperplans en dimension $d$. Avec le 1-ppv, la frontière de décision suit exactement ce diagramme. Un point mal étiqueté dans l'ensemble d'entraînement crée une cellule entière où les prédictions seront incorrectes.

## Le fléau de la dimensionnalité

Les k-ppv fonctionnent bien en basse dimension mais souffrent en haute dimension. Ce phénomène, le **fléau de la dimensionnalité**, affecte toutes les méthodes basées sur la localité.

Considérons des points uniformément distribués dans $[0, 1]^d$. Pour capturer une fraction $p$ des points dans un hypercube, le côté doit être $r = p^{1/d}$. En dimension 1, capturer 10% des points requiert un intervalle de longueur 0.1. En dimension 100, il faut un hypercube de côté $0.1^{1/100} \approx 0.98$, couvrant presque tout l'espace.

| Dimension $d$ | Côté $r$ pour $p = 0.1$ |
|---------------|-------------------------|
| 1 | 0.10 |
| 2 | 0.32 |
| 10 | 0.79 |
| 100 | 0.98 |

En haute dimension, les "voisins" ne sont plus locaux. La distance au plus proche voisin augmente et tous les points deviennent approximativement équidistants. Pour maintenir une densité constante, le nombre d'exemples requis croît exponentiellement avec la dimension.

La réduction de dimension et les distances adaptatives peuvent atténuer le problème, mais fondamentalement, les méthodes de voisinage perdent leur efficacité quand $d$ est grand.

## Régression

L'algorithme s'adapte à la régression en remplaçant le vote majoritaire par une moyenne:

$$
\hat{y} = \frac{1}{k} \sum_{i \in \mathcal{N}_k(x)} y_i
$$

Cette moyenne locale estime $\mathbb{E}[Y \mid X = x]$. Une variante pondère les voisins par l'inverse de leur distance:

$$
\hat{y} = \frac{\sum_{i \in \mathcal{N}_k(x)} w_i \, y_i}{\sum_{i \in \mathcal{N}_k(x)} w_i}, \quad w_i = \frac{1}{d(x, x_i)}
$$

Les voisins plus proches ont alors plus d'influence sur la prédiction.

## Complexité

L'entraînement consiste à stocker les données: $O(N)$. L'inférence requiert de calculer la distance à tous les points et d'identifier les $k$ plus proches: $O(Nd)$ par requête. Pour de grands ensembles, des structures comme les arbres k-d ou le hachage sensible à la localité (LSH) réduisent ce coût.

## Implémentation

```python
import numpy as np

def knn_classify(X_train, y_train, X_test, k=3):
    predictions = []
    for x in X_test:
        distances = np.sqrt(np.sum((X_train - x)**2, axis=1))
        k_nearest_idx = np.argsort(distances)[:k]
        k_nearest_labels = y_train[k_nearest_idx]
        unique, counts = np.unique(k_nearest_labels, return_counts=True)
        predictions.append(unique[np.argmax(counts)])
    return np.array(predictions)

def knn_regression(X_train, y_train, X_test, k=3):
    predictions = []
    for x in X_test:
        distances = np.sqrt(np.sum((X_train - x)**2, axis=1))
        k_nearest_idx = np.argsort(distances)[:k]
        predictions.append(np.mean(y_train[k_nearest_idx]))
    return np.array(predictions)
```

## Méthodes paramétriques et non paramétriques

Les k-ppv sont une méthode **non paramétrique**: les données sont le modèle. Il n'y a pas de paramètres appris; les prédictions consultent directement l'ensemble d'entraînement. La complexité du modèle croît avec $N$.

| | Non paramétrique | Paramétrique |
|--|------------------|--------------|
| **Modèle** | Les données | Un vecteur $\theta \in \mathbb{R}^p$ |
| **Complexité** | Croît avec $N$ | Fixe |
| **Inférence** | Requiert les données | Requiert seulement $\theta$ |

Les méthodes **paramétriques** distillent l'information dans un vecteur de paramètres de taille fixe. Un réseau de neurones entraîné sur des milliards d'exemples n'a besoin que de ses poids pour faire des prédictions, pas des données d'entraînement.

## Résumé

Les k plus proches voisins classifient un point par vote majoritaire parmi ses $k$ voisins les plus proches. Le paramètre $k$ contrôle le compromis biais-variance. Le choix de la distance encode les hypothèses sur la similarité. Le fléau de la dimensionnalité limite l'efficacité en haute dimension.

La méthode illustre la tension entre mémorisation et généralisation: avec $k=1$, l'erreur d'entraînement est nulle mais la généralisation est mauvaise. Elle illustre aussi la distinction entre approches non paramétriques (les données sont le modèle) et paramétriques (un vecteur de paramètres résume les données).

Le chapitre suivant développe l'approche paramétrique: l'apprentissage comme problème d'optimisation, où nous cherchons les paramètres qui minimisent une fonction de perte.
